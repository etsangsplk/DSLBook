
\section{Design and Implementation}
\label{design} 
 
 
This section discusses the implementation of mbeddr language
extensions. We briefly discuss the structure of the C core language
(\sect{ccore}). The major part of this chapter discusses each of the
ways $W_m$ of extending C (\sect{toplevel} through \sect{restriction}) based
on the extensions discussed in the previous section. 

 

\subsection{The mbeddr Core Languages}
\label{ccore}


C can be partitioned into expressions, statements, functions, etc. We
have factored these parts into separate language modules to make each of them
reusable without pulling in all of C. The \lcr{expressions} language is the most
fundamental language. It depends on no other language and defines the primitive
types, the corresponding literals and the basic operators. Support for pointers
and user defined data types (\lcr{enum, struct, union}) is factored into the
\lcr{pointers} and \lcr{udt} languages, respectively. \lcr{statements} contains
the procedural part of C, and the \lcr{modules} language covers modularization.
\fig{umldiagram} shows an overview over some of the languages and constructs.


\begin{figure}[h]   
\begin{center} 
  \includegraphics[width=75mm]{figures/impl/umldiagram.png}
\end{center}

\caption{Anatomy of the mbeddr language stack: the diagram shows some of the 
language concepts, their relationships and the languages that contain them.}

\label{umldiagram}
\end{figure} 
 

\subsection{Addressing $W_1$ (Top-Level Constructs): Test Cases}
\label{toplevel}

In this section we illustrate the implementation of the \ic{test case} construct
as well as of the \ic{assert} and \ic{fail} statements available inside test
cases.

\parhead{Structure} \lcr{Module}s own a collection of \lcr{IModuleCon- tent}s,
an interface that defines the properties of everything that can reside directly in
a module. All top-level constructs such as \lcr{Function}s implement
\lcr{IModuleCon- tent}. \lcr{IModuleContent} extends MPS'
\lcr{IIdentifier- NamedConcept} interface, which provides a \lcr{name} property.
\lcr{IModuleContent} also defines a Boolean property \lcr{exported} that
determines whether the respective module content is visible to modules that
import this module. This property is queried by the scoping rules that determine
which elements can be referenced. Since the \lcr{IModuleContent} interface can
also be implemented by concepts in other languages, new top level constructs
such as the \lcr{TestCase} in the \ic{unittest} language can implement this
interface, as long as the respective language has a dependency on the
\lcr{modules} language, which defines \lcr{IModuleContent}. \fig{umldiagram}
shows some of the relevant concepts and languages.



\parhead{Constraints} A test case contains a \lcr{StatementList}, so any C
statement can be used in a test case. \lcr{Statement- List} becomes available
to the unit test language through its dependency on the \lcr{statements}
language. \lcr{unittest} also defines new statements: \lcr{assert} and
\lcr{fail}. They extend the abstract \lcr{Statement} concept defined in the
\lcr{statements} language. This makes them valid in \emph{any} statement list,
for example in a function body. This is undesirable, since the transformation of
\lcr{assert}s into C depends on them being used in a \lcr{TestCase}. To
enforce this, a \emph{can be child} constraint is defined
(\fig{assertConstraints}).



\begin{figure}[h]
\begin{code} 
  concepts constraints AssertStatement { 
    can be child 
      (context, scope, parentNode, link, childConcept)->boolean { 
        parentNode.ancestor<TestCase>.isNotNull; 
  }   } 
\end{code}
\caption{This constraint restricts an \lcr{AssertStatement} to be used only
inside a \lcr{TestCase} by checking that at least one of its ancestors is a
\lcr{TestCase}.}
\label{assertConstraints} 
\end{figure}




\begin{figure}[h]
\begin{minipage}[b]{0.3\linewidth}
\begin{code}
test case exTest {
  int x = add(2, 2);               
  assert(0) x == 4;         
} 

\end{code}
\end{minipage}
\hspace{0.025\linewidth}
\begin{minipage}[b]{0.3\linewidth}
\begin{code} 
void test_exTest {
  int x = add(2, 2);               
  report 
    test.FAIL(0) 
    on !(x == 4);}
\end{code}
\end{minipage}
\hspace{0.025\linewidth}
\begin{minipage}[b]{0.3\linewidth}
\begin{code} 
void test_exTest {
  int x = add(2, 2);
  if (!(x == 4)) {
    printf("fail:0");
} } 
\end{code}
\end{minipage} 
\caption{Two-stage transformation of \lcr{TestCase}s. The \lcr{TestCase} is
transformed into a C function using the logging framework to output error
messages. The \lcr{report} statement is in turn transformed into a
\lcr{printf} statement \emph{if} we generate for the Windows/Mac environment. It
would be transformed to something else if we generated for the actual target
device (configured by the user in the build configuration).}
\label{unittest2c}

\end{figure}





\parhead{Transformation} The new language concepts in \lcr{unittest} are reduced
to C concepts: the \lcr{TestCase} is transformed to a \lcr{void} function without
arguments and the \lcr{assert} statement is transformed into a \lcr{report}
statement defined in the logging language. The \ic{report} statement, in turn,
it is transformed into a platform-specific way of reporting an error (console,
serial line or error memory). \fig{unittest2c} shows an example of this
two-step process.



\subsection{Addressing $W_2$ (Statements): Safeheap Statement}


We have seen the basics of integrating new statements in the previous section
where \ic{assert} and \ic{fail} extended the \ic{Statement} concept inherited
from the C core languages. In this section we focus on statements that require
handling local variable scopes and visibilities. We implement the \ic{safeheap}
statement mentioned earlier (see \fig{safeheap}), which automatically frees
dynamically allocated memory. The variables introduced by the \ic{safeheap}
statement must only be visible inside its body and they have to shadow
variables of the same name declared in outer scopes (such as the \ic{a} declared
in the second line of the \ic{measure} function in \fig{safeheap}).

\parhead{Structure} The \lcr{safeheap} statement extends \lcr{State- ment}. It
contains a \lcr{StatementList} as its body, as well as a list of
\lcr{SafeHeapVar}s. These extend \lcr{LocalVarDecl}, so they fit with the
existing mechanism for handling variable shadowing (explained below).
  
\begin{figure} 

\rule{\columnwidth}{0.4pt}
\vspace{-5mm}
\includegraphics[width=85mm]{figures/impl/safeheap.png}
\vspace{1mm}
\caption{A \lcr{safeheap} statement declares heap variables which can only be
used inside the body of the statement. When the body is left, the memory is
automatically freed. Notice also how we report an error in case the variable
tries to escape.}
\label{safeheap}  
\end{figure}


\parhead{Behaviour} \lcr{LocalVarRef}s are expressions that reference
\lcr{LocalVarDecl}. A scope constraint, a mechanism provided by MPS, determines
the set of visible variables for a given \lcr{LocalVarRef}. We implement this
constraint by plugging into mbeddr's generic local variable scoping mechanism
using the following approach. The constraint ascends the containment tree
until it finds a node which implements \lcr{ILocalVarScopeProvider} and calls its \lcr{getLocalVarScope}
method. A \lcr{LocalVarScope} has a reference to an outer scope, which is set by
finding \emph{its} \lcr{ILocalVarScopeProvider} ancestor, effectively building a
hierarchy of \lcr{LocalVarScope}s. To get at the list of the visible variables,
the \lcr{LocalVarRef} scope constraint calls the \lcr{getVisibleLocalVars}
method on the innermost \lcr{LocalVarScope} object. This method returns a flat
list of \lcr{LocalVarDecl}s, taking into account that variables owned by a
\lcr{LocalVarScope} that is \emph{lower} in the hierarchy shadow variables of the same
name from a \emph{higher} level in the hierarchy. So, to plug the
\lcr{SafeHeapStatement} into this mechanism, it has to implement
\lcr{ILocalVarScopeProvider} and implement the two methods shown in
\fig{safeheapbehaviour}.


\begin{figure}[h]
\begin{code} 
public LocalVarScope getLocalVarScope(node<> ctx, int stmtIdx) {
  LocalVarScope scope = 
      new LocalVarScope(getContainedLocalVariables()); 
  node<ILocalVarScopeProvider> outerScopeProvider = 
      this.ancestor<ILocalVarScopeProvider>; 
  if (outerScopeProvider != null)  
    scope.setOuterScope(outerScopeProvider.
                   getLocalVarScope(this, this.index)); 
  return scope; 
}
public sequence<node<LocalVariableDecl>> getContainedLocalVars() {
  this.vars;  
}
\end{code} 
\caption{A \lcr{safeheap} statement implements the two methods declared by the
\lcr{ILocalVarScopeProvider} interface. \lcr{getContainedLocalVariables}
returns the \lcr{LocalVarDecl}s that are declared between the parentheses (see
\fig{safeheap}). \lcr{getLocalVarScope} constructs a scope that contains these
variables and then builds the hierarchy of outer scopes by relying on its
ancestors that also implement \lcr{ILocalVarScopeProvider}. The index of the
statement that contains the reference is passed in to make sure that only
variables declared \emph{before} the reference site can be referenced.}
\label{safeheapbehaviour}
\end{figure}


\parhead{Type System} To make the \ic{safeheap} statement work correctly, we
have to ensure that the variables declared and allocated in the \emph{safeheap}
statement do not escape from its scope. To prevent this, an error is reported if
a reference to a \ic{safeheap} variable is passed to a function.
\fig{safeheapts} shows the code.



\begin{figure}[t]
\begin{code} 
checking rule check_safeVarRef for concept = LocalVarRef as lvr {                  
    boolean isInSafeHeap = 
      lvr.ancestor<SafeHeapStatement>.isNotNull; 
    boolean isInFunctionCall = 
      lvr.ancestor<FunctionCall>.isNotNull; 
    boolean referencesSafeHeapVar = 
      lvr.var.parent.isInstanceOf(SafeHeapStatement); 
    if (isInSafeHeap && isInFunctionCall && referencesSafeHeapVar) 
         error "cannot pass a safe heap var to a function" -> lvr; 
} 
\end{code} 
\caption{This type system rule reports an error if a reference to a local
variable declared and allocated by the \ic{safeheap} statement is used
in a function call.}
\label{safeheapts}
\end{figure}




 

\subsection{Addressing $W_3$ (Expressions): Decision Tables}


Expressions are different from statements in that they can be evaluated to a
\emph{value} as the program executes. During editing and compilation, the
\emph{type} of an expression is relevant for the static correctness of the
program. So extending a language regarding expressions requires extending the
type system rules as well.
  
\fig{dectab} shows the decision table expression. It is evaluated to the
expression in a cell \emph{c} if the column header of \emph{c} and the row
header of \emph{c} are \ic{true}\footnote{Strictly speaking, it is the
\emph{first} of the cells for which the headers are \ic{true}. It is optionally
possible to use static verification based on an SMT solver to ensure that only
one of them will be \ic{true} for any given set of input values}.
If none of the condition pairs is \ic{true}, then the default value, \ic{FAIL} in the
example, is used as the resulting value. A decision table also specifies the
type of the value it will evaluate to, and all the expression in content cells
have to be compatible with that type. The type of the header cells has to be
Boolean. 



\parhead{Structure} The decision table extends the \lcr{Expression} concept
defined in the \ic{expressions} language. Decision tables contain a list of
expressions for the column headers, one for the row headers and
another one for the result values. It also contains a child of type \lcr{Type} 
to declare the type of the result expressions, as well as a default value 
expression. The concept defines an alias \ic{dectab} to allow users to
instantiate a decision table in the editor. Obviously, for non-textual notations
such as the table, the alias will be different than the concrete syntax (in
textual notations, the alias is typically made to be the same as the "leading
keyword", e.g. \ic{assert}).


\parhead{Editor} Defining a tabular editor is straight forward: the editor
definition contains a \ic{table} cell, which delegates to a Java class that
implements \ic{ITableModel}. This is similar to the approach to the approach
used by Java Swing. It provides methods such as \ic{getValueAt( int row, int
col)} or \ic{deleteRow(int row)}, which have to be implemented for any specific
table-based editor. To embed another node in a table cell (such as the
expression in the decision table), the implementation of \ic{getValueAt} simply returns this node.


\parhead{Type System} As mentioned above, MPS uses unification in the type
system. Language concepts specify type equations that contain type literals
(such as \lcr{boolean}) as well as type variables (such as
\lcr{typeof(dectab)}). The unification engine then tries to assign values to the
type variables so that all applicable type equations become \ic{true}. New language
concepts contribute additional type equations. \fig{dectabtyping} shows those
for decision tables. New equations are solved along with those for existing
concepts. For example, the typing rules for a \lcr{ReturnStatement} ensure that
the type of the returned expression is the same or a subtype of the type of the
surrounding function. If a \lcr{ReturnStatement} uses a decision table as the
returned expression, the type calculated for the decision table must be
compatible with the return type of the surrounding function.


\begin{figure}[h]
\begin{code} 
  // the type of the whole decision table expression
  // is the type specified in the type field
typeof(dectab) :==: typeof(dectabc.type);   
  // for each of the expressions in
  // the column headers, the type must be boolean
foreach expr in dectab.colHeaders {        
  typeof(expr) :==: <boolean>;               
}                                          
  // ... same for the row headers
foreach expr in dectabc.rowHeaders {       
  typeof(expr) :==: <boolean>;             
}                                          
  // the type of each of the result values must 
  // be the same or a subtype of the table itself
foreach expr in dectab.resultValues {      
  infer typeof(expr) :<=: typeof(dcectab);  
}  
  // ... same for the default
typeof(dc.def) :<=: typeof(dectab);        
\end{code} 
\caption{The type equations for the decision table (see the
comments for details).}
\label{dectabtyping}
\end{figure}

 
 

\subsection{Addressing $W_4$ (Types and Literals): Physical Units}


To illustrate adding new types and literals we use physical units. We had
already shown example code earlier in \fig{unitsexample}.



\parhead{Structure} Derived and convertible \lcr{UnitDeclaration}s are
\ic{IModuleContents}. Derived unit declarations specify a name (\ic{mps},
\ic{kmh}) and the corresponding SI base units (\ic{m}, \ic{s}) plus an
exponent; a convertible unit declaration specifies a name and a conversion
formula. The backbone of the extension is the \ic{UnitType} which is a composite type that has another type (\ic{int}, \ic{float}) in its
\ic{valueType} slot, plus a unit (either an SI base unit or a reference 
to a \ic{UnitDeclaration}). It is
represented in programs as \ic{baseType/unit/}. We also provide
\ic{LiteralWithUnit}s, which are expressions that contain a
\ic{valueLiteral} and, like the \ic{UnitType}, a unit (so we can write 
\ic{100 kmh}).


\parhead{Scoping} \lcr{LiteralWithUnit}s and \lcr{UnitType}s refer to a
\lcr{UnitDeclaration}, which is a module content. According to the visibility
rules, valid targets for the reference are the \lcr{UnitDeclaration}s in the
same module, and the \emph{exported} ones in all imported modules. This rule
applies to \emph{any} reference to \emph{any} module contents, and is
implemented generically in mbeddr. \fig{scoping} shows the code for the scope of
the reference to the \lcr{UnitDeclaration}. We use an interface
\lcr{IVisibleNodeProvider}, (implemented by \lcr{Module}s) to find all
instances of a given type. The implementation of \ic{visibleContentsOfType}
simply searches through the contents of the current and imported modules and
collects instances of the specified concept. The result is used as the scope for
the reference.
 

\begin{figure}[h]
\begin{code} 
link {unit} search scope: 
    (model, scope, refNode, enclosingNode, operationContext) 
                             ->sequence<node<UnitDeclaration>> { 
      enclosingNode.ancestor<IVisibleNodeProvider>.
              visibleContentsOfType(concept/UnitDeclaration/); } 
\end{code} 
\caption{The \lcr{visibleContentsOfType} operation returns all instances of the
concept argument in the current module, as well as
all exported instances in modules imported by the current module.}
\label{scoping}

\end{figure}


 



\parhead{Type System} We have seen how MPS uses equations and unification to
specify type system rules. However, there is special support for binary
operators that makes overloading for new types easy:
overloaded operations containers essentially specify 3-tuples of
\emph{(leftArgType, rightArgType, resultType)} plus applicability conditions to
match type patterns and decide on the resulting type. Typing rules for new
(combinations of) types can be added by specifying additional 3-tuples.
\fig{ovopcont} shows the overloaded rules for C's \lcr{MultiExpression} (the
language concept the implements the multiplication operator \ic{*}) when applied
to two \ic{UnitType}s: the result type will be a \lcr{UnitType} as well, where
the exponents of the SI units are added.


\begin{figure}[h]
\begin{code}
operation concepts: MultiExpression
  left operand type: new node<UnitType>() 
  right operand type: new node<UnitType>() 
is applicable:
  (operation, leftOpType, rightOpType)->boolean {
    node<> resultingValueType = operation type(operation,
                  leftOpType.valueType , rightOpType.valueType ); 
    resultingValueType != null; }
operation type:
  (operation, leftOpType, rightOpType)->node<> {
     node<> resultingValueType = operation type(operation, 
               leftOpType.valueType,  rightOpType.valueType );
     UnitType.create(resultingValueType,
                      leftOpType.unit.toSIBase().add(
                         rightOpType.unit.toSIBase(), 
                         1 ) ); 
  }
\end{code} 
\caption{This code overloads the \ic{MultiExpression} to work for
\ic{UnitType}s. In the \ic{is applicable} section we check whether there is a
typing rule for the two value types (e.g. \ic{int * float}). This is achieved by
trying to compute the resulting value type. If none is found, the types cannot
be multiplied. In the computation of the \ic{operation type} we create a new
\ic{UnitType} that uses the \ic{resultingValueType} as the value type and then
computes the resulting unit by adding up the exponents of component SI units
of the two operand types.}
\label{ovopcont}
\end{figure}

While any two units can legally be used with \ic{*} and \ic{/}
(as long as we compute the resulting unit exponents correctly), this is not true for \ic{+}
and {-}. There, the two operand types must be the same (in terms of their
representation in SI base units). We express this by using the following
expression in the \ic{is applicable} section:
\ic{leftOpType.unit.isSameAs(rightOpType.unit)}.

The typing rule for the \ic{LocalVariableDeclaration} requires that the type of
the \ic{init} expression must be the same or a subtype of the \ic{type} of the
variable. To make this work correctly, we have to define a type hierarchy for
\ic{UnitType}s. We achieve this by defining the supertypes for each
\ic{UnitType}: the supertypes are those \ic{UnitType}s whose unit is the same,
and whose \ic{valueType} is a supertype of the current \ic{UnitType}'s value type.
\fig{supertypes} shows the rule.



\begin{figure}[h]
\begin{code}
subtyping rule supertypeOf_UnitType 
              for concept = UnitType as ut {
  nlist<> res = new nlist<>;
  foreach st in immediateSupertypes(ut.valueType) {
    res.add(UnitType.create(st, ut.unit.copy));
  }
  return res;
}
\end{code} 
\caption{This typing rule computes the direct supertypes of a \ic{UnitType}. It
iterates over all immediate supertypes of the current \ic{UnitType}'s value
type, wrapped into a \ic{UnitType} with the same unit as the original one.}
\label{supertypes}
\end{figure}





\subsection{Addressing $W_5$ (Alternative Transformations): Range Checking}


The \ic{safemodules} language defines an \emph{annotation} to mark \lcr{Modules}
as safe (we will discuss annotations in the next subsection). If a module is
safe, the binary operators such as \lcr{+} or \lcr{*} are replaced with calls to
functions that, in addition to performing the addition or multiplication,
perform a range check.

\parhead{Transformation}  The transformation that replaces the binary operators
with function calls is triggered by the presence of this annotation on the
\lcr{Module} which contains the operator.
\fig{safetransformation} shows the code. The
\ic{@safeAnnotation != null} checks for the presence of the annotation. MPS 
uses priorities to specify relative orderings of transformations, and MPS
then calculates a global transformation order for any given model. We use a
priority to express that this transformation runs \emph{before} the final
transformation that maps the C tree to C text for compilation.
 

\begin{figure}[t]
\rule{\columnwidth}{0.4pt}
  \includegraphics[width=85mm]{figures/impl/safetransformation.png}
\caption{This \emph{reduction rule} transforms instances of \lcr{PlusExpression}
into a call to a library function \lcr{addWithRangeChecks}, passing in the left 
and right argument of the \lcr{+} using the two \lcr{COPY\_SRC} macros. The 
\lcr{condition} ensures that the transformation is only executed if
the containing \lcr{Module} has a \lcr{safeAnnotation} attached to it. A
transformation priority defined in the properties of the transformation makes
sure it runs before the C-to-text transformation.}
\label{safetransformation}  
\end{figure} 


\subsection{Addressing $W_6$ (Meta Data): Requirements Traces}

Annotations are concepts whose instances can be added as children to a node
\emph{N} without this being specified in the definition of \emph{N}'s concept.
While structurally the annotations are children of the annotated node, the
editor is defined the other way round: the annotation editor delegates to the
editor of the annotated element. This allows the annotation editor to add
additional syntax \emph{around} the annotated element. Optionally, it is
possible to explicitly restrict the concepts to which a particular annotation
can be attached. We use annotations in several places: the \lcr{safe} annotation discussed in the
previous section, the requirements traces and the product line variability
presence conditions.

\parhead{Structure} We illustrate the annotation mechanism based on the
requirements traces. \fig{tracestructure} shows the structure. Notice how it
extends the MPS-predefined concept \ic{NodeAttribute} (it sould be named
\ic{Node- Annotation}). It also specifies a \ic{role}, which is the name of the
property that is used to store \ic{TraceAnnotation}s under the annotated node.



\begin{figure}[h]
\begin{code}
concept TraceAnnotation extends NodeAttribute implements <none>     
  children:                                   
    TraceKind        tracekind   1    
    TraceTargetRef   refs        0..n 
  concept properties:                         
    role = trace                                
  concept links:                              
    attributed = BaseConcept                    
\end{code} 
\caption{Annotations have to extend the MPS-predefined concept
\ic{NodeAttribute}. They can have an arbitrary child structure (\ic{tracekind},
\ic{refs}), but they have to specify the \ic{role} (the name of the property
that holds the annotated child under its parent) as well as the \ic{attributed}
concept: the annotations can only be attached to instances of this concept (or
subconcepts).}
\label{tracestructure}
\end{figure}

\parhead{Editor} As mentioned above, in the editor, annotations look as if they
\emph{surrounded} their parent node (although they are in fact children). 
\fig{traces} shows the definition of the editor of the requirements trace
annotation (and an example is shown in \fig{screenshot}): it puts the trace to
the right of the annotated node. Since MPS is a projectional editor, there is
base-language grammar that needs to be made aware of the additional syntax
in the program. This is key to enabling arbitrary annotations on arbitrary program nodes.

       
\begin{figure}[t]
\vspace{-8mm}
\begin{center} 
  \includegraphics[width=85mm]{figures/impl/tracesAll.png}
\end{center}
\caption{The editor definition for the \lcr{ReqTrace}
annotation (an example trace annotation is shown in \fig{screenshot}). It
consists of a vertical list \lcr{[/ .. /]} with two lines.
The first line contains the reference to the requirement. The second line uses the
\lcr{attributed node} construct to embed to the editor of the program node
to which this annotation is attached. So the annotation is always rendered
right on top of whatever syntax the original node uses.}
\label{traces}  
\end{figure}                                          


Annotations are typically attached to a program node via an intention.
Intentions are an MPS editor mechanism: a user
selects the target element, presses \ic{Alt-Enter} and selects \ic{Add Trace} from the popup menu. \fig{traceIntention} shows the code for the 
intention that attaches a requirements trace.

\begin{figure}[h]
\begin{code}
intention addTrace for BaseConcept {
  description(node)->string { 
    "Add Trace"; }                                                                                                                                                                           
  isApplicable(node)->boolean { 
    node.@trace == null; }                                                                
  execute(editorContext, node)->void { 
    node.@trace = new node<TraceAnnotation>(); }
}
\end{code} 
\caption{An intention definition consists of three parts. The \ic{description}
returns the string that is shown in the intentions popup menu. The
\ic{isApplicable} section determines under which conditions the intention is
avavailable in the menu --- in our case, we can only add a trace if there is no
trace yet on the target node. Finally, the \ic{execute} section performs the
action associated with the intention. In our case we simply put an instance of
\ic{TraceAnnotation} into the \ic{@trace} property of the target node.}
\label{traceIntention}
\end{figure}    




\subsection{Addressing $W_7$ (Restriction): Preventing Use of Reals Numbers} 
\label{restriction}

We have already seen in \sect{toplevel} how constraints can prevent the use of
specific concepts in certain contexts. We use the same approach for preventing
the use of real number types inside model-checkable state machines: a \ic{can be
ancestor} constraint in the state machine prevents instances of \ic{float} in
the state machine if the \ic{verifiable} flag is set.


\subsection{Revisiting Tool Extensions}
\label{toolext}

In this paper we emphasize the importance of extending the IDE along with the
languages. The way MPS works, a language extension automatically includes IDE
extension regarding syntax highlighting and code completion. However, some
IDE adaptations are automatically done as a consequence of a language
definition. One such adaptation is refactorings. \fig{refa} shows the core of an
\emph{Introduce Local Variable} refactoring. In addition to the core, a
refactoring specified the concepts to which it is applicable, a label for the
entry in the refactoring menu, and a set of parameters queried from the user.


\begin{figure}[h]
\begin{code}
node<Expression> selectedExpr = refactoringContext.selectedNode;
node<Statement> stmt = selectedExpr.ancestor<concept = Statement>;

nlist<Expression> matches = new nlist<Expression>;
foreach s in stmt.siblings.union(stmt) {
  if (s.index >= stmt.index) {
    foreach e in s.descendants<concept = Expression> {
      if (MatchingUtil.matchNodes(selectedExpr, e)) {
        matches.add(e);
} } } }

node<LocalVariableDecl> lvd = new node<LocalVariableDecl>();
lvd.name = varNameAsSpecifiedByUserInADialog;
lvd.type = selectedExpr.type.copy;
lvd.init = selectedExpr.copy;
stmt.add prev-sibling(lvd);

foreach e in matches {
  node<LocalVarRef> ref = new node<LocalVarRef>();
  ref.var = lvd;
  e.replace with(ref);
}
\end{code}
\caption{The core of the \emph{introduce variable} refactoring is made of four
steps. The first one creates variables for the selected expression and the
statement in which it is embedded. The second step looks through the rest of
the current statement list and finds all other uses of the same expression. In
the third step we create a new local variable, set its name, set its type to
the type of the original expression and then insert the new variable right
before the current statement. In the fourth step we replace all the occurences
of the original expression with a reference to the new variable.}
\label{refa}
\end{figure}





\todo{Do we want to add the debugger here? At least reference the debugger in
the debugger chapter.}


















