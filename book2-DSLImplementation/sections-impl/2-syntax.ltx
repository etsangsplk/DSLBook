\chapter{Concrete and Abstract Syntax}

\chapterabstract{In this chapter we look at the definition of abstract syntax
and concrete syntax, and the mapping between the two in parser-based and
projectional editing. We also discuss the advantages and drawbacks of these two
appraoches. We discuss the characteristics of typical AST definition
formalisms. The meat of the chapter is made of extensive examples of defining
language structure and syntax with Xtext, Spoofax and MPS.}

\todo{remove the mentioning of the example tools in all abstracts. They setence
doesn't really help.}

\todo{Show stuff in MPS with tables and graphics}

The \emph{concrete syntax} (CS) of a language is what the user interacts
with to create programs. It may be textual, graphical, tabular or any
combination thereof. In this book we focus mostly on textual concrete syntaxes
for reasons described in \todo{reference to the respective place in the
introduction}. Examples of other forms are briefly discussed \todo{ref}. We
refer to other forms where appropriate.
  
The \emph{abstract syntax} (AS) of a language is a data structure that
holds the core information in a program, but without any of the notational
details contained in the concrete syntax: keywords and symbols, layout (e.g.,
whitespace), and comments are typically not included in the AS. Note that the
syntactic information that doesn't end up in the AS is often preserved in some
"hidden" form so the CS can be reconstructed from the combination of the AS and
this hidden information --- this bidirectionality simplifies creating IDE
features such as quick fixes or formatters a lot.

In most cases, the abstract syntax is a tree data structure. Instances that
represent actual programs (i.e., sentences in the language) are hence often
called an abstract syntax tree or AST. Most formalisms also support
cross-references across the tree, in which case the data structure becomes a
graph (with a primary containment hierarchy). It is still usually called an AST.

While the CS is the interface of the language to the user, the AS acts as the
API to access programs by processing tools: it is used by developers of validators,
transformations and code generators. The concrete syntax is not relevant in
these cases.

To illustrate the relationship between the concrete and abstract syntax,
consider the following example program:

\begin{lstlisting}[morekeywords={var, int, calc}]
var x: int; 
calc y: int = 1 + 2 * sqrt(x)
\end{lstlisting}
  
\noindent This program has a hierarchical structure: definitions of \ic{x} and
\ic{y} at the top, inside \ic{y} there's a nested expression. This
structure is reflected in the corresponding abstract syntax tree. A possible AST
is illustrated in \fig{astexample}\footnote{We say \emph{possible} because
there are typically several ways of structuring the abstract syntax.}. 

\todo{gro§ genug?}
\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.65]{figures-impl/2/astexample.png}
  \caption[]{Abstract syntax tree for the above program. Boxes
  represent instances of language concepts, solid lines represent containment,
  dotted lines represent cross-references}
  \label{astexample} 
\end{center}
\end{figure}

\noindent There are two ways of defining the relationship between the CS and the
AS as part of language development:
\begin{description}
  \item[CS first] From a concrete syntax definition, an abstract syntax is
  derived, either automatically or using hints in the concrete
  syntax specification\footnote{This is more convenient, but the resulting AS
  may not be as clean as if it were defined manually; it may contain
  idiosyncracies that result from the derivation from the CS.}. This is the
  default use for Xtext, where Xtext derives the Ecore meta model from an Xtext
  grammar.
  \item[AS first] We first define the AS. We then define the concrete
  syntax, referring to the AS in the definition of the concrete syntax\footnote{This is often done if the AS structure already
  exists, has to conform to externally imposed constraints or is developed by
  another party than the language developer.}. For example, in Xtext it is
  possible to define grammar for an existing meta model.
\end{description}

\noindent Once the language is defined, there are again two ways how the
abstract syntax and the concrete syntax can relate as the language is used to
create programs\footnote{We will discuss these two approaches in more detail in
the next subsection.}:
\begin{description}
  \item[Parsing] In the parser-based approach, the abstract syntax tree is
  constructed from the concrete syntax of a program; a parser instantiates and populates the
  AS, based on the information in the program text. In this case, the
  (formal) definition of the CS is usually called a
  \emph{grammar}\footnote{Sometimes the parser creates a concrete syntax tree
  which is then transformed to an AST --- however, we ignore this aspect in
  the rest of the book, it is not essential.}. Xtext and Spoofax use this
  approach.
  \item[Projection] In the projectional approach, the abstract syntax tree is
  built directly by editor actions, and the concrete syntax is rendered from the AST
  via projection rules. MPS is an example of a tool that uses projectional
  editing.
\end{description} 

\noindent \fig{astandcst} shows the typical combinations of these two
dimensions. In practice, parser-based systems typically derive the AS from the
CS --- i.e., CS first. In projectional systems, the CS is usually annotated onto
the AS data structures - i.e., AS first.

\begin{marginfigure}
\begin{center}
  \includegraphics[width=50mm]{figures-impl/2/astandcst.png}
  \caption[]{Dimensions of defining the concrete and abstract syntax
  of a language. Xtext is mentioned twice because it supports CS first and AS
  first, although CS first is the default.}
  \label{astandcst} 
\end{center}
\end{marginfigure}



\section{Fundamentals of Free Text Editing and Parsing} 
\label{parsingfundamentals}

Most programming environments rely on free text editing, where
programmers edit programs at the text/character level to form (key)words and
phrases.

A \emph{parser} is used to check the program text (concrete syntax) for
syntactic correctness, and create the AST by populating the AS data structures
from information extracted from the textual source. Most modern IDEs perform this task in
real-time as the user edits the program and the AST is always kept in sync with
the program text. Many IDE features --- such as content assist, validation,
navigation or refactoring support --- are based on the always up-to-date AST.

\begin{marginfigure}
  \includegraphics[width=3.5cm]{figures-impl/2/parserbased.png}
  \caption{In parser-based systems, the user only interacts with the concrete
  syntax, and the AST is constructed from the information in the text via a
  parser.}
  \label{parserbased}
\end{marginfigure} 

A key characteristic of the free text editing approach is its strong separation
between the concrete syntax (i.e., text) and the abstract syntax. The concrete
syntax is the principal representation, used for both editing and persistence.
The abstract syntax is used under the hood by the implementation of the DSL,
e.g.\ for providing an outline view, validation, and for transformations and code
generation.

Many different approaches to parser implementation exist. Each may restrict the
syntactic freedom of a DSL, or constrain the way in which a particular syntax
must be specified. It is important to be aware of these restrictions since not
all languages can be comfortably, or even at all implemented by every parser
implementation approach. You may have heard terms like context free,
ambiguity, look-ahead, LL, (LA)LR or PEG. These all pertain to a certain class
of parser implementation approaches. We provide more details on the various
grammar and parser classes further on in this section.

\subsection{Parser Generation Technology}

In traditional compilers and IDEs (such as gcc or the Eclipse JDT), parsers are
often written by hand as a big program that reads a stream of characters and
uses recursion to create a tree structure.
However, manually writing a parser requires significant expertise
in parsing and a large development effort. For standardized programming languages
that don't change very often, and that have a large user community, this
approach makes sense. It can lead to very fast parsers that also exhibit good
error reporting and error recovery (the ability to continue parsing after a
syntax error has been found).

\todo{Do we want to point to books on compiler construction and parsers?}
In contrast, language workbenches, and most of today's compilers,
\emph{generate} a parser from a grammar. A grammar is a syntax specification
written in a DSL for formally defining textual concrete syntax. These generated
parsers may not provide the same performance or error reporting/recovery as a
hand-tailored parser constructed by an expert, but they provide bounded
performance guarantees that make them (usually) more than fast enough for modern
machines. Also, they generate a \emph{complete} parser for the \emph{complete}
grammar --- developers may forget corner cases if they write the parser
manually. However, the most important argument to use parser generation is that
the effort for building a parser is \emph{much} lower than manually writing a
custom parser. The grammar definition is also much more readable and
maintainable than the actual parser implementation (either custom-written or
generated). Finally, it means that the developer who defines a language does not
have to be an expert in parsing technology.

\parhead{Parsing versus scanning} Because of the complexity inherent in parsing,
parser implementations tend to split the parsing process into a number of
phases. In the majority of cases the text input is first separated into a
sequence of \emph{tokens} (i.e., keywords, identifiers, literals, comments, or
whitespace) by a \emph{scanner} (sometimes also called lexer). The \emph{parser}
then constructs the actual AST from the token sequence\footnote{Note that many
parser generators allow you to add arbitrary code (called actions) to the
grammar, for example to check constraints or interpret the program. We strongly
recommend not to do this: instead, a parser should \emph{only} check for
syntactic correctness and build the AST. All other processing should be built
on top of the AST. This separation between AST construction and AST processing
results in much more maintainable language implementations.}. This simplifies the
implementation compared to directly parsing at the character level. A scanner is
usually implemented using direct recognition of keywords and a set of regular
expressions to recognize all other valid input as tokens.\guido{Many lexers support lexer states (I'm pretty sure Antlr is the only
popular parser gen that does not support these states). They'd allow the same 
sophisticated contextual keywords as scannerless parsers do.}

Both the scanner and parser can be generated from grammars (see below). A
well-known example of a scanner (lexer) generation tool is
\ic{lex}\footnote{http://dinosaur.compilertools.net/}. Modern parsing 
frameworks, such as ANTLR\footnote{http://www.antlr.org/} do their own scanner
generation.

Note that the word "parser" now has more than one meaning: it can either refer
to the combination of the scanner and the parser or to the post-scanner parser
only. Usually the former meaning is intended (both in this book as well as
in general) unless scanning and parsing are discussed specifically.

A separate scanning phase has direct consequences for the overall parser
implementation, because the scanner typically isn't aware of the context of any
part of the input --- only the parser has this awareness. An example of a
typical problem that arises from this is that keywords can't be used as
identifiers even though often the use of a keyword wouldn't cause ambiguity in
the actual parsing. The Java language is an example of this: it uses a fixed set
of keywords, such as \ic{class} and \ic{public}, that cannot be used
as identifiers.

A context-unaware scanner can also introduce problems when grammars
are extended or composed. In the case of Java, this was seen with the
\ic{assert} and \ic{enum} keywords that were introduced in
Java 1.4 and Java 5, respectively. Any programs that used identifiers with those
names (such as unit testing APIs) were no longer valid. For composed
languages, similar problems arise as constituent languages have
different sets of keywords and can define incompatible
regular expressions for lexicals such as identifiers and numbers.

The term "scannerless parsing" refers to the absence of a separate
scanner and in this case we don't have the problems of context-unaware
scanning illustrated above, because the parser operates at a character
level and statefully processes lexicals and keywords. Spoofax (or
rather: the underlying parser technology SDF) uses scannerless
parsing.


\parhead{Grammars} Grammars are the formal definition for concrete textual
syntax. They consist of so-called production rules which define how valid
textual input ("sentences") looks like\footnote{They can also be used to
"produce" valid input by executing them "the other way round", hence the name.}.
Grammars form the basis for syntax definitions in text-based workbenches such as
Spoofax and Xtext\footnote{In these systems, the production rules are
enriched with information beyond the pure grammatical structure of the language, such as the
semantical relation between references and declarations.}.

Fundamentally, grammar production rules can be expressed in Backus-Naur Form
(BNF)\footnote{http://en.wikipedia.org/wiki/Backus-Naur\_Form}, written as \(S\)
\ic{::=} \(P_1\) \ic{...} \(P_n\). This grammar defines a symbol $S$ by a series
of pattern expressions $P_1$ \ic{...} $P_n$. Each pattern expression can refer
to another symbol or can be a literal such as a keyword or a punctuation symbol.
If there are multiple possible patterns for a symbol, these can be written as
separate productions (for the same symbol), or the patterns can be separated by
the \ic{|} operator to indicate a choice. An extension of BNF, called Extended
BNF (EBNF)\footnote{http://en.wikipedia.org/wiki/Extended\_Backus-Naur\_Form},
adds a number of convenience operators such as \ic{?} for  an optional pattern,
\ic{*} to indicate zero or more occurrences, and \ic{+} to indicate one or more
occurrences of a pattern expression.

As an example, the following code is an example of a grammar for
a simple arithmetic expression language using BNF notation. Basic expressions
are built up of \ic{NUM} number literals and the \ic{+} and \ic{*} 
operators.

\begin{lstlisting}[morekeywords={Exp, NUM, Expr}]
Exp ::= NUM
      | Exp "+" Exp
      | Exp "*" Exp
\end{lstlisting}

 
\noindent Note how expression nesting is described using recursion in this
grammar: the \ic{Exp} rule calls itself, so sentences like \ic{2 + 3 * 4} are
possible. This poses two practical challenges for parser generation systems:
first, the precedence and associativity of the operators is not described by
this grammar. Second, not all parser generators provide full support for
recursion\todoGuido{example?}. We elaborate on these issues in the remainder of
the section and in the Spoofax and Xtext examples.




\parhead{Grammar classes} BNF can describe any grammar that maps textual
sentences to trees based only on the input symbols. These are called
\emph{context-free grammars} and can be used to parse the majority of modern
programming languages\footnote{For example, SAP's ABAP language requires a
custom, hand-written parser}. In contrast, \emph{context-sensitive grammars} are
those that also depend on the context in which a partial sentence occurs, making
them suitable for natural language processing but at the same time, making
parsing itself a lot harder since the parser has to be aware of a lot more than
just the syntax.

Parser generation was first applied in command-line tools such as \ic{yacc} in
the early seventies\footnote{http://dinosaur.compilertools.net}. As a
consequence of relatively slow computers, much attention was paid to the
efficiency of the generated parsers. Various algorithms were designed that could
parse text in a bounded amount of time and memory. However, these time and space
guarantees could only be provided for certain subclasses of the context-free
grammars, described by acronyms such as LL(1), LL($k$), LR(1), and so on. A
particular parser tool supports a specific class of grammars --- e.g., ANTLR
supports LL($k$) and LL(*). In this naming scheme, the first L stands for
left-to-right scanning, and the second L in LL and the R in LR stand for
leftmost and rightmost derivation. The constant $k$ in LL($k$) and LR($k$)
indicates the maximum number (of tokens or characters) the parser will look
ahead to decide which production rule it can recognize. The bigger $k$, the more
syntactic forms can be parsed\footnote{Bigger values of $k$ may also reduce
parser performance, though}. Typically, grammars for "real" DSLs tend to need
only finite look-ahead and many parser tools effectively compute the optimal
value for $k$ automatically. A special case is LL(*), where $k$ is unbounded and
the parser can look ahead arbitrarily many tokens to make decisions. This
increases the set of syntactic forms that can be parsed successfully.


Supporting only a subclass of all possible context-free grammars poses
restrictions on the languages that are supported by a parser generator. For some
languages, it is not possible to write a grammar in a certain subclass, making
that particular language unparseable with a tool that only supports that
particular class of grammars. For other languages, a natural context-free
grammar exists, but it must be written in a different, sometimes awkward or
unintuitive way to conform to the subclass. This will be illustrated in the
Xtext example, which uses ANTLR as the underlying LL($k$) parser technology.


\guido{several reviewers suggested that we should remove the listing of the
various conflicts since just "dropping their names" really doesn't help. We do
explain some of the later, so I agree to the reviewers.} Parser generators can
detect if a grammar conforms to a certain subclass, reporting conflicts that
relate to the implementation of the algorithm:
\emph{shift/reduce} or \emph{reduce/reduce} conflicts for LR parsers, and
\emph{first/first} or \emph{first/follow} conflicts and direct or indirect
\emph{left recursion} for LL parsers (we will discuss some of these in detail
below). DSL developers can then attempt to manually refactor the grammar to
address those errors\footnote{Understanding these errors and then refactoring
the grammar to address them can be non-trivial since it requires an
understanding of the particular grammar class and the parsing algorithm.}. As an
example, consider a grammar for property or field access, expressions of the
form \ic{customer.name} or \ic{"Tim".length}\footnote{Note that we use ID to
indicate identifier patterns and STRING to indicate string literal patterns in
these examples.}:
\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING}] Exp ::= ID
      | Exp "." ID | STRING
\end{lstlisting}

\noindent 
This grammar uses left-recursion: the left-most symbol of one of the
definitions of \ic{Exp} is a call to \ic{Exp}, i.e.\ it is recursive. Left-recursion is
not supported by LL parsers such as ANTLR. 

The left-recursion can be removed by \emph{left-factoring} the grammar, i.e. by
changing it to a form where all left recursion is eliminated. The essence of
left-factoring is that the grammar is rewritten in such a way that all recursive
production rules consume at least one token or character before going into the
recursion. Left-factoring introduces additional rules that act as intermediaries
and often makes repetition explicit using the \ic{+} and \ic{*} operators. The
example grammar uses recursion for repetition, which can be made explicit as
follows:
\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING}]
Exp ::= ID
      | (Exp ".")+ ID
      | STRING
\end{lstlisting}

\noindent 
The resulting grammar is still left-recursive, but we can introduce an
intermediate rule to eliminate the recursive call to \ic{Exp}:

\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart}]
Exp ::= ID
      | (FieldPart ".")+ ID
      | STRING

FieldPart ::= ID
            | STRING
\end{lstlisting}

\noindent 
Unfortunately, this resulting grammar still has overlapping rules (a
first/first conflict) as the \ic{ID} symbol matches more than one rule. This
conflict can be eliminated by removing the \ic{Exp ::= ID} rule and making
the \ic{+} (one or more) repetition into a \ic{*} (zero or more)
repetition:

\guido{'m not an expert in gramarsÉ But it looks strange that you are talking 
about first/first conflict for ID and not talking about same conflict for 
STRING.. The only difference between these two tokens is: ID used in 
(FieldPart + ".")+ ID rule, but STRING - not..
}

\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart}]
Exp       ::= (FieldPart ".")* ID
            | STRING
            
FieldPart ::= ID
            | STRING
\end{lstlisting}

\guido{The refactored grammar is bogus. It allows things like
'stringliteral'.'stringliteral'.'stringliteral'.id}

\noindent 
This last grammar now describes the same language as the original field access
grammar shown above, but conforms to the LL(1) grammar
class\footnote{Unfortunately, it is also much more verbose. Refactoring "clean"
context free grammars to make them conform to a particular grammar class usually
makes the grammars larger and/or uglier.}. In the general case, not all
context-free grammars can be mapped to one of the restricted classes. Valid,
unambiguous grammars exist that cannot be factored to any of the restricted
grammar classes. In practice, this means that some languages cannot be parsed
with LL or LR parsers.

\parhead{General parsers} Research into parsing algorithms has produced parser
generators specific to various grammar classes, but there has also been research
in parsers for the full class of context-free grammars. A naive approach to
avoid the restrictions of LL or LR parsers may be to add backtracking, so that
if any input doesn't match a particular production, the parser can go back and
try a different production. Unfortunately, this approach risks exponential
execution times or non-termination and usually exhibits poor performance.

There are also general parsing algorithms that can \emph{efficiently} parse the
full class. In particular, generalized LR (GLR)
parsers\footnote{http://en.wikipedia.org/wiki/GLR\_parser} and Earley
parsers\footnote{http://en.wikipedia.org/wiki/Earley\_parser} can parse
unambiguous grammars in linear time and gracefully cope with ambiguities with 
cubic ($O(n^3)$) time in the worst case. Spoofax is an example of a language
workbench that uses GLR parsing.
\guido{s O(n^3) really efficient? Is there a space / time tradeoff as there is
usually with optimizations in one or the other direction.}


\parhead{Ambiguity} Grammars can be \emph{ambiguous} meaning that at least one
valid sentence in the language can be constructed in more than one
(non-equivalent) way from the production rules\footnote{This also means that
this sentence can be parsed in more than one way.}, corresponding to multiple
possible ASTs. This obviously is a problem for parser implementation as some
decision has to be made on which AST is preferred. Consider again the expression
language introduced above.

\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart}]
Exp ::= NUM
      | Exp "+" Exp
      | Exp "*" Exp
\end{lstlisting}
 
\noindent 
It is ambiguous, since for a string \ic{1*2+3} there are two possible trees
(corresponding to different operator precedences). 


\begin{Tree}
\node {Exp}
    child { node {Exp}
        child { node {Exp}
            child { node {1}}}
        child { child { node {*}}}
        child { node {Exp}
            child { node {2}}}}
    child {edge from parent[draw=none]}
    child { child { child { node {+}}}}
    child { node {Exp}
        child { child { node {3}}}};
\end{Tree}
\hspace{2em}
\begin{Tree}
\node {Exp} 
    child { node {Exp}
        child { child { node {1}}}}
    child { child { child { node {*}}}}
    child {edge from parent[draw=none]}
    child { node {Exp}
        child { node {Exp}
            child { node {2}}}
        child { child { node {+}}}
        child { node {Exp}
            child { node {3}}}};
\end{Tree}

\noindent The grammar does not describe which interpretation should be preferred.
Parser generators for restricted grammar classes and generalized parsers handle
ambiguity differently. We discuss both approaches.

\parhead{Ambiguity with grammar classes} LL and LR parsers are deterministic
parsers: they can only return one possible tree for a given input. This means
they can't handle any grammar that has ambiguities, including our simple
expression grammar. Determining whether a grammar is ambiguous is a classic
undecidable problem. However, it is possible to detect violations of the LL or
LR grammar class restrictions, in the form of conflicts. These conflicts do not
always indicate ambiguities (as seen with the field access grammar discussed
above), but by resolving all conflicts (if possible) an unambiguous grammar can
be formed.

Resolving grammar conflicts in the presence of associativity, precedence, and
other risks of ambiguity requires carefully layering the grammar in such a way
that it encodes the desired properties. To encode left-associativity and a lower
priority for the \ic{+} operator we can rewrite the grammar as follows:

\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart, Mult}]
Expr ::= Expr "+" Mult
       | Mult
Mult ::= Mult "*" NUM
       | NUM
\end{lstlisting}

\noindent 
The resulting grammar is a valid LR grammar. Note how it puts the \ic{+}
operator in the highest layer to give it the lowest priority\footnote{A \ic{+}
will end up further up in the expression tree than a \ic{*}; this means that the
\ic{*} has higher precedence, since any interpreter or generator will encounter
the \ic{*} first.}, and how it uses left-recursion to encode left-associativity
of the operators. The grammar can be left-factored to a corresponding LL grammar
as follows. We will see more extensive examples of this approach in the section
on Xtext (\Section{Sec:xtext-syntax}).

\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart, Mult,
NUM}] 
Expr ::= Mult ("+" Mult)*
Mult ::= NUM ("*" NUM)*
\end{lstlisting}


\parhead{Ambiguity with generalized parsers} General parsers accept grammars
regardless of recursion or ambiguity. That is, the expression grammar is readily
accepted as a valid grammar. In case of an ambiguity, the generated parser
simply returns \emph{all possible abstract syntax trees}, e.g.\ a left-associative tree
and a right-associative tree for the expression \ic{1*2+3}. The different trees
can be manually inspected to determine what ambiguities exist in the grammar, or
the desired tree can be programmatically selected.

A way of programmatically selecting one alternative is \emph{disambiguation
filters}. For example, left-associativity can be indicated on a per-production
basis:

\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart, Mult,
NUM}] 
Exp ::= NUM
      | Exp "+" Exp {left}
      | Exp "*" Exp {left}
\end{lstlisting}

\noindent 
This indicates that both operators are left-associative (using the
\ic{\{left\}} annotation from Spoofax). Operator precedence can be
indicated with relative priorities or with precedence annotations:

\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart, Mult,
NUM}] 
Exp ::= Exp "*" Exp {left}
>
Exp ::= Exp "+" Exp {left}
\end{lstlisting}

\noindent 
The \ic{>} indicates that the \ic{*} operator binds stronger than the
\ic{+} operator. This kind of declarative disambiguation is commonly found in
GLR parsers, but typically not available in parsers that support only more
limited grammar classes\footnote{As even these simple examples show, this style
of specifying grammars leads to simpler, more readable grammars. It also makes
language specification much simpler, since developers don't have to understand
the errors mentioned above}.



\parhead{Grammar Evolution and Composition} Grammars evolve as languages change
and new features are added. These features can be added by adding single, new
productions, or by composing the grammar with an existing grammar. Composition
of grammars is an efficient way of reusing grammars and quickly constructing or
extending new grammars\footnote{We discuss this extensively in
\sect{modularization}}. As a basic example of grammar composition, consider once
again our simple grammar for arithmetic expressions:
\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart, Mult,
NUM}] 
Expr ::= NUM
       | Expr "*" Expr
       | Expr "+" Expr
\end{lstlisting}

\todo{it would be interesting to see how one could use MPS/xtext/spoofax to 
solve the "expression problem" (really well known)
http://en.wikipedia.org/wiki/Expression_problem
http://c2.com/cgi/wiki?ExpressionProblem
http://channel9.msdn.com/Shows/Going+Deep/C9-Lectures-Dr-Ralf-Laemmel-Advanced-Functional-Programming-The-Expression-Problem
}

\noindent 
Once more operators are added and the proper associativities and precedences are
specified, such a grammar forms an excellent unit for reuse\footnote{For
examople, expressions can be used as guard conditions in state machines, for
pre- and postconditions in interface definitions or to specify derived
attributes in a data definition language.}. As an example, suppose we want to
compose this grammar with the grammar for field access expressions\footnote{
Here we consider the case where two grammars use a symbol with the identical
name \ic{Expr}. Some grammar definition formalisms support mechanisms such as
grammar mixins and renaming operators to work with grammar modules where the
symbol names do not match.}:

\begin{lstlisting}[morekeywords={Exp, NUM, Expr,ID, STRING, FieldPart, Mult,
NUM}] 
Expr ::= ID
       | STRING
       | Expr "." ID
\end{lstlisting}

\noindent 
In the ideal case, composing two such grammars should be trivial --- just copy
them into the same file. However, reality is often less than ideal. There are a
number of challenges that arise in practice, related to ambiguity and to grammar
class
restrictions\footnote{http://tratt.net/laurie/tech_articles/articles/parsing\_the\_solved\_problem\_that\_isnt}.


\begin{itemize}
\item Composing arbitrary grammars risks introducing ambiguities that did not exist in
either of the two constituent grammars. In the case of the arithmetic
expressions and field access grammars, care must specifically be taken to
indicate the precedence order of all operators with respect to all others. With
a general parser, new priority rules can be added without changing imported two
grammars. When an LL or LR parser is used, it is often necessary to change one
or both of the composed grammars to eliminate any conflicts. This is because ina general parser, the precedences are \emph{declarative} (additional preferencespecification can simply be added at the end), whereas in LL or LR parsers theprecedence information is encoded in the grammar structure (and invasivechanges to this structure may be required).\item We have shown how grammars can be massaged with techniques such as left-factoring in order to
conform to a certain grammar class. Likewise, any precedence order or
associativity can be encoded by massaging the grammar to take a certain form.
Unfortunately, all this massaging makes grammars very resistant to change and
composition: after two grammars are composed together, the result is often
no longer LL or LR, and another manual factorization
step is required.

\item Another challenge is in composing scanners. When two grammars that depend on a
different lexical syntax are composed together, conflicts can arise. For
example, consider what happens when we compose the grammar of Java with the
grammar of SQL:

\begin{lstlisting}[morekeywords={for, SELECT, FROM, WHERE}] 
for (Customer c : SELECT customer FROM accounts WHERE balance < 0) {
  ...
}
\end{lstlisting}

\noindent The SQL grammar reserves keywords such as \ic{SELECT}, even though
they are not reserved in Java. Such a language change could break compatibility
with existing Java programs which happen to use a variable named \ic{SELECT}.
This problem can only be avoided by a scannerless parser, which considers the
lexical syntax in the context it appears instead during a separate scanning
stage, where no context is considered.

\guido{Can be adressed with multiplex parsing / layered lexers / state based
lexers}

\end{itemize}




\section{Fundamentals of Projectional Editing} 

In parser-based approaches, users use text editors to enter character sequences
that represent programs. A parser then checks the program for syntactic
correctness and constructs an abstract syntax tree from the character sequence.
The AST contains all the semantic information expressed by the program.

\todo{point to a screencast that shows PE in action}
In projectional editors, the process happens the other way round: as a user
edits the program, the AST is modified directly. A projection engine then
creates some representation of the AST with which the user interacts, and which
reflects the changes. This approach is well-known from graphical editors in
general, and the MVC pattern specifically. When users edit a UML diagram,
they don't draw pixels onto a canvas, and a "pixel parser" then creates
the AST. Rather, the editor creates and instance of \ic{uml.Class} as you drag a class from
the palette to the canvas. A projection engine renders the diagram, in this case
drawing a rectangle for the class. This approach can be generalized to work
with any notation, including textual.

\begin{marginfigure}[-20mm]
  \includegraphics[width=3.5cm]{figures-impl/2/projectional.png}
  \caption{In projectional systems, the user sees the concrete syntax, but all
  editing gestures directly influence the AST. The AST is \emph{not} extracted
  from the concrete syntax, which means the CS does not have to be parseable.}
  \label{projectional}
\end{marginfigure} 

This explicit instantiation of AST objects happens by picking the respective
concept from the code completion menu using a character sequence defined by the
respective concept. If at any given program location two concepts can be
instantiated using \emph{the same character sequence}, then the projection
editor prompts the user to decide\footnote{As discussed above, this is the
situation where many grammar-based systems run into problems from ambiguity.}.
Once a concept is instantiated, it is stored as a node with a unique ID (UID) in
the AST. References between program elements are based on actual pointers
(references to UIDs), and the projected syntax that represents the reference can
be arbitrary. The AST is actually an abstract syntax graph \emph{from the
start} because cross-references are first-class rather than being resolved after
parsing\footnote{There is still one single containment hierarchy, so it is
really a tree with cross-references.} The program is stored using a generic tree
persistence mechanism, often XML.

Defining a projectional editor, instead of defining a grammar, involves the
definition of projection rules that map language concepts to a notation. It also
involves the definition of event handlers that modify the AST based on
users' editing gestures. The way to define the projection rules and the event
handlers is specific to the particular tool used. 

The projectional approach can deal with arbitrary syntactic forms including
traditional text, symbols (as in mathematics), tables or graphics. Since no
grammar is used, grammar classes are not relevant here. In principle,
projectional editing is simpler in principle than parsing, since there is no
need to "extract" the program structure from a flat textual source. However, as
we will see below, the challenge in projectional editing lies making the editing
experience convenient\footnote{In particular, editing notations that look like
text should be editable with the editing gestures known from text editors.}.



\section{Comparing Parsing and Projection}

\subsection{Editing Experience}

In free text editing, any regular text editor will do. However, users expect a
powerful IDE that includes support for syntax coloring, code completion,
go-to-definition, find references, error annotations, refactoring and the like.
Xtext and Spoofax provide IDE support that is essentially similar to what a
modern IDE provides for mainstream languages (e.g. Eclipse for Java)\footnote{We
assume that you are familiar with modern IDEs, so we do not discuss their
features in great detail in this section}. However, you can always go back to
any text editor to edit the programs.

In projectional editing, this is different since a normal text editor is
obviously not sufficient as you would be editing some textual representation of
the AST. A specialized editor has to be supplied. Like in free text editing, it
has to provide the IDE support features mentioned above. MPS provides those.
However, there is another challenge: for textual-looking notations, it is
important that the editor tries and makes the editing experience as text-like as
possible, i.e. the keyboard actions we have gotten used to from free-text
editing should work as far as possible. MPS does a decent job here, using, among
others, the following strategies\footnote{The following list may be hard torelate to if you have never used a projectional editor. However, understandingthis section in detail is not essential for the rest of the book.}:
\todo{We should add a screenshot here. Maybe the "big one" from the ASE paper.}

\begin{itemize}

  \item Every language concept that is legal at a given program location is
  available in the code completion menu. In naive implementations, users have to
  select the language concept (based on its name) and instantiate it. This is
  inconvenient. In MPS, languages can instead define aliases for language
  concepts, allowing users to "just type" the alias, after which the concept
  is immediately instantiated\footnote{By making the alias the same as the
  leading keyword (e.g. \ic{if} for an \ic{IfStatement}), users can "just
  type" the code.}.

  \item So-called side transforms make sure that expressions can be entered
  conveniently. Consider a local variable declaration \ic{int a = 2;}. If this
  should be changed to \ic{int a = 2+3;} the \ic{2} in the init expression needs
  to be replaced by an instance of the binary \ic{+} operator, with the \ic{2}
  in the left slot and the \ic{3} in the right. Instead of removing the \ic{2}
  and manually inserting a \ic{+}, users can simply type \ic{+} on the right
  side of the \ic{2}; the system performs the tree restructuring that moves the
  \ic{+} to the root of the subtree, puts the \ic{2} in the left slot, and then
  puts the cursor into the right slot, to accept the second argument. This means
  that expressions (or anything else) can be entered linearly, as expected. For
  this to work, operator precedence has to be specified, and the tree has to be
  constructed taking these precedences into account. Precedence is typically
  specified by a number associated with each operator, and whenever using a side
  transformation to build an expression, the tree is automatically reshuffled to
  make sure that those operators with a higher precedence number are further
  down in the tree.
  
  \item Delete actions are used to similar effect when elements are deleted.
  Deleting the \ic{3} in \ic{2+3} first keeps the plus, with an empty right slot.
  Deleting the \ic{+} then removes the \ic{+} and puts the \ic{2} at the root of the subtree.

  \item Wrappers support instantiation of concepts that are actually children of
  the concepts allowed at a given location. Consider again a local variable
  declaration \ic{int a;}. The respective concept could be
  \ic{LocalVariableDeclaration}, a subconcept of \ic{Statement}, to make it
  legal in method bodies (for example). However, users simply want to start
  typing \ic{int}, i.e.\ selecting the content of the \ic{type} field of the
  \ic{LocalVariableDeclaration}. A wrapper can be used to support entering
  \ic{Type}s where \ic{LocalVariableDeclaration}s are expected. Once a \ic{Type}
  is selected, the wrapper implementation creates a
  \ic{LocalVariableDeclaration}, puts the \ic{Type} into its \ic{type} field,
  and moves the cursor into the \ic{name} slot.

  \item Smart references achieve a similar effect for references (as opposed to
  children). Consider pressing \ic{Ctrl-Space} after the \ic{+} in \ic{2+3}.
  Assume further, that a couple of local variables are in scope and that these
  can be used instead of the \ic{3}. These should be available in the code
  completion menu. However, technically, a \ic{VariableReference} has to be
  instantiated, whose \ic{variable} slot then is made to point to any of the
  variables in scope. This is tedious. Smart references trigger special editor
  behavior: if in a given context a \ic{VariableReference} is allowed, the
  editor \emph{first} evaluates its scope to find the possible targets and then
  puts those targets into the code completion menu. If a user selects one,
  \emph{then} the \ic{VariableReference} is created, and the selected element is
  put into its \ic{variable} slot. This makes the reference object effectively
  invisible in the editor.

  \item Smart delimiters are used to simplify inputting list-like data that is
  separated with a specific separator symbol, such as parameter lists. Once a
  parameter is entered, users can press comma, i.e.\ the list delimiter, to
  instantiate the next element.

\end{itemize}

\noindent Notice that except for having to get used to the somewhat different
way of editing programs, the strategies mentioned above (plus some others)
result in a reasonably good editing experience. Traditionally, projectional
editors have \emph{not} used these or similar strategies, and projectional
editors have gotten a bit of a bad reputation because of that. In case of MPS
this tool support is available, and hence, MPS provides a productive and
pleasant working environment.



\subsection{Language Modularity}

As we have seen in \todo{ref to the respective design chapter}, language
modularization and composition is an important building block in working with
DSLs. Parser-based and projectional editors come with different trade-offs in
this respect. Notice that is this section we only consider syntax issues ---
semantics has been covered in \todo{the respective section in DSL design}.

In parser-based systems it depends on the grammar class supported by the tool
whether composition can be supported well. As we have said above, the problem is
that the result of combining two or more independently developed grammars into
one may become ambiguous, for example, because the same character sequence is defined as two
different tokens. The resulting grammar cannot be parsed and has to be
disambiguated manually, typically by invasively changing the composite grammar.
This of course breaks modularity and hence is not an option. Parsers that
do not support the full set of context-free grammars, such as ANTLR, and hence
Xtext, have this problem. Parsers that do support the full set of context-free
grammars, such as the GLR parser used as part of Spoofax, do not have this problem. While
a grammar may become ambiguous in the sense that a program may be parseable in
more than one way, this can be resolved by declaratively specifying which
alternative should be used. This specification can be made externally, \emph{without} invasively changing the resulting grammar, retaining modularity.

In projectional editors, language modularity and composition is not a problem at
all. There is no grammar, no parsing, no grammar classes, and hence no problem
with composed grammars becoming ambiguous. Any combination of languages will be
syntactically valid. If a composed language would be ambiguous, the user has to
make a disambiguating decision as the program is entered. For example, in MPS,
if in a given location two language concepts are available under the same alias,
just typing the alias won't bind, and the user has to manually decide by picking
one alternative from the code completion menu.


\subsection{Notational Freedom}

Parser-based systems process linear sequences of character symbols.
Traditionally, the character symbols were taken from the ASCII character set,
resulting in textual programs being made up from "plain text". With the advent
of unicode, a much wider variety of characters is available while still sticking
to the linear sequence of character symbols approach. For example, the Fortress
programming
language\footnote{http://en.wikipedia.org/wiki/Fortress\_(programming\_language)}
makes use of this: greek letters and a wide variety of different bracket styles
can be used in the programs. However, character layout is always ignored. For
example it is not possible to use parsers to handle tabular notations, fraction
bars or even graphics\footnote{There have been experimental parsers for
two-dimensional structures such as tables and even for graphical shapes, but
these have never made it beyond the experimental stage.}.
\todo{Table based syntax is possible with Text based tools (even with Xtext -
see JNario test data specs) - JNario in the last chapter of the book? refernce
to it?}

\begin{marginfigure}
  \includegraphics[width=7cm]{figures-impl/2/dectabincomp.png}
  \caption{A table embedded in an otherwise textual program}
  \label{dectabincomp}
\end{marginfigure} 
In projectional editing, this limitation does not exist. A projectional editor
never has to extract the AST from the concrete syntax; editing gestures directly
influence the AST, and the concrete syntax is rendered from the AST. This
mechanism is basically like a graphical editor. Notations other than text can be
used. For example, MPS supports tables as well as simple diagrams. Since these
non-textual notations are handled the same way as the textual ones (possibly 
with other input gestures), they can be mixed easily: tables can be embedded
into textual source, and textual languages can be used within table cells (see
\fig{dectabincomp}). Textual notations can also be used inside boxes or as connection labels in
diagrams. 
 
\subsection{Language Evolution}
If the language changes, existing instance models temporarily become
outdated in the sense that they had been developed for the old version of the
language. If the new language is not backward compatible, these existing models
have to be migrated to conform to the updated language. 

Since projectional editors store the models as structured data where each
program node points to the language concept it is an instance of, the tools have
to take special care that such "incompatible" models can still be opened and the
migrated, manually or by a script, to the new version of the language. MPS
supports this feature, and it is also possible to distribute migration scripts
with (updated) languages to run the migration automatically\footnote{It is also
possible to define quick fixes that run \emph{automatically}; so whenever a
concept that is marked as \ic{deprecated}, this quick fix can trigger an
automatic migration to a new concpet.}.

Most textual IDEs do not come with explicit support for evolving programs as
languages change. However, since a model is essentially a sequence of
characters, it can \emph{always} be opened in the editor. The program may not be
parseable, but users can always update the program manually, or with global
search and replace using regular expressions. More complex migrations may
require explicit support via transformations on the AST.

\subsection{Infrastructure Integration}

Today's software development infrastructure is typically text-oriented. Many
tools used for diff and merge, or tools like \ic{grep} and regular expressions
are geared towards textual storage. This means that DSLs that use free text
editing integrate nicely with these tools. 

In projectional IDEs, special support needs to be provided for infrastructure
integration. Since the CS is not pure text, a generic persistence format is
used, typically based on XML. While XML is technically text as well, it is not
practical to require uses to perform diff, merge and the like on the level of
the XML. Therefore, special tools need to be provided for diff and merge. MPS
provides integration with the usual version control systems and handles diff and
merge in the IDE, using the concrete, projected syntax\footnote{Note that since every
program element has a unique ID, \emph{move} can potentially be distinguished
from \emph{delete/create}, providing richer semantics for diff and merge.}.
\fig{fig:mps-diff.png} shows an example of an MPS diff. However, it clearly is a
drawback of projectional editing (and the associated abstract syntax-based
storage) that many well-known text utilities don't work.

Also, copy and paste with textual environments may be a challenge. MPS, for
example, supports pasting a projected program that has a textual-looking syntax
into a text editor. However, for the way back (from a textual environment to
the projectional editor), there is no automatic support. However, MPS comes with
an integrated Java parser to handle these cases: when a user pastes Java text
into a Java program in MPS, this parser is executed and builds the respective
MPS tree. While this works reasonably well, it is neither automatic nor is it as
painless as in a purely textual world.

\begin{figure}[ht] 
\begin{center}
  \includegraphics[width=\columnwidth]{figures-impl/2/mps-diff.png}
  \caption[]{The diff/merge tool presents MPS programs in their
  concrete syntax, i.e. text for textual notations. However, other notations,
  such as tables, would also be rendered in their native form.}
  \label{fig:mps-diff.png} 
\end{center}
\end{figure}


\subsection{Tool Lock-in}

As mentioned, in the worst case, textual programs can be edited with any text
editor. Unless you are prepared to edit XML, programs expressed with a
projectional editor \emph{always} require that editor to edit programs. As soon
as you take IDE support into account though, both approaches lock users into a
particular tool. Also, there is essentially no standard for exchanging
language definitions between the various language workbenches\footnote{There is
\emph{some} support for exchanging the abstract syntax based on formalisms such
as MOF or Ecore.}. So the effort of implementing a language is always lost in
case the tool should be changed.

\subsection{Other}

In parser-based systems, the complete AST has to be reconstructable from the CS.
This implies that there can be no information in the tree that is \emph{not}
obtained from parsing the text. This is different in projectional editors. For
example, the textual notation could only project a subset of the information in
the tree. The same information can be projected with different projections, each
possibly tailored to a different stakeholder, and showing a different subset
from the overall data. Since the tree uses a generic persistence mechanism, it
can hold data that has not been planned for in the original language definition.
All kinds of meta data (documentation, presence conditions, requirements traces)
can be stored, and projected if required. MPS supports so-called annotations,
where additional data can be added to model elements of existing languages and
projecting that data inside the original projection, all without changing the
original language specification.





\section{Characteristics of AST formalisms}

\todo{can we provide the same example for all tools?}

Most AST formalisms, aka meta meta models\footnote{\emph{Abstract syntax} and
\emph{meta model} are typically considered synonyms, even though they have
different histories (the former comes from the parser/grammar community whereas
the latter comes from the modeling community). Consequently, the formalisms for
defining ASTs are conceptually similar to meta meta models.}, are ways to
represent trees or graphs. Usually, such an AST formalism is "meta circular" in
the sense that it can describe itself.
Note that this section is really just a very brief overview over the three AST
formalisms relevant to Xtext, Spoofax and MPS. We will illustrate them in more
detail in the respective tool example sections.

\subsection{EMF Ecore}

The Eclipse Modeling Framework\footnote{http://www.eclipse.org/modeling/emf/}
(EMF) is at the core of all Eclipse Modeling tools. It provides a wide variety
of services and tools for persisting, editing and processing models and abstract
syntax definitions. Its core component is Ecore, a variant of the EMOF
standard\footnote{http://en.wikipedia.org/wiki/Meta-Object\_Facility}. Xtext
uses Ecore as the foundation for the AS: from a grammar definition, Xtext
derives the AS as an instance of Ecore. Ecore's central concepts are:
\ic{EClass} (representing AS elements or language concepts), \ic{EAttribute}
(representing primitive properties of \ic{EClass}es), \ic{EReference}
(representing associations between \ic{EClass}es) and \ic{EObject} (representing
instances of \ic{EClass}es, i.e. AST nodes.
\ic{EReferences} can be containing or not -- each \ic{EObject} can be contained
by at most one \ic{EReference} instance. \fig{fig:ecore-metamodel} shows a class
diagram of Ecore.

\begin{figure*}[ht]
  \includegraphics[width=16cm]{figures-impl/2/ecore-metamodel-2.png}
  \caption[]{The Ecore meta model rendered as a UML diagram.}
  \label{fig:ecore-metamodel} 
\end{figure*}

\noindent When working with EMF, the Ecore file plays a central role. From that,
all kinds of other aspects are derived, specifically, a generic tree editor, and a generated Java API for
accessing an AST. This also forms the basis for Xtext's model processing: The
Ecore file is derived from the grammar, and the parser, when executed, builds an
in-memory tree of \ic{EObject}s representing the AST of the parsed program.

EMF has grown to be a fairly large ecosystem within the Eclipse community and
numerous projects use EMF as a basis for model manipulation and persistence,
with Ecore for meta model definition.

\subsection{Spoofax' ATerm}
\label{Sec:spoofax-aterm}

Spoofax uses the ATerm format to represent abstract syntax. ATerm provides
a generic tree structure representation format that can be serialized
textually similar to XML or JSON. Each tree node is called an ATerm, or simply a \emph{term}.
Terms consist of the following elements: Strings (\ic{"Mr. White"}), Numbers
(\ic{15}), Lists (\ic{[1,2,3]}) and constructor applications (\ic{Order(5, 15, 
"Mr. White")} for labelled tree nodes with a fixed number of children.

\guido{While ECore section was mostly about meta-model definition ATerm one
looks like specific to the model representation & storage. m.b. It make sense 
to say something about the way to represent meta-model in Spoofax (or meta-meta 
model) unless it was done intentionally.}

Compared to XML or JSON, perhaps the most significant distinction is that
ATerms often rely on the order of subterms rather than on labels. For example,
a product may be modeled in JSON as follows:

\begin{lstlisting}[morekeywords={}] 
{
  "product": {
    "itemnumber": 5,
    "quantity": 15,
    "customer": "Mr. White"  
  }
}
\end{lstlisting}

\noindent
Note how this specification includes the actual data describing the particular product (the model), but also a
description of each of the elements (the meta model). With XML, a product would be modeled in a similar
fashion. An equivalent of the JSON above written in ATerm format would be the
following:

\begin{lstlisting}[morekeywords={}]
Order([ItemNumber(5), Quantity(15), Customer("Mr.\ White")])
\end{lstlisting}

\noindent 
However, this representation contains a lot of redundant information that also
exists in the grammar. Instead, such a product can be written as \ic{Order(5,
15, "Mr.\ White")}. This more concise notation tends to make it slightly more
convenient to use in handwritten transformations.

The textual notation of ATerms can be used for exchanging data between
tools and as a notation for model transformations or code generation
rules. In memory, ATerms can be stored in a tool-specific way
(i.e., simple Java objects in the case of Spoofax). The generic
structure and serializability of ATerms also allows them to be
converted to other data formats. For example, the \ic{aterm2xml} and \ic{xml2aterm}
tools can convert between ATerms and XML.

In addition to the basic elements above, ATerms support annotations to add
additional information to terms. These are similar to attributes in XML. For
example, it is possible to annotate a product number with its product name:
\begin{lstlisting}[morekeywords={}]
Order(5{ProductName("Apples")}, 15, "Mr. White")
\end{lstlisting}

\noindent 
Spoofax also uses annotations to add information about references to other
parts of a model to an abstract syntax tree. While ATerms only form trees,
the annotations are used to represent the graph-like references.

\subsection{MPS' Structure Definition}

In MPS, programs are trees/graphs of \emph{nodes}. A node is an instance of a
\emph{concept} which defines the structure, syntax, type system and semantics of
its instance nodes\footnote{The term concept used in this book to refer to
language constructs including abstract syntax, concrete syntax and semantics is
inspired by MPS' use of the term}. Like \ic{EClass}es\footnote{Nodes correspond
to \ic{EObject}s in EMF, concepts resemble \ic{EClass}es}, concepts are meta
circular, i.e.\ there is a concept that defines the properties of concepts:


\begin{lstlisting}[language=mps] 
concept ConceptDeclaration extends AbstractConceptDeclaration                          
                           implements INamedConcept            
                                                                                
  instance can be root: false                                                   
                                                                                
  properties:                                                                   
    helpURL : string 
    rootable : boolean                                                             
                                                                                
  children:         
    InterfaceConceptReference  implementsInterfaces       0..n                                                             
    LinkDeclaration            linkDeclaration            0..n                       
    PropertyDeclaration        propertyDeclaration        0..n               
    ConceptProperty            conceptProperty            0..n                       
    ConceptLink                conceptLink                0..n                               
    ConceptPropertyDeclaration conceptPropertyDeclaration 0..n 
    ConceptLinkDeclaration     conceptLinkDeclaration     0..n         
                                                                                
  references:                                                  
    ConceptDeclaration         extendsConceps             0..1
                                                                                
  concept properties:                                                           
    alias = concept                       
\end{lstlisting}

 
\noindent 
A concept may extend a single other concept and implement any number of
interfaces\footnote{Note that interfaces can provide implementations for the
methods they specify --- they are hence more like Scala traits or mixins
known from AOP and some programming languages.}.
It can declare references and child collections. It may also have a number of primitive-type
properties as well as a couple of "static" features. In addition, concepts can
have behavior methods.
                 
        
While the MPS structure definition is proprietary to MPS and does not implement
any accepted industry stanard, it is conceptually very close to
Ecore\footnote{This is illustrated by the fact the exporters and importers to
and from Ecore have been written.}.
                 
                                                                            
\section{Xtext Example}
\label{Sec:xtext-syntax}

Cooling programs\footnote{This and the other examples refer back to the case
studies introduced at the beginning of the book in section \todo{}} represent
the behavioral aspect of the refrigerator descriptions. Here is a trivial
program that can be used to illustrate some of the features of the language. The
program is basically a state machine.

\begin{lstlisting}[language=cooling] 
cooling program HelloWorld uses stdlib {
     
    var v: int
    event e
        
    init { set v = 1 }
       
    start:   
       on e { state s } 
       
    state s:
        entry { set v = 0 }
          
} 
\end{lstlisting}

\noindent 
The program declares a variable \ic{v} and an event \ic{e}. When the program
starts up, the \ic{init} section is executed, setting \ic{v} to \ic{1}. The
system then (automatically) transitions into the \ic{start} state. There it
waits until it receives the \ic{e} event. It then transition to the state
\ic{s}, where it uses an entry action to set \ic{v} back to \ic{0}. More complex
programs include checks of changes of properties of hardware elements
(\ic{aCompartment->currentTemp}) and commands to the hardware\ic{set aCompartment->isCooling = true}, as shown in the next snippet: 

\begin{lstlisting}[language=cooling] 
start:
    check ( aCompartment->currentTemp > maxTemp ) {
        set aCompartment->isCooling = true
        state initialCooling
    }
    check ( aCompartment->currentTemp <= maxTemp ) {
        state normalCooling
    } 
    
state initialCooling:
    check ( aCompartment->currentTemp < maxTemp ) {
        state normalCooling
    }
\end{lstlisting}


\parhead{Grammar basics} \marginnote[5mm]{It is also possible to first create
the Ecore meta model and then define a grammar for it. While this is a bit more
work, it is also more flexible because not all possible Ecore meta models can be
described implicitly by a grammar. For example, Ecore interfaces cannot be
expressed from the grammar. A middle ground is to have Xtext generate the meta
model while the grammar is still in flux and then switch to maintaining the meta
model manually when the grammar stabilizes.} In Xtext, the syntax is specified
using an EBNF-like grammar, a collection of productions that are typically
called \emph{parser rules}. These rules specify the concrete syntax of a program
element, as well as its mapping to the AS. From the grammar, Xtext generate the
abstract syntax represented in Ecore\footnote{The entity that contains the meta
classes is actually called an \ic{EPackage}.}. Here is the definition of the
\ic{CoolingProgram} rule:

\begin{lstlisting}[language=xtextgrammar] 
CoolingProgram:
    "cooling" "program" name=ID "{"
      (events+=CustomEvent |
        variables+=Variable)*
      (initBlock=InitBlock)?
      (states+=State)*
    "}";
\end{lstlisting} 

\noindent 
Rules begin with the name (\ic{CoolingProgram} in the example above), a colon,
and then the rule body. The body defines the syntactic structure of the language concept defined
by the rule. In our case, we expect the keywords \ic{cooling} and \ic{program},
followed by an \ic{ID}. \ic{ID} is a \emph{terminal rule} that is defined in the
parent grammar from which we inherit (not shown). \ic{ID} is defined as an
unbounded sequence of lowercase and uppercase characters, digits, and the
underscore, although it may not start with a digit. This terminal rule is
defined as follows:
\begin{lstlisting} [language=xtextgrammar]
terminal ID: ('a'..'z'|'A'..'Z'|'_') ('a'..'z'|'A'..'Z'|'_'|'0'..'9')*;
\end{lstlisting} 

\noindent 
In pure grammar languages, one would typically write the following: 

\begin{lstlisting} [language=xtextgrammar]
"cooling" "program" ID "\{ ..."} 
\end{lstlisting} 

This expresses that after the two keywords we expect an \ic{ID}.
However, Xtext grammars don't just express the concrete syntax --- they also
determine the mapping to the AS. We have encountered two such mappings so far.
The first one is implicit (although it can be made explicit as well): the name
of the rule will be the name of the derived meta class\todo{did we explain the
derivation of meta classes already? Otherwise we'd have to say this here}.
So we will get a meta class \ic{CoolingProgram}. The second mapping we have
encountered is \ic{name=ID}. It specifies that the meta class gets a property \ic{name} that
holds the contents of the \ic{ID} from the parsed program text. Since nothing
else is specified in the \ic{ID} terminal rule, the type of this property
defaults to \ic{EString}, Ecore's version of a string data type.

The rest of the definition of a cooling program is enclosed in curly braces. It
contains three elements: first the program contains a collection of events and
variables (the \ic{*} specifies unbounded multiplicity), an optional init
block (optionality is specified by the \ic{?}) and a list of states. Let
us inspect each of these in more detail. 

The expression \ic{(states+=State)*} specifies that there can be any number of
\ic{State} instances in the program. The \ic{CoolingProgram} meta class gets a
property \ic{states}, it is of type \ic{State} (the meta class derived from the
\ic{State} rule). Since we use the \ic{+=} operator, the \ic{states}
property will be typed to be a \emph{list} of \ic{State}s. In case of the
optional \ic{init} block, the meta class will have an \ic{initBlock} property,
typed as \ic{InitBlock} (whose parser rule we don't show here), with a
multiplicity of 0..1. Events and variables are more interesting, since the
vertical bar operator is used within the parentheses. The asterisk expresses
that whatever is inside the parentheses can occur any number of times --- note
that the use of a \ic{*} usually goes hand in hand with the use of a \ic{+=}.
Inside the parentheses we expect either a \ic{CustomEvent} \emph{or} a
\ic{Variable}, which is expressed with the \ic{|}. Variables are assigned to the
variables collection, events are assigned to the events collection.

This notation means that we can mix events and variables in any order. The
following alternative notation would first expect all events, and then all
variables. 
\begin{lstlisting}[language=xtextgrammar] 
      (events+=CustomEvent)*
      (variables+=Variable)*
\end{lstlisting} 

\noindent 
The definition of \ic{State} is interesting, since \ic{State} is intended to be an
abstract meta class with several subtypes.

\begin{lstlisting}[language=xtextgrammar] 
State:
    BackgroundState | StartState | CustomState;
\end{lstlisting} 

\noindent 
The vertical bar operator is used here to express syntactic alternatives. This
is translated to inheritance in the meta model. The definition of
\ic{CustomState} is shown in the following code snippet. It uses the set of
grammar language features explained above.

\begin{lstlisting}[language=xtextgrammar] 
CustomState:
    "state" name=ID ":"
        (invariants+=Invariant)*
        ("entry" "{"
            (entryStatements+=Statement)*
        "}")?
        ("eachTime" "{"
            (eachTimeStatements+=Statement)*
        "}")?
        (events+=EventHandler | signals+=SignalHandler)*;
\end{lstlisting}  

\noindent 
\ic{StartState} and \ic{BackgroundState}, the other two subtypes of \ic{State},
share some properties. Consequently, Xtext's AS derivation algorithm pulls them
up into the abstract \ic{State} meta class so they can be accessed polymorphically.
\fig{Fig:xtext-metamodel} shows the resulting meta model using EMF's tree view.
\todo{should this go to the beginning of the section?}
\begin{figure}[ht]
\fbox{
\begin{minipage}{104.5mm}
  \includegraphics[scale=0.7]{figures-impl/2/xtext-metamodel.png}
  \caption[]{Part of the Ecore meta model derived from the Xtext grammar
  for the cooling program. Grammar rules become \ic{EClass}es, assignments in
  the grammar rules become \ic{EProperties} (i.e. attributes and references).}
  \label{Fig:xtext-metamodel}  
\end{minipage}
}
\end{figure}




\parhead{References} Let us now look at statements and expressions. \ic{State}s
have entry and exit actions, procedural statements that are executed when a
state is entered and left, respectively. \ic{set v = 1} in the hello world program is an
example. \ic{Statement} itself is \ic{abstract} and has the various kinds
of statements as subtypes/alternatives:

\begin{lstlisting}[language=xtextgrammar] 
Statement:
    Statement | AssignmentStatement | PerformAsyncStatement | 
    ChangeStateStatement | AssertStatement; 

ChangeStateStatement:
    "state" targetState=[State];

AssignmentStatement:
    "set" left=Expr "=" right=Expr;
\end{lstlisting}  

\noindent 
The \ic{ChangeStateStatement} is used to transition into another state. It uses
the keyword \ic{state} followed by a reference to the actual target state.
Notice how Xtext uses square brackets to express the fact that the \ic{targetState}
property points to an \emph{existing} state as opposed to containing a new one
(which would be written as \ic{targetState=State}), i.e. the square brackets
leads to non-containing cross-references. 

This is another example of where the Xtext grammar language goes beyond
classical grammar languages, where one would write \ic{"state"
targetStateName=ID;}. Writing it this way only specifies that we expect an \ic{ID} 
after the \ic{state} keyword. The fact that we call it \ic{targetStateName}
communicates \emph{to the programmer} that we expect this text string to
correspond to the name of a state --- a later phase in model
processing \emph{resolves} the name to an actual state reference. Typically,
the code to resolve the reference has to be written manually, because there is
no way for the tool to automatically derive from the grammar that this \ic{ID}
is actually a reference to a \ic{State}. In Xtext, the \ic{targetState=[State]}
notation makes this explicit, so the resolution of the reference can be
automatic (taking into account manually written scopes, as discussed in the
next chapter). This notation also has the advantage that the resulting meta
class types the \ic{targetState} property to \ic{State} (and not just to a
string), which makes processing the models much easier.

Note that the cross-reference definition only specifies the target type
(\ic{State}) of the cross-reference, but not the concrete syntax of the
reference itself. By default, the \ic{ID} terminal is used for the reference
syntax, i.e. a simple (identifier-like) text string is expected. However, this
can be overridden by specifying the concrete syntax terminal behind a vertical
bar in the reference\footnote{Notice that in this case the vertical bar does not
represent an \emph{alternative}, it is merely used as a \emph{separator} between
the target type and the terminal used to represent the reference.}. In thefollowing piece of code, the \ic{targetState} reference uses the \ic{QID}terminal as the reference syntax.

\begin{lstlisting}[language=xtextgrammar] 
ChangeStateStatement:
    "state" targetState=[State|QID];

QID: ID ("." ID)*;
\end{lstlisting}  

\noindent 
The other remaining detail is scoping. During the linking phase, where the text
of \ic{ID} (or \ic{QID}) is used to find the target node, several objects with
the same name might exist, or some target elements are not visible based on visibility
rules of the language. To constrain the possible reference targets, scoping
functions are used. These will be explained in the next chapter.


\parhead{Expressions} The \ic{AssignmentStatement} shown earlier is one of the
statements that uses expressions. We repeat it here:

\begin{lstlising}[language=xtextgrammar]
AssignmentStatement:
  "set" left=Expr "=" right=Expr;
\end{lstlising}


\noindent The following snippet is a subset of the actual definition of
expressions (we have omitted some additional expressions that don't add anything to the description
here).

\begin{lstlisting}[language=xtextgrammar]  
Expr:
    ComparisonLevel;

ComparisonLevel returns Expression:
    AdditionLevel ((({Equals.left=current} "==") |
                    ({LogicalAnd.left=current} "&&") |
                    ({Smaller.left=current} "<"))
                                            right=AdditionLevel)?;

AdditionLevel returns Expression:
    MultiplicationLevel ((({Plus.left=current} "+") |
                         ({Minus.left=current} "-")) right=MultiplicationLevel)*;

MultiplicationLevel returns Expression:
    PrefixOpLevel ((({Multi.left=current} "*") |
                    ({Div.left=current} "/")) right=PrefixOpLevel)*;

PrefixOpLevel returns Expression:
    ({NotExpression} "!" "(" expr=Expr ")") |
    AtomicLevel;

AtomicLevel returns Expression:
    ({TrueLiteral} "true") |
    ({FalseLiteral} "false") |
    ({ParenExpr} "(" expr=Expr ")") |
    ({NumberLiteral} value=DECIMAL_NUMBER) |
    ({SymbolRef} symbol=[SymbolDeclaration|QID]);
\end{lstlisting}   

\todo{Kann man das ParensExpr weglassen? Siehe Mail vom SZ}

\noindent 
To understand the above definition, we first have to explain in more detail how
AST construction works in Xtext. Obviously, as the text is parsed, meta classes
are instantiated and the AST is assembled. However, instantiation of the
respective meta class happens only upon the first assignment to one of its
properties. If no assignment is performed at all, no object is created. For
example in grammar rule \ic{TrueLiteral: "true";} no instance of
\ic{TrueLiteral} will ever be created, because there is nothing to assign. In
this case, an action can be used to force instantiation: \ic{TrueLiteral:
\{TrueLiteral\} "true";}\footnote{Notice that the action can instantiate meta
classes other than those that are derived from the rule name (we could write
\ic{TrueLiteral: \{SomeOtherThing\} "true";}. While this would not make sense in
this case, we'll use this feature later.}. Unless otherwise specified, an
assignment such as \ic{name=ID} is always interpreted as an assignment on the
object that has been created most recently. The \ic{current} keyword can be used
to access that object in case it \emph{itself} needs to be assigned to a
property of another AST object.

Now we know enough about AST construction to understand how expressions are
encoded and parsed. In the expression grammar above, for the rules with the
\ic{Level} suffix, no meta classes are created, because (as Xtext is able to
find out statically) they are never instantiated. They merely act as a way to
encode precedence. To understand this, let's consider how \ic{2 * 3} is parsed:
\todo{can we show the resulting AST as it "grows" next to the text? maybe
number the items and then use these numbers in the diagram.}
\begin{itemize}
  \item The \ic{AssignmentStatement} refers to the \ic{Expr} rule in its
  \ic{left} and \ic{right} properties, so we "enter" the expression tree at the
  level of \ic{Expr} (which is the root of the expression hierarchy).
  \item The \ic{Expr} rule just calls the \ic{ComparisonLevel} rule, which calls
  \ic{AdditionLevel}, and so on. No objects are created at this point, since no
  assignment to any property is performed. 
  \item The parser "dives down" until it finds something that matches the
  first symbol in the parsed text: the \ic{2}. This occurs on \ic{AtomicLevel}, as
  it matches the \ic{DECIMAL\_NUMBER} terminal. At this point it creates an 
  instance of the \ic{NumberLiteral} meta class and assigns the number \ic{2} to
  the \ic{value} property. It also sets the \ic{current} object to point to the
  just created \ic{NumberLiteral}, since this is now the AST object created
  most recently.
  \item The \ic{AtomicLevel} rule ends, and the stack is unwound. We're back at
  \ic{PrefixOpLevel}, in the second branch. Since nothing else is specified
  after the call to \ic{AtomicLevel}, we unwind once more.
  \item We're now back at the \ic{MultiplicationLevel}. The rule 
  is not finished yet and we try to match a \ic{*} and a \ic{/}. The match on 
  \ic{*} succeeds. At this point the so-called assignment action on the left 
  side of the \ic{*} kicks
  in (\ic{{Multi.left=current}}). This action creates an instance of \ic{Multi},
  and assigns the \ic{current} (the \ic{NumberLiteral} created before) to its
  \ic{left} property. Then it makes the newly created \ic{Multi} the new 
  \ic{current}. At this point we have a subtree with the \ic{*} at the root, 
  and the \ic{NumerLiteral} in the left   property.
  \item The rule hasn't ended yet. We dive down to \ic{PrefixOpLevel} and
  \ic{AtomicLevel} once more, matching the \ic{3} in the same way as the two before.
  The \ic{NumerLiteral} for \ic{3} is assigned to the \ic{right} property as we
  unwind the stack.
  \item At this point we unwind the stack further, and since no more text is
  present, no more objects are created. The tree structure is as we had
  expected.
\end{itemize}
\todo{Can we make this paragraph more readable (Laurie)}
\noindent If we'd parsed \ic{4 + 2*3} the \ic{+} would have matched before the
\ic{*}, because it is "mentioned earlier" in the grammar. It is in a lower-precedence
group, the \ic{AdditionLevel}. Once we're a \ic{4 +}, we'd go down
again to match the \ic{2}. As we unwind the stack after matching the \ic{2} we'd
match the \ic{*}, creating a \ic{Multi} again. The \ic{current} at this point
would be the \ic{2}, so it would be put onto the \ic{left} side of the \ic{*},
making the \ic{*} the \ic{current}. Unwinding further, that \ic{*} would be put
onto the \ic{right} side of the \ic{+}, building the tree just as we'd expect.


Notice how a rule at a given level only always delegates to rules at higher
precedence levels. So higher precedence rules always end up further down in the
tree. If we want to change this, we can use parentheses (see the \ic{ParenExpr}
in the \ic{AtomicLevel}): inside those, we can again embed an \ic{Expr}, i.e.\ we
jump back to the lowest precedence level\footnote{This somewhat convolutedapproach to parsing expressions and encoding precedence is a consequence ofthe LL($k$) grammar class support by ANTLR, which underlies Xtext. We havediscussed this topic earlier \todo{ref}. Xtext also supports syntacticpredicates which are annotations in the grammar that tell the parser whichalternative to take in case of an ambiguity. We don't discuss this any furtherin the book.}.
Note that once you understand the basic approach, it is easy to add new
expressions with a precedence similar to another one (just add it as an
alternative to the respective \ic{Level} rule) or to introduce a new precedence
level (just interject a new \ic{Level} rule between two existing ones).



\section{Spoofax Example}

Mobl's \footnote{Mobl is a DSL for defining applications for mobile devices. It
is based on HTML 5 and is closely related to WebDSL, which has been introduced
earlier\todo{ref}.} data modeling language provides entities, properties and
functions. To illustrate the language, below are two data type definitions
related to a shopping list app. It supports lists of items that can be
favorited, checked, and so on, and are associated with some \ic{Store}.

\begin{lstlisting}[language=webdsl]
module shopping

entity Item {
  name     : String
  checked  : Bool
  favorite : Bool
  onlist   : Bool
  order    : Num
  store    : Store
}
\end{lstlisting}

\guido{I would expect more information on the semantics:
what is this header for? does it allow us to define namespaces?}

\noindent 
In mobl, most files starts with a module header, which can be followed by a list
of entity type definitions. In turn, each entity can have one or more property
or function definitions (shown in the next example snippet).


\parhead{Grammar basics} In Spoofax, the syntax of languages is described using
SDF\footnote{http://www.syntax-definition.org/}. SDF is short for Syntax
Definition Formalism and is a modular and flexible syntax definition formalism,
supported by the SGLR parser generator. It can generate efficient, Java-based scannerless and general parsers. An example of a
production written in SDF is

\guido{what is SGLR?}
\guido{In what way is SDF different from e.g. ANTLR}

\begin{lstlisting}[language=sdf]
"module" ID Entity* -> Start {"Module"}
\end{lstlisting}

\noindent 
The pattern on the left-hand side of the arrow is matched by the
symbol \ic{Start} on the right-hand side. Note that SDF uses the exact opposite
order for productions as the grammars we've discussed so far, switching the
left-hand and right-hand side. After the right-hand side, SDF productions may
specify annotations using curly brackets. Most productions specify a quoted
\emph{constructor label} that is used for the abstract syntax. This particular
production creates a tree node with the label \ic{Module}. The tree node willhave two children that represent the \ic{ID} and the list of \ic{Entities},respectively. In contrast to Xtext, these children are not named; instead,they are identified via the position in the child collection (the \ic{ID} isfist, the \ic{Entity} list is second).

\guido{Could you draw a SDF-based AST for a small example. From my
understanding there shouldn't be labels at the edges}

The left-hand side of an SDF production is the pattern it matches against.
SDF supports symbols, literals, and character classes in this pattern.
Symbols are references to other productions, such as \ic{ID}. Literals are
quoted strings such as \ic{"module"} that must appear in the input literally.
Character classes specify a range of characters expected in the input, e.g.\
\ic{[A-Za-z]} specifies that an alphabetic character is expected. We discuss
character classes in more detail below.

The basic elements of SDF productions can be combined using operators. The
\ic{A*} operator shown above specifies that zero or more occurrences of \ic{A}
are expected. \ic{A+} specifies that one or more are expected. \ic{A?} specifies
that zero or one are expected. \ic{\curlies{A B}*} specifies zero or more \ic{A}
symbols, separated by \ic{B} symbols are expected. As an example,
\ic{\curlies{ID ","}*} is a comma-separated list of identifiers.
\ic{\curlies{A B}+} specifies one or more \ic{A} symbols separated by \ic{B}
symbols.

\Figure{Fig:mobl-grammar-basic} shows an SDF grammar for a subset of mobl's
entities and functions syntax. The productions in this grammar should have few
surprises, but it is interesting to note how SDF groups a grammar in different
sections. First, the \ic{context-free start symbols} section indicates the start
symbol of the grammar. Then, the \ic{context-free syntax} section list the
context-free syntax productions, forming the main part of the grammar. Terminals
are defined in the \ic{lexical syntax} section.

\begin{figure}[ht]
\begin{center}
\begin{lstlisting}[language=sdf]
module MoblEntities

context-free start symbols

  Module
  
context-free syntax

  "module" ID Decl*                   -> Module {"Module"}
  "import" ID                         -> Decl   {"Import"}
  "entity" ID "{" EntityBodyDecl* "}" -> Decl {"Entity"}
  ID ":" ID                           -> EntityBodyDecl {"Property"}

  "function" ID "(" {Param ","}* ")" ":" ID "{" Statement* "}"
                                            -> EntityBodyDecl {"Function"}
  ID ":" ID                           -> Param {"Param"}
  "var" ID "=" Expr ";"               -> Statement {"Declare"}
  "return" Exp ";"                    -> Statement {"Return"}

  Exp "." ID "(" Exp  ")" -> Exp {"MethodCall"}
  Exp "." ID              -> Exp {"FieldAccess"}
  Exp "+" Exp             -> Exp {"Plus"}
  Exp "*" Exp             -> Exp {"Mul"}
  ID                      -> Exp {"Var"}
  INT                     -> Exp {"Int"}
  
lexical syntax

  [A-Za-z][A-Za-z0-9]* -> ID
  [0-9]+               -> INT
  [\ \t\n]             -> LAYOUT
\end{lstlisting}
  \caption[]{A basic SDF grammar for a subset of Mobl. The grammar
  does not yet specify the associativity, priority, or name bindings
  of the language.}
  \label{Fig:mobl-grammar-basic} 
\end{center}
\end{figure}
\guido{{Param ","}* looks wrong -> last parameter should not be followed by ","}

\parhead{Lexical syntax} As Spoofax uses a scannerless parser, all lexical
syntax can be customized in the SDF grammar. It provides default definitions for
common lexical syntax elements such as strings, integers, floats, whitespace and
comments. But when needed, custom syntax can be specified.
\guido{Where do the default defintiions come from? I can't find them in the
grammar.}

Most lexical syntax is specified using character classes such as \ic{[0-9]}.
Each character class is enclosed in square brackets, and can consist of ranges
of characters (\ic{c$_1$-c$_2$}), letters and digits (e.g.,\ \ic{x} or \ic{4}),
non-alphabetic literal characters (e.g.,\ \ic{\_}), and escapes (e.g.\
\ic{$\backslash{}$n}). A complement of a character class can be obtained using
the {\raise.17ex\hbox{$\scriptstyle\sim$}} operator, e.g.\
{\raise.17ex\hbox{$\scriptstyle\sim$}}\ic{[A-Za-z]} matches all non-alphabetic
characters. For whitespace and comments a special terminal \ic{LAYOUT} can be
used.

SDF implicitly inserts \ic{LAYOUT} in between all symbols in context-free
productions. This behavior is the key distinguishing feature between
context-free and lexical productions: lexical symbols such as identifiers and
integer literals cannot be interleaved with layout. The second distinguishing
feature is that lexical syntax productions usually do not have a constructor
label in the abstract syntax, as they form terminals in the abstract syntax
trees (i.e. they don't own any child nodes).

\parhead{Abstract syntax} To produce abstract syntax trees, Spoofax uses the
ATerm format, described in \Section{Sec:spoofax-aterm}. SDF combines the
specification of concrete and abstract syntax, primarily through the
specification of constructor labels. The way this works is most easily
understood using an example. Spoofax allows users to view the abstract syntax of
any input file. As an example, the following is the textual representation of an
abridged abstract syntax term for the shopping module shown at the beginning of
this section:

\begin{lstlisting}
Module(
  "shopping",
  [ Entity(
      "Item",
      [Property("name", "String"), Property("checked", "Bool"), ...]
    )
  ]
]) 
\end{lstlisting}

\noindent 
Note how this term uses the constructor labels of the syntax above:
\ic{Module}, \ic{Entity}, and \ic{Property}. The children of each node
correspond to the symbols referenced in the production: the \ic{Module}
production first referenced \ic{ID} symbol for the module name and then included
a list of \ic{Decl} symbols (lists are in square brackets).
In addition to constructor labels, productions that specify parentheses can
use the special \ic{bracket} annotation:  

\begin{lstlisting}[language=sdf]
"(" Exp ")" -> Exp {bracket}
\end{lstlisting}

\noindent 
The \ic{bracket} annotation specifies that there should not be a
separate tree node in the abstract syntax for the production. This means that an
expression \ic{1 + (2)} would produce \ic{Plus("1","2")} in the abstract syntax,
and not \ic{Plus("1",Parens("2"))}.


\parhead{Precedence and associativity} SDF provides special support for
specifying the associativity and precedence of operators or other
syntactic constructs. As an example, let us consider the production of the
\ic{Plus} operator. So far, it has been defined as
\begin{lstlisting}[language=sdf]
Exp "+" Exp -> Exp {"Plus"}
\end{lstlisting}

\noindent 
Based on this operator, a parser can be generated that can parse an
expression such as \ic{1 + 2} to a term \ic{Plus("1", "2")}. However, the
production does not specify if an expression \ic{1 + 2 + 3} should be parsed to
a term \ic{Plus("1", Plus("2", "3"))} or \ic{Plus(Plus("1", "2"), "3")}. If you
try the grammar in Spoofax, it will show \emph{both} interpretations using the
special \ic{amb} constructor:

\begin{lstlisting}
amb([
  Plus("1", Plus("2", "3")),
  Plus(Plus("1", "2"), "3")
)]
\end{lstlisting}

\noindent 
The \ic{amb} node indicates an \emph{ambiguity} and shows all
possible interpretations. Whenever an ambiguity is encountered in a file, it is
marked with a warning in the editor. 

Ambiguities can be resolved by adding annotations to the grammar that describe
the intended interpretation. For the \ic{Plus} operator, we can resolve the
ambiguity by specifying that it is left-associative, using the \ic{left}
annotation:

\begin{lstlisting}[language=sdf]
Exp "+" Exp -> Exp {"Plus", left}
\end{lstlisting}

\noindent 
In a similar fashion, SDF makes it possible to describe the precedence order of
operators. For this, the productions can be placed in a \ic{context-free
priorities} section:

\begin{lstlisting}[language=sdf]
context-free priorities
  Exp "*" Exp -> Exp {"Mul", left}
>
  Exp "+" Exp -> Exp {"Plus", left}
\end{lstlisting}

\noindent This example specifies that the \ic{Mul} operator has a higher
priority than the \ic{Plus} operator, resolving the ambiguity that arises for an
expression such as \ic{1+2*3}.

 
\parhead{Reserved keywords and production preference} Parsers generated with SDF
do not use a scanner, but include processing of lexical syntax in the parser.
Since scanners operate without any context information, they will simply
recognize any token that corresponds to a keyword in the grammar as a reserved
keyword, \emph{irrespective of its location in the program}. In SDF, it is also
possible to use keywords that are not reserved, or keywords that are only
reserved in a certain context. As an example, the following is a legal entity in
mobl\footnote{Note how we cannot provide syntax highlighting, since the Latex
Listing package \emph{cannot} deal with this case!}:

\begin{lstlisting}
entity entity {
}
\end{lstlisting}

\noindent 
Since our grammar did not specify that \ic{entity} is a reserved word, it can be
used as a normal \ic{ID} identifier. However, there are cases where it is useful
to reserve keywords, for example to prevent ambiguities. Consider what would
happen if we added a new production for a \ic{true} literal:

\begin{lstlisting}[language=sdf]
"true" -> Exp {"True"}
\end{lstlisting}
 
\noindent 
If we would now parse an expression \ic{true}, it would be ambiguous:
it matches the \ic{True} production above, but it also matches the \ic{Var}
production, as \ic{true} is a legal variable identifier\footnote{So it is
ambiguous because \emph{at the same location in a program} both interpretations
are possible.}. Keywords can be reserved in SDF by using a production that
rejects a certain interpretation:

\begin{lstlisting}[language=sdf]
"true" -> ID {reject}
\end{lstlisting}

\noindent 
This expresses that \ic{true} can never be interpreted as an identifier.
Alternatively, we could say that we prefer the one interpretation over the
other:

\begin{lstlisting}[language=sdf]
"true" -> Exp {"True", prefer}
\end{lstlisting}

\noindent 
This means that this production is to be preferred if there are any
other interpretations. However, since these interpretations cannot always be
foreseen as grammars are extended, it is considered good practice to use the
more specific \ic{reject} approach instead\footnote{This is the situationwhere a projectional editor like MPS is more flexible, since instead ofrunning into an ambiguity, it would prompt the user to decide whichinterpretation is correct as he types \ic{true}.}.

\parhead{Longest match} Most scanners apply a \emph{longest match} policy for
scanning tokens. This means that if it is possible to include the next character
in the current token, the scanner will always do so. For most languages, this is
the expected behavior, but in some cases longest match is not what users expect.
SDF instead allows the grammar to specify the intended behavior. In Spoofax, the
default is specified in the \emph{Common} syntax module using a \ic{lexical
restrictions} section:

\begin{lstlisting}[language=sdf]
lexical restrictions
  ID -/- [A-Za-z0-9]
\end{lstlisting}

\noindent 
This section restricts the grammar by specifying that any \ic{ID} cannot be
directly followed by a character that matches \ic{[A-Za-z0-9]}. Effectively, it
enforces a longest match policy for the \ic{ID} symbol. SDF also allows the use
of lexical restrictions for keywords. By default it does not enforce longest
match, which means it allows the following definition of a mobl entity:

\begin{lstlisting}
entityMoblEntity {}
\end{lstlisting}

\noindent 
As there is no longest match, the parser can recognize the \ic{entity} keyword
even if it is not followed by a space. To avoid this behavior, we can specify a
longest match policy for the \ic{entity} keyword:

\begin{lstlisting}[language=sdf]
lexical restrictions
  "entity" -/- [A-Za-z0-9]
\end{lstlisting}
 
\parhead{Name bindings} So far we have discussed purely syntax specification in
SDF. Spoofax also allows the specification of name binding annotations in SDF
grammars, which specify semantic relations between productions. We discuss how
these relations are specified in \Chapter{Ch:scopes}.

 
\section{MPS Example}

We start by defining a simple language for state machines, roughly similar to
the one used in the mbeddr extensible C\footnote{http://mbeddr.com}. Core
concepts include \ic{StateMachine}, \ic{State}, \ic{Transition} and
\ic{Trigger}. The state machine can be embedded in C code as we will see later.
The language supports the definition of state machines as shown in the following
piece of code:

\begin{lstlisting}[language=mbeddr]
module LineFollowerStatemachine { 
   
  statemachine LineFollower {                                                              
    events unblocked()                                                               
           blocked()                                                                 
           bumped()                                                                  
           initialized()                                                            
    states (initial = initializing) {   
      state initializing {                                                       
        on initialized [ ] -> running {  } 
      }                          
      state paused { 
        on unblocked [ ] -> running {  } 
      }                                  
      state running { 
        on blocked [ ] -> paused {  } 
        on bumped [ ] -> crashed {  } 
      }   
      state crashed { 
      }
   }                                                                  
  }
}                                                                             
\end{lstlisting}


\parhead{Concept Definition} MPS is projectional, so we start with the
definition of the AS. In MPS, AS elements are called concepts. The code below
shows the definition of the concept \ic{Statemachine}. It contains a collection
of \ic{State}s and a collection of \ic{InEvent}s. It also contains a reference
to one of the states to mark it as the \ic{initial} state. The \ic{alias} is
defined as \ic{statemachine}, so typing this word inside C modules instantiates
a state machine (it picks the \ic{Statemachine} concept from the code completion
menu). State machines also implements a couple of interfaces;
\ic{IIdentifierNamedElement} contributes a property \ic{name},
\ic{IModuleContent} makes the state machine embeddable in C \ic{Module}s --- the
module owns a collection of \ic{IModuleContents}, just like the state machine
contains states and events.

\begin{lstlisting}[language=mps]
concept Statemachine extends BaseConcept                            
                     implements IModuleContent                      
                                ILocalVarScopeProvider
                                IIdentifierNamedElement              
  children:                                                         
    State   states   0..n                             
    InEvent inEvents 0..n                         
                                                                    
  references:                                                       
    State   initial  1                               
                                                                    
  concept properties:                                               
    alias = statemachine                                              
\end{lstlisting}

\noindent 
A \ic{State} contains two \ic{StatementLists} as \ic{entryActions} and
\ic{exitActions}. \ic{StatementList} is a concept defined by the
\ic{com.mbeddr.core.statements} language. To make that visible, our statemachine 
language extends \ic{com.mbeddr.core.statements}. Finally, a \ic{State}
contains a collection of \ic{Transition}s.

\begin{lstlisting}[language=mps]
concept Transition     
  children:                             
    Trigger        trigger  1 
    Expression     guard    1
    StatementList  actions  1
                                        
  references:                           
    State          target   1     
                                        
  concept properties:                   
    alias = on                    
\end{lstlisting}


\noindent 
\ic{Transition}s contain a \ic{Trigger}, a guard condition, transition actions
and a reference to the target state. The trigger is an abstract concept;
various specializations are possible, the default implementation is the
\ic{EventTrigger}, which references an \ic{Event}. The guard condition is an
\ic{Expression}, a concept reused from \ic{com.mbeddr.core.expressions}. A type
system rule will be defined later to constrain this expression to be Boolean.
The target state is a reference, i.e. we point to an existing state instead of
owning it. \ic{actions} is another \ic{StatementList} that can contain arbitraryC statements used as the transition actions.

\parhead{Editor Definition} Editors, i.e. the projection rules, are made of
cells. When defining editors, various cell types are arranged so that the
resulting syntax has the desired structure. \fig{Fig:mps-stateeditor} shows the
editor definition for the \ic{State} concept. It uses an \ic{indent} collection
of cells with various style attributes to arrange the \ic{state} keyword and
name, the entry actions, the transitions and the exit actions in a vertical
list. Entry and exit actions are shown only if the respective \ic{StatementList}
is not empty (a condition is attached to the respective cells, marked by the
\ic{?} in front of the cell). An intention is used (see next section) to add a
new statement and hence make the respective list visible.
\begin{figure}[ht]
\fbox{
\begin{minipage}{104.5mm}
  \includegraphics[width=10cm]{figures-impl/2/mps-stateeditor.png}
  \caption[]{The definition of the editor for the \ic{State} concept.
  In MPS, editors are made from cells. In the editor definition your arrange
  the cells and define what they project; this defined the projection rule that
  is then used when instances of the concept are edited.}
  \label{Fig:mps-stateeditor}  
\end{minipage}
} 
\end{figure}


\noindent 
\fig{Fig:mps-transeditor} shows the definition of the editor for a
\ic{Transition}. It arranges the keyword \ic{on}, the trigger, the guard
condition, target state and the actions in a horizontal list of cells, the guard
surrounded by brackets, and an arrow (\ic{->}) in front of the target state. The
editor for the actions \ic{StatementList} comes with its own set of curly
braces. 

\todo{Fix image}
\begin{figure*}[ht]
\fbox{
\begin{minipage}{104.5mm}
  \includegraphics[width=16cm]{figures-impl/2/mps-transeditor.png}
  \caption[][-30mm]{The editor for transitions. Note how we embed the
  guard condition expression simply by referring to the \ic{guard} child
  relationship. We "inherit" the syntax for expressions from the
  \ic{com.mbeddr.core.expressions} language.}
  \label{Fig:mps-transeditor}  
\end{minipage}
} 
\end{figure*}
\noindent The \ic{\%targetState\% -> \{name\}} part is interesting; it expresses
that in order to render the target state, the target's state's \ic{name} attribute
should be shown. We could use any text string to refer to the target state\footnote{We could even use the symbol \ic{X} to render \emph{all} target state references.
The reference would still work, because the underlying data structure uses the
target's unique ID to establish the reference. It does not matter what we use to
represent the target in the model. Using \ic{X} for all references would of
course be bad for human readability, but technically it would work}.

Note how we use \ic{on} both as the leading keyword for a transition and as the
alias. This way, if a user types the \ic{on} alias to instantiate a transition,
it feels as if she would just type the leading keyword of a transition (as in a
regular text editor). 

If a language extension defined a new concept \ic{SpecialKindOfTransition}, they
could use another alias to uniquely identify this concept in the code completion
menu. When the user enters a transition, he has to decide which alias to use,
depending on whether he wants to instantiate a \ic{Transition} or a
\ic{SpecialKindOfTransition}. Alternatively, the \ic{SpecialKindOfTransition}
could use \emph{the same alias} \ic{on}. In this case, if the user types
\ic{on}, the code completion menu pops open and the user has to decide which of
the two concepts to instantiate\footnote{The code completion menu by default
shows from which language a language concept originates, so this is a way to
distinguish the two. Alternatively, a short explaining text can be shown for
each entry in the code completion menu that helps the user make the decision.}.
As we have discussed above, this means that there is never an ambiguity that
cannot be handled --- as long as the user is willing and able to make the
decision which concept should be instantiated.


\todo{Once tables work well in MPS: add a table projection to show off non
textual notations}

\begin{marginfigure}
  \includegraphics[width=3.5cm]{figures-impl/2/intention.png}
  \caption{The intentions menu for a local variable declaration. It can be
  opened via \ic{Alt-Enter}. Note that, to select an action from the menu, you
  can just start typing the action label, so this is very keyboard-friendly.}
  \label{intention} 
\end{marginfigure} 
\parhead{Intentions} Intentions are MPS' term for what is otherwise known as a
Quick Fix: a little menu can be popped up on a program element that contains a
set of actions that typically change the underlying program element (see
\fig{intention}). In MPS the intentions menu is opened via \ic{Alt-Enter}. In MPS, intentions play an
important role in the editor. Some changes to the program can \emph{only} be
made via an intention\footnote{This is mostly because building a just-type-along
solution would be a lot of work in a projectional editor in some cases.}. For
example, in the previous section we mentioned that we use them to add entry actiom statements to
a \ic{State}. Here is the intention code:

\begin{lstlisting}[language=mps]
intention addEntryActions for concept State {                                                                                             
  error intention : false                                                                                                                 
  available in child nodes : true                                                                                                          
                                                                                                                                          
  description(editorContext, node)->string { 
    "Add Entry Action"; 
  }                                                                    
                                                                                                                                          
  isApplicable(editorContext, node)->boolean { 
    node.entryAction.isNull; 
  }                                                             
                                                                                                                                          
  execute(editorContext, node)->void { 
    node.entryAction.set new(<default>); 
    editorContext.selectWRTFocusPolicy(node.entryAction); 
  }
}
\end{lstlisting}

\noindent 
An intention is defined for a specific language concept (\ic{State} in the
example). It can then be invoked by pressing \keystroke{Alt-Enter} on any
instance of this concept. Optionally it is possible to also make it available in
child nodes. For example, if you are in the guard expression of an transition,
an intention for \ic{State} with \ic{available in child nodes} set to \ic{true}
will be available as well. The intention implementation also specifies an
expression used as the title in the menu and an applicability condition. In the example the
intention is only applicable if the corresponding state does not yet have any
entry action. Finally, the \ic{execute} section contains procedural code that
performs the respective change on the model. In this case we simply create a new
instance of \ic{StatementList} in the \ic{entryAction} child. We also set the
cursor into this new \ic{StatementList}. 

Notice how we don't have to specify any formatter or serializer for our
language. Remember how a projectional editor \emph{always} goes from AS to CS.
So after changing the AS procedurally, the respective piece of the tree is
simply rerendered to update the representation of the program in the editor.
However, we do have to define an editor for each language concept\footnote{Using
Xtext's automatic derivation of the AS from the grammar, defining simple
languages is faster with Xtext. However, when one switches to a manually
maintained AS \ic{EPackage}, then the efforts are comparable.}.



\parhead{Expressions} Since we inherit the expression structure and
syntax from the C core language, we don't have to define expressions ourselves
so we can use them in guards. It is nonetheless interesting to look at their
implementation in \ic{com.mbeddr.core.expressions}.

Expressions are arranged into a hierarchy starting with the abstract concept
\ic{Expression}. All other kinds of expressions extend \ic{Expression}, directly
or indirectly. For example, \ic{PlusExpression} extends \ic{BinaryExpression}
which in turn extends \ic{Expression}. \ic{BinaryExpressions} have \ic{left} and
\ic{right} child \ic{Expressions}. This way, arbitrarily complex expressions can
be built. Representing expressions as trees is a standard approach that we have
seen with the Xtext example already; in that sense, the abstract syntax of
mbeddr expressions (and more generally, the way to handle expressions in MPS) is
not very interesing. The editors are also trivial --- in case of the \ic{+}
expression, they are a horizontal list of: editor for \ic{left} argument, the
\ic{+} symbol, and the editor for the \ic{right} argument.

As we have explained in the general discussion about projectional editing
\todo{ref}, MPS supports linear input of hierarchical expressions using
so-called side transforms. The code below shows the right side transformation for
expressions that transforms an arbitrary expression into a \ic{PlusExpression}
by putting the \ic{PlusExpression} "on top" of the current node. 

\begin{lstlisting}[language=mps, morekeywords={node, add, custom, items,
output, tag, simple, item, matching, text, do, right}] 
side transform actions makeArithmeticExpression                                                                                                                                                                                                                                  
 
  right transformed node: Expression tag: default_                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                 
  actions :                                                                                                                                                                                                                                                                      
    add custom items  (output concept: PlusExpression)                                                                                                                                                                                                                           
      simple item                                                                                                                                                                                                                                                                
        matching text                                                                                                                                                                                                                                                            
          +                                                                                                                                                                                                                                                                      
        do transform                                                                                                                                                                                                                                                             
          (operationContext, scope, model, sourceNode, pattern)->node< > { 
            node<PlusExpression> expr = new node<PlusExpression>(); 
            sourceNode.replace with(expr); 
            expr.left = sourceNode; 
            expr.right.set new(<default>); 
            return expr.right; 
          }                    
\end{lstlisting}


\noindent 
Using the alias (i.e. the operator symbol) of the respective
\ic{BinaryExpression} and the inheritance hierarchy, it is possible to factor
all side transformations for all binary operations into one single action
implementation, resulting in much less implementation effort.
The fact that you can enter expressions linearly, leads to a problem not unlike
the one found in grammars regarding operator precedence. If you enter \ic{2 + 3
* 4} by typing these characters sequentially, there are two ways how the tree
could look, depending on whether \ic{+} or \ic{*} binds more
tightly\footnote{Note how this really is a consequence of the linear input
method; you could build the tree by first typing the \ic{+} and then filling in
the left and right arguments, in which case it would be clear that the \ic{*}
is lower in the tree and hence binds tighter. However, this is tedious and
hence not an option in practice.}.

To deal with this problem, we proceed as follows: each subconcept of
\ic{BinaryExpression} has a numerical value associated with it that expresses
its precedence. The higher the number, the higher the precedence (i.e. the
lower in the tree). The action code shown above is change to include a call to a
helper function that rearranges the tree according to the precedence values.  

\begin{lstlisting}[language=mps, morekeywords={do}]
do transform                                                                                                                                                                                                                                                             
  (operationContext, scope, model, sourceNode, pattern)->node< > { 
    node<PlusExpression> expr = new node<PlusExpression>(); 
    sourceNode.replace with(expr); 
    expr.left = sourceNode; 
    expr.right.set new(<default>); 
    PrecedenceHelper.rearrange(expr);
    return expr.right; 
  }
\end{lstlisting}

\noindent 
This helper function scans through an expression tree and checks for cases where
a binary expression with a higher precedence is an ancestor of a binary
expression with a lower precedence value. If it finds one, it simply rearranges
the tree to resolve the problem. Since the problem can only arise as a
consequence of the linear input method, it is sufficient to include this
rearrangement in the side transformation shown above. 


\parhead{Context Restrictions} MPS makes strong use of polymorphism. If a
language concept defines a child relationship to another concept \ic{C}, then
any subtype of {C} can also be used in this child relationship. For example, a
function has a \ic{body} which is typed to \ic{StatementList}, which contains a
list of \ic{Statement}s. So every subtype of \ic{Statement} can be used inside a
function body. In general, this is the desired behavior, but in some cases, it
is not. Consider test cases. Here is a simple example:

\begin{lstlisting}[language=mbeddr]
module UnitTestDemo imports nothing { 
   
  test case testMultiply { 
    assert (0) times2(21) == 42; 
  } 
   
  int8 times2(int8 a) { 
    return 2 * a; 
  } 
}
\end{lstlisting} 

\noindent 
Test cases are defined in a separate language \ic{com.mbeddr.core.unittest}. The
language defines the \ic{TestCase} concept, as well as the \ic{assert}
statement. \ic{AssertStatement} extends \ic{Statement}, so by default, an
\ic{assert} can be used wherever a \ic{Statement} is expected, once the
\ic{com.mbeddr.core.unittest} is used in a program. However, this is not what we
want: \ic{assert} statements should be restricted to be used inside a
\ic{UnitTest}\footnote{This is, among other reasons, because the transformation
of the \ic{assert} statement to C expects code generated from the \ic{UnitTest}
to surround it}. To support such a use case, MPS supports a set of constraints.
Here is the implementation for \ic{AssertStatement}:

\begin{lstlisting}[language=mps]
concept constraints AssertStatement { 
  can be child 
    (operationContext, scope, parentNode, link, childConcept)->boolean { 
      parentNode.ancestor<concept = TestCase, +>.isNotNull; 
    } 
}
\end{lstlisting}

\noindent 
This constraint checks that a \ic{TestCase} is among the ancestors of a
to-be-inserted \ic{AssertStatement}. The constraint is checked \emph{before} the
new \ic{AssertStatement} is inserted and \emph{prevents} insertion if not under
a \ic{TestCase}. The constraint is written from the perspective of the potential
child element. 

For reasons of dependency management, it is also possible to write the
constraint from the perspective of the parent or an ancestor. This is useful if
a new container concept wants to restrict the use of \emph{existing} child
concepts without changing those concepts. For example, the \ic{Lambda} concept,
which contains a statement list as well, prohibits the use of
\ic{LocalVariableRef}s, in any of its statements.


\todo{Do we want to have wrap-up for each chapter and compare things?}