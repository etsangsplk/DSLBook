\section{Testing DSLs}

DSL testing is a multi-faceted problem. The following aspects of a DSL need to
be tested:

\todo{Better argue, which kinds of tests are necessary: syntax, constraints/TS,
semantics are in a natural order. Better check semantics than generated text.
But this is an integration test. Maybe test intermediate results? That would be
a structural test. Try to avoid it. Look at negative and positive testing in
each of these cases. Maybe add additional lang constructs to "inspect" internal
state. Allow this only in tests, not anywhere else (better than making
everything public\ldots). Testing semantics works only if executable. And
target env is required! Is some kind of integration test. Maybe simulate? Mock?
the whole simualtion story goes here as well. And interpreter. BSH.}

\begin{itemize}
  \item can the syntax cover all required sentences? Is the concrete syntax
  "correct"?
  \item do the scopes work correctly?
  \item do the constraints work? are all "wrong" programs actually detected, and
  is the right error message attached to the right program element?
  \item are the semantics correct? do transformations, generators and
  interpreters work correctly?
  \item can all programs relevant to the users actually be expressed?
  Does the language cover the complete domain?
\end{itemize}



\subsection{Syntax Testing}

Testing the syntax is simple. Developers simply try to write all relevant
programs and see if they can be expressed with the language. 

The following piece of code is the fundamental code that needs to be written in
Xtext to test a DSL program using the Xtext testing
utilities\footnote{http://code.google.com/a/eclipselabs.org/p/xtext-utils/wiki/Unit\_Testing}. 
It is a JUnit 4 test case with special support for the Xtext infrastructure.

\begin{code}
@RunWith(XtextRunner.class)
@InjectWith(CoolingLanguageInjectorProvider.class)
public class InterpreterTests extends XtextTest {

    @Test 
    public void testET0() throws Exception {
        testFileNoSerializer("interpreter/engine0.cool", "tests.appl", "stdparams.cool" ); 
    }
    
}                                                                                                                                                                                                               
\end{code}


The single test method loads the \ic{interpreter/engine0.cool} program, as
well as two more files which contain elements referenced from
\ic{engine0.cool}. The \ic{testFileNoSerializer} method loads the file, parses it,
and checks constraints. If either parsing or constraint checking fails, the test
fails. There is also a \ic{testFile} method which, after loading and parsing the
file, reserializes the AST to the text file, writes it back, and loads it again,
the comparing the two ASTs. This way, the (potentially adapted) formatter is
tested.

On a more fine grained level it is often useful to test partial sentences
instead of complete sentences or programs. The following piece of Xtext example
code tests the \ic{CustomState} parser rule:


\begin{code}
    @Test 
    public void testStateParserRule() throws Exception {
        testParserRule("state s:", "CustomState" );
        testParserRule("state s: entry { do fach1->anOperation }", "CustomState" );
        testParserRule("state s: entry { do fach1->anOperation }", "State" );
    }
\end{code}

\marginnote{Note that these unit tests do not at all depend on the Eclipse UI or
the Xtext editor, so they can be run from the command line or any other way of
running JUnit tests. They only need a couple of Xtext libraries to be on the
classpath. Consequently, it is simple to integrate these tests with continuous
regression test infrastructures or a build server.} The first line asserts that
the string \ic{state s:} can be parsed with the \ic{CustomState} parser rule.
The second line passes in a more complex state, one with a command in an entry
action. Line three tries the same text with the \ic{State} rule, which itself
calls the \ic{CustomState}. Notice that these tests really just test the parser.
No linking or constraints checks are performed. This is why we can "call"
\ic{anOperation} on the \ic{fach1} object, although \ic{anOperation} is not
defined as a callable operation anywhere.



\subsection{Constaints Testing}

Especially for languages with complex constraints, such as those implied by type
systems, testing of constraints is essential. A special API is necessary to be
able to verify that a program which makes a particular constraint fail actually
annotates the corresponding error message to the respective program element.
This way, tests can then be written which assert that a given program has a
specific set of error annotations. 

The unit testing utilities mentioned above also support testing constraints. The
utilities come with an internal Java DSL that support checking for the presence
of error annotations after parsing and constraint-checking a model file.

\begin{code}
    @Test 
    public void testTypesOfParams() throws Exception {
        testFileNoSerializer("typesystem/tst1.cool", "tests.appl", "stdparams.cool");
        assertConstraints( issues.sizeIs(3) );                             // 1
        assertConstraints( issues.forElement(Variable.class, "v1").        // 2  
          theOneAndOnlyContains("incompatible type") );                    // 2
        assertConstraints( issues.under(Variable.class, "w1").             // 3
          errorsOnly().sizeIs(2).oneOfThemContains("incompatible type") ); // 3
    }
\end{code}


We first load the model file that constains constraint errors (in this case,
type system errors). Then we assert the total number of errors in the file to be
three (line 1). Next, in line 2, we check that the instance of \ic{Variable} named \ic{v1}
has exactly one error annotation, and it has the text "incompatible type" in the
error message. Finally, in line 3 we assert that there are exactly two errors
anywhere under (i.e. in the subtree below) a \ic{Variable} named \ic{w1}, and one of these
contains "incompatible type" in the error message. Using the fluent API style
shown by these examples, it is easy to express errors and their locations in the
program. If a test fails, a meaningful error message is output that supports
localizing (potential) problems in the test. The following is the error message
if no error message is found that contains the substring "incompatible type":


\begin{code}
junit.framework.AssertionFailedError: <no id> failed
  - failed oneOfThemContains: none of the issues 
    contains substring 'incompatible type' 
  at junit.framework.Assert.fail(Assert.java:47)
  at junit.framework.Assert.assertTrue(Assert.java:20)
  ...
\end{code}


If the test fails earlier in the filter expression, more than one output is
provided:


\begin{code}
junit.framework.AssertionFailedError: <no id> failed
  - no elements of type 
    com.bsh.pk.cooling.coolingLanguage.Variable named 'v1' found 
  - failed oneOfThemContains: none of the issues 
    contains substring 'incompatible type'
  at junit.framework.Assert.fail(Assert.java:47) 
  ...
\end{code}


A similar facility is available for MPS, it is called a \ic{NodesTestCase}
(\fig{mps-testexample}). It supports special annotations to express assertions
on types and errors, directly in the program. For example, the third line of the
nodes section reads \ic{var double d3 = d} without annotations. This is a valid
variable declaration in mbeddr C. After this has been written down, annotations
can be added. They are rendered in green. Line three asserts that the type of the
variable \ic{d} is \ic{double}, i.e. it tests that variable references assume the type of
the referenced variable. In line four we assign a \ic{double} to an \ic{int}, which is
illegal according to the typing rules. The error is detected, hence the red
squiggly. We use another annotation to assert the presence of the error.

In addition to using these annotations to check models, developers can also
write more detailed test cases. In the example I assert that the \ic{var} reference
of the node referred to as \ic{dref} points to the node referred to as \ic{dnode}. Note
how labels (green, underlined) are used to add names to program elements so they
can be referred to from test expressions.

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.65]{figures/7/mps-test.jpg}
  \caption[]{}
  \label{mps-testexample} 
\end{center}
\end{figure}

Scopes can be tested in the same way: we can write example programs where
references point to valid targets (i.e. those in scope) and invalid targets
(i.e. not in scope). Valid references may not have errors, invalid referneces
must have errors. But using the facilities shown above, which check for the
presence of error annotations, scopes can be tested. Some frameworks may provide
direct access to the scoping API to test scopes more directly.


\subsection{Semantics Testing}

Fundamentally, testing the execution semantics of a program involves the
writing constraints against the \emph{execution} of a program as it is executed.
In the simplest case this can be done the following way:

\begin{itemize}
  \item write a DSL program, based on an understanding what this program is
  expected to do
  \item generate the program into its executable representation
  \item \emph{manually} write unit tests that assert the generated program's
  behavior based on the understanding of what the DSL program should do
\end{itemize}

Notice how we do \emph{not} test the structure or syntax of the generated
artifact. Instead we test it's meaning, which is exactly what we want to test.
An important variation of this appraoch is the following: instead of writing the
unit tests manually in the target language, we can also write the tests in the
DSL, assuming the DSL has syntax to express such tests\footnote{Many DSLs are
explicitly extended to support this use case}. Writing the test cases on DSL
level results in more concise and readable tests.\marginnote{Testing DSL
programs by running tests expressed in the same language runs the risk of
doubly-negating errors. If the generators for the tests and the core program in
a compatible way, errors in either one may not be found. However, this problem
can be alleviated by running a large enough number of tests.}

The same approach can be used to test execution semantics based on an
interpreter, although it may be a little bit more difficult to manually write
the test cases in the target language; the interpreter must provide a means to
"inspect" its execution so we can check if it is correct. If the tests are
written in the DSL and the interpreter executes them along with the core
program, the approach works well.

Strictly speaking, this approach tests the semantics of a \emph{specific}
program. As always in testing, we have to write many of these tests to make sure
we have covered all of the possible executions paths through a generator or
interpreter. If we do that, the set of tests implicitly tests the generator or
interpreter --- which is the goal we want to achieve in semantics testing.

If we have several execution backends, such as an interpreter \emph{and} a
compiler it must be ensured that both have the same semantics. This can be
achieved by writing the tests in the DSL and then executing them in \emph{both}
backends. By executing enough tests, the semantic equivalence of the backends
can be ensured.


\parhead{Testing an Interpreter with Xtext} The cooling language provides a way
to express test cases for the cooling programs within the cooling language
itself. These tests are executed with an interpreter inside the IDE, and they
can also be executed on the level of the C program. Note that this approach
cannot just be used for testing interpreters or generators, it can also be used
to test whether a program written in the DSL works correclty. This is in fact
why the interpreter and the test sub-language have been built in the first
place: DSL users should be able to test the programs written in the DSL.

The following code shows one of the simplest possible cooling programs, as well
as a test case for that program:


\begin{code}
cooling program EngineProgram0 for Einzonengeraet uses stdlib {
         
    var v: int
    event e1
        
    init { set v = 1 }
       
    start:   
       entry { set v = v * 2 }
       on e1 { state s2 } 
           
    state s2:
        entry { set v = 0 }
          
} 


test EngineTest0 for EngineProgram0 {
    assert-currentstate-is ^start  // 1
    assert-value v is 2            // 2
    step                           // 3
    event e1                       // 4
    step                           // 5
    assert-currentstate-is s2      // 6
    assert-value v is 0            // 7
}
\end{code}


The test first asserts that, as soon as the program starts, it is in the
\ic{start} state (line 1). We then assert that \ic{v} is \ic{2}. The only
reasonable way how \ic{v} can become \ic{2} is that the code in the \ic{init}
block as well as the code in the entry action of the \ic{start} start have been
executed. Note that this arrangement even checks that the init block is executed
before the entry action of the start state, since otherwise \ic{v} would be
\ic{1}!. We then perform one step in the execution of the program (the language
is stepped) in line 3. At this point nothing should happen, since no event was
triggered. Then we trigger the event \ic{e1} (line 4) and perform another
\ic{step} (line 5). After this step, the program must transition to the state
\ic{s2}, whose entry action sets \ic{v} back to 0. We assert both of these
(lines 6 and 7).

These tests can be run interactively from the IDE, in which case assertion
failures are annotated as error marks on the program, or from within JUnit. 
The following piece of code shows how to run the tests from JUnit.


\begin{code}
@RunWith(XtextRunner.class)
@InjectWith(CoolingLanguageInjectorProvider.class)
public class InterpreterTests extends PKInterpreterTestCase {

    @Test 
    public void testET0() throws Exception {
        testFileNoSerializer("interpreter/engine0.cool", "tests.appl", "stdparams.cool" ); 
        runAllTestsInFile( (Model) getModelRoot());
    }
}
\end{code}

This is basically a JUnit test that inherits from a base class that helps with
loading models and running the interpreter. We call the \ic{runAllTestsInFile}
method, passing in the model's root element. \ic{runAllTestsInFile}
is defined by the \ic{PKInterpreterTestCase} base class, which in turn inherits from
\ic{XtextTest}, which we have seen before. The method iterates over all tests in the
model and executes them by creating and running a \ic{TestExecutionEngine}. The
\ic{TestExecutionEngine} is a wrapper around the interpreter for cooling
programs that we have discussed before.


\begin{code}
protected void runAllTestsInFile(Model m) {
    CLTypesystem ts = new CLTypesystem();
    EList<CoolingTest> tests = m.getTests();
    for (CoolingTest test : tests) {
        TestExecutionEngine e = new TestExecutionEngine(test, ts);
        final LogEntry logger = LogEntry.root("test execution");
        LogEntry.setMostRecentRoot(logger);
        e.runTest(logger);    
    }
}
\end{code}

The cooling programs are generated to C for execution in the refrigerator. To
make sure the generated C code has the same semantics as the interpreter, we
simply generate C code from the test cases as well. This way, the same tests are
executed against the generated C code. By ensuring that all of them work in the
interpreter and the generator, we ensure that both behave the same way.


\parhead{Testing a Generator with MPS} The following is a test case expressed
using the testing extension to mbeddr C. It contributes \ic{test case}s to
modules. \ic{testMultiply} is the actual test case. It calls the to-be-tested
function \ic{times2} several times with different arguments and then uses an
\ic{assert} to check for the expected value.

\begin{code}
module UnitTestDemo { 
   
  int32_t main(int32_t argc, int8_t*[ ] argv) { 
    return test testMultiply; 
  } 
   
  test case testMultiply { 
    assert(0) times2(21) == 42; 
    assert(1) times2(0) == 0; 
    assert(2) times2(-10) == -20; 
  } 
   
  int8_t times2(int8_t a) { 
    return 2 * a; 
  }  
}
\end{code}

Note that, while this unit testing extension can be used to test any C
program, we use it a lot to test the generator. Consider the following example:

\begin{code}
  assert(0) 4 * 3 + 2 == 14; 
\end{code}

The result of \ic{4 * 3 + 2} is only 14 if the correct operator precedences are
observed (otherwise the result would be 20). One problem we had initially in MPS
was to make sure that the expression tree that was created while manually
entering expressions like \ic{4 * 3 + 2} is built correctly. If the tree were
built in the wrong way, the generated code could end up as \ic{4 * (3 + 2)}
leading to 20. So we've used tests like these to implicltly test quite intricate
aspects of our language implementation\footnote{This is also the reason why
the unit test extension was the first extension we've built for C. We needed
it to test many other aspects of the language.}.

We have built much more elaborate support for testing various other extensions.
It is illustrative to take a look at two of them. The next piece of code shows a
test for a state machine:

\begin{code}
exported test case test1 { 
  initsm(c1);   
  assert(0) isInState<c1, initialState>; 
  test statemachine c1 { 
    start -> countState 
    step(1) -> countState 
    step(2) -> countState 
    step(7) -> countState 
    step(1) -> initialState 
  } 
}
\end{code}

\ic{c1} is an instance of a state machine. After initializing it, we assert that
it is in the \ic{initialState}. We then use a special \ic{test statemachine}
statement, which consists of event/state pairs: after triggering the event (on
the left side of the \ic{->}) we expect the state machine to go into the state
specified on the right side of the \ic{->}. We could have achieve the same goal
by using sequences of \ic{trigger} and \ic{assert} statements, but the syntax
used here is much more concise. 

The second example concerns mocking. A mock is a part of a program that can be
used in place of the real one, to simulate some kind of environment of the
system under test. We use this in the components extension. The following is a
test case that checks if the \ic{client} uses the \ic{PersistenceProvider} interface
correctly. Let's start by taking a look at the interface:

\begin{code}
interface PersistenceProvider { 
  boolean isReady() 
  void store(DataPacket* data) 
  void flush() 
}
\end{code}

The interface is expected to be used in the following way: clients first have to
call \ic{isReady}, and only if that method returns \ic{true} are they supposed
to call \ic{store}, and then after any number of calls to \ic{store}, they have
to call \ic{flush}\footnote{Our components language actually also supports
protocol state machines which support the declarative specification of valid
call sequences}. Let us assume now we want to check if a certain client
component uses the interface correctly. Assuming it provides an operation
\ic{run} that uses the persistence provider, we could write the following test:

\begin{code}
exported test case runTest { 
  client.run(); 
  // somehow check is behaved correctly 
}
\end{code}

To check if the client behaves correctly, we can use a mock. Our mock specifies
the \emph{icoming} methods calls it expects to see. We have provided a mocking
extension to components to support this. Here is the mock:

\begin{code}
exported mock component PersistenceMock { 
  ports: 
    provides PersistenceProvider pp 
  expectations: 
    total no. of calls is 4 
    sequence { 
      0: pp.isReady return false; 
      1: pp.isReady return true; 
      2: pp.store { 
          0: parameter data: data != null 
        } 
      3: pp.flush 
    } 
}
\end{code}

The mock provides the \ic{PersistenceProvider} interface, so any other component
that wants to use this interface can use this component as the implementation.
But instead of actually implementing the operations prescribed by
\ic{PersistenceProvider}, we specify the sequence of invocations we're expect to
see. We expect a total number of 4 invocations. The first one is expected to be
to \ic{isReady}. We return false, expecting that the client tries again later.
If it does, we return \ic{true} and expect the client to continue with
persisting data. We can now validate the mock as part of the test case:

\begin{code}
exported test case runTest { 
  client.run(); 
  validate mock persistenceMock  
}
\end{code}

In case the \ic{persistenceMock} saw behaviour different from the one specified
above, the \ic{validate mock} statement will fail --- and with it, the whole
test.

\parhead{Structural Testing} What we suggested in the previous subsection tests
the execution semantics of programs written in DSLs, and, if we have enough of
these tests, it tests the correctness of the transformation, generator or
interpreter. However, there is a significant drawback to this approach: it only
works if the DSL actually specifies behavior! If the DSL only specifies
structures and cannot be executed, the approach does not work\footnote{For
testing execution semantics, this does not matter --- if you cannot execute
something because it has no behavior, then there is no execution semantics to
test in the first place. But you also cannot use the approach to implicitly test
generators or transformations then}. In this case you have to perform a
structural test. In principle, this is simple:

\begin{itemize}
  \item You write an example model
  \item You generate it\footnote{I have never seen an interpreter to process
  languages that only specify structures}
  \item And then you inspect the resulting model or test for the expected
  structures
\end{itemize}

Depending on the target formalism you can use regular expressions, XPath
expressions or OCL-like expressions to achieve this\ic{Remember that you should
still automate this!}. 

Note that you really should only use this if you cannot use semantics testing
based on execution. Inspecting the generated C
code for syntactic correctness, based on the input program, would be much more
work. And if we evolve the generator to generate better (faster, smaller, more
robust) code, tests based on the execution semantics will still work, while
those that test the structure may fail because line numbers or variable names
change.

Structural testing can also be useful to unit test model-to-model
transformations. Conside the example in \sect{m2mxtext}. There, inserted
additional states and transitions into whatever input state machine our
transformation processed. Testing this via execution invariably tests the
model-to-model transformation as well as the generator (or interpreter). If we
wanted to test the model-to-model transformation in isolation, we have to use
structural testing. The following piece of code could be used to check that, for
a specific input program, the transformation works correctly:

\begin{code}
// run transformation
val tp = p.transform

// test result structurally
val states = tp.states.filter(typeof(CustomState))
assert( states.filter(s|s.name.equals("EMERGENCY_STOP")).size == 1 )

val emergencyState = states.findFirst(s|s.name.equals("EMERGENCY_STOP"))
states.findFirst(s|s.name.equals("noCooling")).eAllContents.
    filter(typeof(ChangeStateStatement)).
        exists(css|css.targetState == emergencyState)
\end{code}

This program first runs the transformation, and then grabs all \ic{CustomState}s
(those that are not start or stop states). We then assert that in those states
there is exactly one with the name \ic{EMERGENCY\_STOP}, because we assume that
the transformation has added this state. We then check that in the
(one and only) \ic{noCooling} state there's at least one
\ic{ChangeStateStatement} whose target state is the \ic{emergencyState} we had
retrieved above.

Notice that we don't write an algorithmic check that closely resembles the
transformation itself. Rather, we test a specific model for the presence of
specific structures. For example, we explicitly look for a state called
\ic{noCooling} and check that this one has the correct
\ic{ChangeStateStatement}.


\parhead{Formal Verification} Formal verification is an alternative to testing
in some cases. The fundamental different between testing and verification is
this: in testing, each test case specifies \ic{one} particular execution
scenario. To get reasonable or full coverage of the whole model or generator,
you have to write and execute many tests. This can be a lot of work, and, more
importantly, you may not think about certain (exceptional) scenarios, and hence
you may not test then. Bug may go unnoticed.

Verification checks \ic{the whole program} at once. Varios non-trivial
algorithms are used to do that, and understanding these algorithms in detail is
beyond the scope of this book. However, it is very useful to know these
appraoches exist, especially since, over the last couple of years, they have
become scalable enough to address real-world problems. 

In this section we look at three examples cases --- the first will be the most
detailed.

\parhead{Model Checking State Machines} Model Checking is a verification
technique for state machines. In this section we can only scratch the surface;
to learn more about model checking, we recommend \cite{BerardBidoitM2001}. Here
is how it works in principle:

\begin{itemize}
  \item a piece of functionality is expressed as a state machine
  \item you then specify \emph{properties} of the state machine. Properties are
  expressions that have to true about the state machine
  \item you run the model checker with the state machine and the properties as
  input
  \item the output of the model checker either confirms that your properties
  hold, or it shows a counter example\footnote{It may also report Out Of
  Memory, in which case you have to reformulate or modularize your program and
  try again}.
\end{itemize}

Conceptually, the model checker performs an exhaustive search as part of the
verification process. Obviously, the more complex your state machine is, the
more possibilities the checker has to address, a problem known as state space
explosion. With finite memory, this limits scalability. In actual fact the model
checker does \emph{not} perform an exhaustive search; clever algorithms have
been devised that are semantically equivalent to an exhaustive search, but don't
actually perform one. This makes model checking scalable and fast enough for
real-world problems, although there is still a limit in terms of input model
complexity.

The interesting aspect of model checking is that the properties you specify are
not just simple boolean expressions such as \emph{each state must have at least
one outgoing transition, unless it is a stop state}. Such a check can be
performed statically, by "just looking" at the model. The properties model
checkers are more elaborate and are often typically expressed in (various
flavours of) temporal logic. Here are some examples, expressed in plain English:

\begin{itemize}
  \item \emph{It is always true that after we have been in state-X we will
  eventually be reaching state-Y}. This is a so-called Fairness
  property. It ensures that the state machine does not get stuck in some state forever.
  For example, \ic{state-Y} may be the green light for pedestrians, and
  \ic{state-X} could be the green light for cars.
  \item \emph{Whereever we are in the state machine, it is always possible to get
  into state-X}. This is a Liveliness property. Consider a machine where
  you always want to be able to turn it off.
  \item \emph{It is not ever possible to get into state-X without having gone
  through state-Y directly before}. This is a Safety property. Imagine a state
  machine where entering \emph{state-X} turns the pedestrian lights gree and
  entering \ic{state-X} turns the car lights red.
\end{itemize}

The important property of these temporal logic specifications is that
quantifiers such as \emph{always}, \emph{whenever} and \emph{there exists} are
available. Using these one can specify \emph{global truths} about the \emph{execution}
of a system. Three different flavours of languages exist to specify these
properties: LTL, CTL and CTL+. The concrete syntax is beyond the scope of this
book.

Model checking does come with its challenges. The input language for specifying
state machines as well as specifying the properties is not necessarily easy to
work with. Interpreting the replies of the model checker can be a challenge. And
for some of the tools, the usability is really bad --- the PROMELA/SPIN model
checker comes to mind here!

To make model checking more user friendly the mbeddr C language provides a nice
syntax for state machines and then generates the corresponding representation in
the input language of the model checker (we use the NuSMV model checker here).
The replies of the model checker are also reinterpreted in the context of the
higher level state machine. Tool integration is provided as well: users can
select the context menu on a state machine and invoke the model checker. The
model checker input is generated, the model checker is executed, and the replies
are rendered in a nice table in MPS. Finally, we have abstracted the property
specification language by providing support for the most important idioms; these
can be specified relatively easily. Also, a number of properties are
automatically checked for each state machine. Let us look at an example. The
following code shows a state machine that represents a counter. We can send the
\ic{step} event into the state machine, and as a consequence, it increments the
\ic{currentVal} counter by the \ic{size} parameter passed with the event. If the
\ic{currentVal} would become greater than \ic{LIMIT}, the counter wraps around.
We can also use the \ic{start} event to reset the counter to \ic{0}.

\begin{code}
verified statemachine Counter { 
  in events 
    start() 
    step(int[0..10] size)  
  local variables 
    int[0..100] currentVal = 0 
    int[0..100] LIMIT = 10 
  states ( initial = initialState ) 
    state initialState { 
      on start [ ] -> countState {  } 
    } 
    state countState { 
      on step [currentVal + size > LIMIT] -> initialState { } 
      on step [currentVal + size <= LIMIT] -> countState { currentVal = currentVal + size; } 
      on start [ ] -> initialState {  } 
    }
}
\end{code}

Since this state machine is marked as \ic{verifiable}, can run the model checker
from the context menu. \fig{verification1} shows the result of running the model
checker.


\begin{figure}[ht]
\begin{center}
  \includegraphics[width=11cm]{figures/7/verification1.png}
  \caption[]{}
  \label{verification1} 
\end{center} 
\end{figure}

Here is a subset of the properties it has checked successfully (it performs
these checks for all states/transitions by default):

\begin{code}
State 'initialState' can be reached	SUCCESS	
Variable 'currentVal' is always between its defined bounds	SUCCESS	
State 'countState' has deterministic transitions	SUCCESS	
Transition 0 of state 'initialState' is not dead	SUCCESS	
\end{code}

The first one reports that NuSMV has successfully proven that the
\ic{initialState} can be reached somehow. The second one reports that the
variable \ic{currentVal} stays within its bounds (notice how \ic{currentVal} is
a bounded integer). Line three reports that \ic{countState} it never happens
that in \ic{countState}, more than one transition is ready to fire at any time.
Finally, it reports that the first of the transitions is actually used at some
point. 

Let us provoke an error. We change the two transitions in \ic{countState} to 

\begin{code}
on step [currentVal + size >= LIMIT] -> initialState {  } 
on step [currentVal + size <= LIMIT] -> countState { currentVal = currentVal + size; }
\end{code}

You perhaps don't even recognize the different --- which is exactly why you
would want to use a model checker! We have changed the \ic{>} to a \ic{>=} in
the first transition. Running the model checker again, we get, among others,

\begin{code}
State 'countState' contains nondeterministic transitions	FAIL	4
\end{code}

This reports that there is a case where the two transitions are
non-deterministic, i.e. both are possible based on the guard, and it not clear
which one should be used. The \ic{4} at the end means that the execution trace
to this problem contains four steps. Clicking on the failed property check
reveals the problematic execution trace:

\begin{code} 
State initialState	
  LIMIT	                       10
  currentVal                    0
State initialState	
  in\_event: start	start()
  LIMIT                        10
  currentVal                    0
State countState	
  in\_event: step	step(10)
  LIMIT	                       10
  currentVal                    0
State initialState	
  LIMIT	                       10
  currentVal                   10
\end{code}

This is one (of potentially many) execution traces of this state machine that
leads to the non-determinism: \ic{currentVal} is 10, and because of the \ic{>=},
both transitions could fire.

In addition to these default properties, it is also possible to specify custom
properties. Instead of requiring users to be able to use the LTL or CTL
specification languages and formulating the property on the level of the
generated NuSMV input code (which is \emph{much} longer than the state machine
itself!), users can use convenient patterns, where they just have to fill in
values. Here are two examples:

\begin{code}
verification conditions 
  never LIMIT != 10 
  always eventually reachable initialState
\end{code}

The first one allows to express that we want the model checker to prove that any
boolean condition will never happen. In our example, we check that the \ic{LIMIT}
really is a constant and is never (accidentally) changed. The second one
specifies that wherever we are in the execution of the state machine, it is
still possible (after any number of steps) to reach the \ic{initialState}. Both
properties hold for the example state machine.


\parhead{SAT Solving}

\parhead{Abstract Execution}



\parhead{Deriving Test Cases}








In general, you have to be careful about what exactly you test. For example, if
you automatically derive the tests from the core model that specifies the
functionality, then successfully running these tests does not \emph{not} test
the model. If you forget to model something important in the model, such tests
cannot find that. Strictly speaking, it only tests that the generators for the
core model and the tests are compatible --- possibly both wrong, but compatibly
so.

If you want to test a program, the test model \emph{must} be separate. 


\subsection{Formally verifying semantics}

\subsection{Testing for language completeness}

 
\subsection{Testing Editor Services}


Testing whether the DSL can express what it should - review, example code...
  (short discussion, relate to process and interaction with domain experts)

These programs are
made persistent and the tests can be rerun ar any time, making sure the grammar
doesn't stop handling existing programs.




editor testing with MPS thingy

Testing the syntax: just write examples and see....

Coverage problematic, since unlimited number of programs

Test AUtomation /Nightly


Testing whether all constraints work: tests (e.g. using the type system f/w)

Testing whether the semantics are correct
    express "tests" in the same (or related) DSL

Then interpret or generate both and see if they fit
  ... you can easily automate that!

If a DSL only describes structure: Constraints, and "xpath" on the 
genreated stuff
