\section{Testing DSLs}

DSL testing is a multi-faceted problem. The following aspects of a DSL need to
be tested:

\todo{Better argue, which kinds of tests are necessary: syntax, constraints/TS,
semantics are in a natural order. Better check semantics than generated text.
But this is an integration test. Maybe test intermediate results? That would be
a structural test. Try to avoid it. Look at negative and positive testing in
each of these cases. Maybe add additional lang constructs to "inspect" internal
state. Allow this only in tests, not anywhere else (better than making
everything public\ldots). Testing semantics works only if executable. And
target env is required! Is some kind of integration test. Maybe simulate? Mock?
the whole simualtion story goes here as well. And interpreter. BSH.}

\begin{itemize}
  \item can the syntax cover all required sentences? Is the concrete syntax
  "correct"?
  \item do the constraints work? are all "wrong" programs actually detected, and
  is the right error message attached to the right program element?
  \item are the semantics correct? do transformations, generators and
  interpreters work correctly?
  \item can all programs relevant to the users actually be expressed?
  Does the language cover the complete domain?
\end{itemize}

\todo{I suggest we put the various tool examples into each of the following
sections}

\subsection{Syntax Testing}

Testing the syntax is simple. Developers simply try to write all relevant
programs and see if they can be expressed with the language. 

The following pieve of code is the fundamental code that needs to be written in
Xtext to test a DSL program using the Xtext testing utilities at
\verb#http://code.google.com/a/eclipselabs.org/p/xtext-utils/wiki/Unit_Testing#.
It is a JUnit4 test case with special support for the Xtext infrastructure.


\begin{code}
@RunWith(XtextRunner.class)
@InjectWith(CoolingLanguageInjectorProvider.class)
public class InterpreterTests extends XtextTest {

    @Test 
    public void testET0() throws Exception {
        testFileNoSerializer("interpreter/engine0.cool", "tests.appl", "stdparams.cool" ); 
    }
    
}                                                                                                                                                                                                               
\end{code}


The single test method loads the \verb#interpreter/engine0.cool# program, as
well as two more files which contain elements referenced from
\verb#engine0.cool#. The testFileNoSerializer method loads the file, parses it,
and checks constraints. If either parsing or constraint checking fails, the test
fails. There is also a testFile method which, after loading and parsing the
file, reserializes the AST to the text file, writes it back, and loads it again,
the comparing the two ASTs. This way, the (potentially adapted) serializer is
tested.

On a more fine grained level it is often useful to test partial sentences
instead of complete sentences or programs. The following piece of Xtext example
code tests the CustomState parser rule:


\begin{code}
    @Test 
    public void testStateParserRule() throws Exception {
        testParserRule("state s:", "CustomState" );
        testParserRule("state s: entry { do fach1->anOperation }", "CustomState" );
        testParserRule("state s: entry { do fach1->anOperation }", "State" );
    }
\end{code}


The first line asserts that the string \verb#state s:# is parsed with the
CustomState parser rule. The second line passes in a more complex state, one
with a command in an entry action. Line three tries the same text with the State
rule, which itself calls the CustomState. Notice that these tests really just
test the parser. No linking or constraints checks are performed. This is why we
can "call" anOperation on the fach1 object, although anOperation is not defined
as a callable operation anywhere.

Note that these unit tests do not at all depend on the Eclipse UI or the Xtext
editor, so they can be run from the command line or any other way of running
JUnit tests. They only need a couple of Xtext libraries to be on the classpath.
Consequently, it is simple to integrate these tests with continuous regression
test infrastructures or a build server.

\subsection{Constaints Testing}

Especially for languages with complex constraints, such as those implied by type
systems, testing of constraints is essential. A special API is necessary to be
able to verify that a program which makes a particular constraint fail actually
annotates the corresponding error message to the respective program element.
Tests can then be written which assert that a given program has a specific set
of error annotations. 

The unit testing utilities mentioned above also support testing constraints. The
utilities come with an internal Java DSL that support checking for the presence
of error annotations after parsing and constraint-checking a model file.


\begin{code}
    @Test 
    public void testTypesOfParams() throws Exception {
        testFileNoSerializer("typesystem/tst1.cool", "tests.appl", "stdparams.cool");
        assertConstraints( issues.sizeIs(3) );                             // 1
        assertConstraints( issues.forElement(Variable.class, "v1").        // 2  
          theOneAndOnlyContains("incompatible type") );                    // 2
        assertConstraints( issues.under(Variable.class, "w1").             // 3
          errorsOnly().sizeIs(2).oneOfThemContains("incompatible type") ); // 3
    }
\end{code}


We first load the model file that constains constraint errors (in this case,
type system errors). Then we assert the total number of errors in the file to be
three (line 1). Next, in line 2, we check that the instance of Variable named v1
has exactly one error annotation, and it has the text "incompatible type" in the
error message. Finally, in line 3 we assert that there are exactly two errors
anywhere under (i.e. in the subtree below) a variable named w1, and one of these
contains "incompatible type" in the error message. Using the fluent API style
shown by these examples, it is easy to express errors and their locations in the
program. If a test fails, a meaningful error message is output that supports
localizing (potential) problems in the test. The following is the error message
if no error message is found that contains the substring "incompatible type":


\begin{code}
junit.framework.AssertionFailedError: <no id> failed
  - failed oneOfThemContains: none of the issues 
    contains substring 'incompatible type' 
  at junit.framework.Assert.fail(Assert.java:47)
  at junit.framework.Assert.assertTrue(Assert.java:20)
  ...
\end{code}


If the test fails earlier in the filter expression, more than one output is
provided:


\begin{code}
junit.framework.AssertionFailedError: <no id> failed
  - no elements of type 
    com.bsh.pk.cooling.coolingLanguage.Variable named 'v1' found 
  - failed oneOfThemContains: none of the issues 
    contains substring 'incompatible type'
  at junit.framework.Assert.fail(Assert.java:47) 
  ...
\end{code}


A similar facility is available for MPS, it is called a NodesTest
(\fig{mps-testexample}). It supports special annotations to express assertions
on types and errors. For example, the third line of the nodes section reads
\verb#var double d3 = d# without annotations. This is a valid variable
declaration in our C language. After this has been written down, annotations can
be added. They are rendered in green. Line three asserts that the type of the
variable d is double, i.e. it tests that variable references assume the type of
the referenced variable. In line four we assign a double to an int, which is
illegal according to the typing rules. The error is detected, hence the red
squiggly. We use another annotation to assert the presence of the error message.

In addition to using these annotations to check models, developers can also
write more detailed test cases. In the example I assert that the var reference
of the node referred to as dref points to the node referred to as dnode. Note
how labels (green, underlined) are used to add names to program elements so they
can be referred to from test expressions.

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.65]{figures/7/mps-test.jpg}
  \caption[labelInTOC]{}
  \label{mps-testexample} 
\end{center}
\end{figure}


\TODO{Testing Scopes}

\subsection{Semantics Testing}

There are fundamentally two different ways of how a transformation or a
generator can be tested. In both cases, example programs are transformed, and 
we expect a valid (syntax and constraints) target program. The first alternative
then involves asserting the correctness of the structure of the generated
program by writing additional, source-program specific constraints. Alternative
two involves running the generated program and writing tests against it. 

The second alternative is generally preferrable since it does not test syntactic
structures of the target program, but instead really tests the semantics.
Ideally, the test themselves are expressed as part of the DSL program (or a
special fragment, built specifically for testing). However, the approach works
only if the to-be-tested DSL expresses behavior and the target program can be
executed. If the target is an intermediate representation that needs to be
further transformed for execution, this kind of test of course tests the whole
stack and not the single transformation. However, for testing code generators
of behavioural languages, this approach is the best way to go. Interpreters are
also typically tested this way.

The first alternative should be used in the remaining cases, i.e. for languages
that express structures only (and hence programs cannot be executed and
unit-tested), or for white-box testing of several chained transformations.

\TODO{can we provide an example for the latter?}

\subsubsection{Testing an Interpreter with Xtext}

The cooling language provides a way to express test cases for the cooling
programs within the cooling language itself. These tests are executed with an
interpreter inside the IDE, and they can also be executed on the level of the C
program. To make this work, the interpreter, as well as the code generator, can
handle the cooling programs as well as the tests. 

Testing DSL programs by running tests expressed in the same language runs the
risk of doubly-negating errors. If the DSL program is wrong, and the test is
wrong in a compatible way, the error will not be found. Also, this approach does
cannot just be used for testing interpreters or generators, it can also be used
to test whether a program written in the DSL works correclty. This is in fact
why the interpreter and the test sub-language have been built in the first
place: DSL users should be able to test the programs written in the DSL.
However, this implicitly also tests the interpreter (and generator), since,
unless the interpreter works "as expected", the expectations expressed as
assertions in the test case will fail.

The following code shows one of the simplest possible cooling programs, as well
as a test case for that program:


\begin{code}
cooling program EngineProgram0 for Einzonengeraet  uses stdlib {
         
    var v: int
    event e1
        
    init { set v = 1 }
       
    start:   
       entry { set v = v * 2 }
       on e1 { state s2 } 
           
    state s2:
        entry { set v = 0 }
          
} 


test EngineTest0 for EngineProgram0 {
    assert-currentstate-is ^start  // 1
    assert-value v is 2            // 2
    step                           // 3
    event e1                       // 4
    step                           // 5
    assert-currentstate-is s2      // 6
    assert-value v is 0            // 7
}
\end{code}


The test first asserts that, as soon as the program starts, it is in the start
state (line 1). We then assert that v is 2. The only reasonable way how v can
become 2 is that the code in the init block as well as the code in the entry
action of the start start have been executed. Note that this arrangement even
checks that the init block is executed before the entry action of the start
state, since otherwise v would be 1!. We then perform one step in the execution
of the program (the language is stepped) in line 3. At this point nothing should
happen, since no even was triggered (we don't test this). Then we trigger the
event e1 (line 4) and perform another step (line 5). After this step, the
program must transition to the state s2, whose entry action sets v back to 0. We
assert both of these (lines 6 and 7).

These tests can be run interactively from the IDE, in which case assertion
failures are annotated as error marks on the program, or from within JUnit. 
The following piece of code shows how to run the tests from JUnit.


\begin{code}
@RunWith(XtextRunner.class)
@InjectWith(CoolingLanguageInjectorProvider.class)
public class InterpreterTests extends PKInterpreterTestCase {

    @Test 
    public void testET0() throws Exception {
        testFileNoSerializer("interpreter/engine0.cool", "tests.appl", "stdparams.cool" ); 
        runAllTestsInFile( (Model) getModelRoot());
    }
}
\end{code}


The runAllTestsInFile method is called, passing in the model's root element. The
method runAllTestsInFile is defined by the PKInterpreterTestCase base class,
which in turn inherits from XtextTest, which we have seen before. The method
iterates over all tests in the model and executes them by creating and running a 
TestExecutionEngine. The TestExecutionEngine is a wrapper around the interpreter
for cooling programs. 


\begin{code}
protected void runAllTestsInFile(Model m) {
    CLTypesystem ts = new CLTypesystem();
    EList<CoolingTest> tests = m.getTests();
    for (CoolingTest test : tests) {
        TestExecutionEngine e = new TestExecutionEngine(test, ts);
        final LogEntry logger = LogEntry.root("test execution");
        LogEntry.setMostRecentRoot(logger);
        e.runTest(logger);    
    }
}
\end{code}


Testing generators in this way works similarly. The generator generates the DSL
program to an executable representation, for example, Java or C code. The tests
are generated to unit test code in the same language. Running the tests
simply involves executing the program and the test code together. 

The following is a test case expressed using the testing extension to C. It
contributes test cases to modules. The first one, testSimpleAdding, simply adds
two numbers. It then asserts that the adding algorithm works. The second test,
testPhysicalQuantities, tests whether the encoding of physical quantities work
correctly. Physical quantitites are another extension to C that supports the
definition of special types (speed in the example). By specifying a min and max
value, the optimal resolution of the value range to an underlying 16 bit
integer is used. The test assigns 10 km/h to the variable s1 using the p<..>
notation. In the assertion, we then assert that the actual value is 2180, which
is 10 times 2180, the resolution factor derived from mapping 300 km/h units to
65535 possible int16 values. Finally, the test contains an initialization
statement that executes the tests. This statement is specific to the Osek target
platform. 


\begin{code}
module tests imports <<imports>> { 
 
  test testSimpleAdding / tests adding two numbers { 
    int16 i = 0;    
    int16 j = i + 1;
    assert j == 1  =>  adding failed                                                                                                                                                                                                                       
  } 
 
  quantity speed range 0 .. 300 unit km/h 
 
  test testPhysicalQuantities / tests the encoding of physical quantities { 
    speed s1 = p<speed:10>;                         
    assert s1 == i<2180>  =>  speed encoding failed
  } 
 
  initialize {                       
    run test testSimpleAdding;     
    run test testPhysicalQuantities;
  }

}
\end{code}

                                                                                                                                                                                                                                                                           
From this program the following low-level C is generated (plus a makefile and
an Osek configuration file). Notice how the generator takes into account the
Osek platform specifics: for example, it uses ecrobot\_stats\_monitor to output
messages to the screen of the lego robot. 


\begin{code}
#include "include/Tests.h"

// used resources
#include "ecrobot_interface.h"

// custom includes
#include "kernel.h"
#include "kernel_id.h"
#include "stdint.h"
#include "stdint.h"

void Tests_tests_testproc_testSimpleAdding(void) {
    ecrobot_status_monitor ("running test: testSimpleAdding" );
    int16_t i = 0;
    int16_t j = (i + 1);
    if ( j != 1 ) {
          ecrobot_status_monitor ("  FAILED: adding failed" );
    } // end if 
}

void Tests_tests_testproc_testPhysicalQuantities(void) {
    ecrobot_status_monitor ("running test: testPhysicalQuantities" );
    int s1 = (10 * 218);
    if ( 2180 != s1 ) {
          ecrobot_status_monitor ("  FAILED: speed encoding failed" );
    } // end if 
}

void ecrobot_device_initialize(){
    Tests_tests_testproc_testSimpleAdding ( );
    Tests_tests_testproc_testPhysicalQuantities ( );
}
\end{code}


While the above examples are trivial test cases, they nontheless illustrate the
benefit of testing semantics vs. testing syntax. Inspecting the generated C
code for syntactic correctness, based on the input program, would be much more
work. Also, in case the DSL has several different generators to achieve a degree
of platform independence, the tests automatically benefit from this platform
independence as well. This is specifically true since in case of the unit
testing extension, its generator output MPS-C, which is subsequently transformed
into one of several versions of platform specific C code. The unit testing
extension gets this platform independency "for free". 


\subsection{Testing for language completeness}

 
\subsection{Testing Editor Services}


Testing whether the DSL can express what it should - review, example code...
  (short discussion, relate to process and interaction with domain experts)

These programs are
made persistent and the tests can be rerun ar any time, making sure the grammar
doesn't stop handling existing programs.


Testing the syntax: just write examples and see....

Coverage problematic, since unlimited number of programs

Test AUtomation /Nightly


Testing whether all constraints work: tests (e.g. using the type system f/w)

Testing whether the semantics are correct
    express "tests" in the same (or related) DSL

Then interpret or generate both and see if they fit
  ... you can easily automate that!

If a DSL only describes structure: Constraints, and "xpath" on the 
genreated stuff
