\section{Concrete and Abstract Syntax}

The \emph{concrete syntax} (short: CS) of a language is what the user interacts
with to create programs. It may be textual, graphical, tabular or any
combination thereof. In this book we focus mostly on textual concrete syntaxes
for reasons described in \ref{???}. We refer to other forms where appropriate.

The \emph{abstract syntax} (short: AS) of a language is a data structure that
holds the information represented by the concrete syntax, but without any of the
syntactic details. Keywords, layout (e.g., whitespace), and comments are
typically not included. Note that syntactic information which doesn't end up in
the AS is often preserved in some "hidden" form so the CS can be reconstructed
from the combination of the AS and this hidden information --- this
bidirectionality simplifies creating IDE features quite a bit.

In most cases, the abstract syntax is a tree data structure. Instances that
represent actual programs (i.e. sentences in the language) are hence often
called an abstract syntax tree or AST. Some formalisms also support
cross-references across the tree, in which case the data structure becomes a
graph (with a primary containment hierarchy). It is still usually called an AST.

\MV{FOwler's Semantic Model is not the same as the AST. The AST is driven by
the needs of the language, whereas the SM is driven by the needs of the domain.
I would not introduce this concept here.}

While the CS is the "UI" of the language to the user, the AS acts as the API to
access the data represented by programs. It is used by developers of validators,
transformations and code generators. The concrete syntax is not relevant in
these cases.

To illustrate this dichotomy between concrete and abstract syntax, consider the
following example program:

\footnotesize
\begin{verbatim}
var x: int;
calc y: int = 1 + 2 * sqrt(x)
\end{verbatim}
\normalsize 

This program has a hierarchical structure: definitions of \texttt{x} and
\texttt{y} at the top, inside \texttt{y} there's a nested expression. This
structure is reflected in the corresponding abstract syntax. A possible AST is
illustrated in \fig{astexample}. The boxes represent program elements, and the
names in the boxes represent language concepts that make up the abstract syntax.

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.5]{figures/1/astexample.png}
  \caption[labelInTOC]{Abstract syntax tree for the above program. Boxes
  represent instances of language concepts, solid lines represent containment,
  dotted lines represent cross-references}
  \label{astexample} 
\end{center}
\end{figure}

There are two ways of defining the relationship between the CS and the AS.
\begin{description}
  \item[AS first] An existing AS is annotated with the concrete syntax.
  Mapping rules determine how the concrete syntax maps to existing abstract 
  syntax structures.
  \item[CS first] From a concrete syntax definition, an abstract syntax is
  derived, either automatically and/or using hints in the concrete
  syntax specification.
\end{description}

Once the language is defined, there are again two ways how the abstract syntax
and the concrete syntax can relate as the language is used to create programs:
\begin{description}
  \item[Parsing] In parser-based systems, the abstract syntax tree is derived
  from the concrete syntax of a program; a parser instantiates and populates the
  AS, based on the information in the program text. In this case, the
  (formal) definition of the CS is usually called a \emph{grammar}.
  \item[Projection] In projectional systems, the abstract syntax tree is built
  directly by editor actions, and the concrete syntax is rendered from the AST
  via projection rules.
\end{description}

\fig{astandcst} shows the typical combinations of these two dimensions. In
practice, parser-based systems typically derive the AS from the CS - i.e., CS
first. In projectional systems, the CS is usually annotated onto the AS data
structures - i.e., AS first.

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.6]{figures/1/astandcst.png}
  \caption[labelInTOC]{}
  \label{astandcst} 
\end{center}
\end{figure}


\subsection{Fundamentals of Free Text Editing} 

Most general-purpose programming environments rely on free text editing, where
programmers edit programs at the character level to form (key)words and phrases.

A \emph{parser} is used to check the program text for syntactic correctness, and
create the AST by populating the AS from information extracted from the textual
source.  Most modern IDEs perform this task in real-time as the user edits the
program. The AST is always kept in sync with the program text. Many IDE features
--- such as content assist, validation, navigation, Refactoring support, etc.
--- are based on the real-time AST.

A key characteristic of the free text editing approach is its strong separation
between the concrete syntax (i.e., text) and the abstract syntax. The concrete
syntax is the principal representation, used for both editing and persistence.
The abstract syntax is used under the hood by the implementation of the DSL,
e.g. for providing an outline view, validation, and for transformations and code
generation.

Many different approaches to parser implementation exist. Each may restrict
the syntactic freedom of a DSL, or constrain the way in which a particular
syntax must be specified. It is important to be aware of this as not all
languages can be comfortably, or even at all implemented by every parser
implementation approach. The reader may have heard terms like context free,
ambiguity, look-ahead, LL, (LA)LR, PEG and these all pertain to a certain class
of parser implementation approaches. We'll provide more details on the various
grammar and parser classes further on in this section %% /chapter?.

\subsubsection{Parser Generation Technology}

In traditional compilers, parsers are often written by hand as a big program
that reads a stream of characters and uses recursion to create a tree structure.
Manually writing a parser in this way requires significant expertise in
parsing and a large development effort. For standardized programming languages
that don't change very often, and that have a large user community, this
approach makes sense. It can lead to very fast parsers that also exhibit good
error recovery (the ability to continue parsing after a syntax error has been found).

In contrast, language workbenches, and increasingly also traditional compilers,
\emph{generate} a parser from a grammar, a DSL for formally defining textual
concrete syntax. These generated parsers may not provide the same performance or
error recovery as a hand-tailored parser constructed by an expert, but they
provide bounded performance guarantees that make them (usually) more than fast
enough for modern machines. However, the most important argument to use parser
generation is that the effort required is \textbf{much} lower than writing a
custom parser from scratch. The grammar definition is also much more readable,
maintainable and adaptable than the actual parser implementation (either
custom-written or generated).

\MV{Do we want to say something regarding our dislike of actions (procedural
code) embedded in grammars? We'd like to separate these concerns\ldots}

\paragraph{Parsing versus scanning}

\LK{note that I changed lexer to scanner: lexer is more commonly used
  nowadays but since it's called 'scannerless parsing' it probably makes more
  sense to use 'scanner'}

Because of the complexity inherent in parsing, parser implementations tend to
split the parsing process into a number of phases. In the majority of cases, the
text input is first separated into a sequence of \emph{tokens} (i.e.,
keywords, identifiers, literal syntax, comments, whitespace, etc.) by
a \emph{scanner} (sometimes called lexer). The
\emph{parser} then constructs the actual AST the that token
sequence. This simplifies the implementation compared to directly parsing at
the character level. A scanner is usually implemented using direct recognition of
keywords and a set of regular expressions to recognize all other valid input as
tokens.

Both the scanner and parser can be generated from grammars (see below). A
well-known example of a scanner (lexer) generation tool is \texttt{lex}. Modern parsing
frameworks, such as ANTLR, do their own scanner generation.

Note that the word "parser" now has more than one meaning: it can either refer
to the combination of the scanner and the parser or to the post-scanner parser.
Usually the former meaning is intended (both in this book as well as outside)
unless scanning and parsing are discussed specifically.

A separate scanning phase has direct consequences for the overall parser
implementation, because the scanner typically isn't aware of the context of any
part of the input --- only the parser has this awareness. An example of a
typical problem that arises from this is that keywords can't be used as
identifiers even though often the use of a keyword wouldn't cause ambiguity in
the actual parsing. The Java language is an example of this: it uses a fixed set
of keywords, such as \keyword{class} and \keyword{public}, that cannot be used
as identifiers.

A context-unaware scanner can also introduce problems when languages
are extended or composed. In the case of Java, this was seen with the
\keyword{assert} and \keyword{enum} keywords that were introduced in
Java 1.4 and Java 5. Any programs that used identifiers with those
names (such as unit testing APIs) were no longer valid. For composed
languages, similar problems arise as constituent languages have
different sets of keywords and can define incompatible
regular expressions for lexicals such as identifiers and numbers.

The term "scannerless parsing" refers to the absence of a separate
scanner and in this case we don't have the problems of context-unaware
scanning illustrated above, because the parser operates at a character
level and statefully processes lexicals and keywords. Spoofax (or
rather: the underlying parser technology) uses scannerless
parsing.

\MV{[symbol tables] should be discussed in the scoping and linking section?}

% \paragraph{Symbol tables}

% Older compiler technology books tend to talk quite a bit about symbol tables,
% i.e.\ a table which stores identifiers encountered in the input, their location
% of definition and a link to the object (e.g., entity, variable, field, function,
% etc.) associated with that identifier. The symbol table is used later on in the
% parsing process to augment the AST to a graph (ASG) by looking up identifiers in
% the symbol table and creating links to the referred object.
% 
% This method often (but not necessarily) implies a strict ordering in the input
% text: objects have to be declared first before they can be used - e.g., the
% Pascal programming language is remeniscent of this with its forward
% declarations.
% 
% Nowadays, symbol tables are hardly used (explicitly) anymore. For example, Xtext
% uses a mechanism called scoping in combination with an Index infrastructure to
% achieve the same goal, but even comfortably across the boundaries of single
% files. Therefore, this will be the last spot we will mention symbol tables.

\paragraph{Grammars}

Grammars are the formal definition for concrete textual syntax. They consist of
so-called production rules which define how valid input ("sentences") looks
like. They can also be used to "produce" valid input, hence their name. Grammars
form the basis for syntax definitions in text-based workbenches such as Spoofax
and Xtext. In these systems, the production rules are enriched with information
beyond the pure grammatical structure of the language, such as the semantical
relation between references and declarations. 

Fundamentally, grammar production rules can be expressed in
Backus-Naur Form (BNF)\footnote{}, written as

  \(S\) \texttt{::=} \(P_1\) \texttt{...} \(P_n\)

This grammar defines a symbol $S$ by a series of pattern expressions $P_1$
\texttt{...} $P_n$. Each pattern expression can refer to another symbol or can
be a literal such as a keyword or a punctuation symbol. If there are
multiple possible patterns for a symbol, these can be written as
separate productions, or the patterns can
be separated by the '$|$' operator to indicate a choice. An
extension of BNF is called Extended BNF (EBNF), which adds a number of convenience
operators such as '?' for  an optional pattern, '*' to indicate zero
or more occurrences, and '+' to indicate one or more occurrences.

As an example, \Figure{Fig:exp-grammar-simple} shows an example of a grammar for
a simple arithmetic expression language using BNF notation. Basic expressions
are built up of \verb$NUM$ number literals and the \verb$+$ and \verb$*$
operators.

\MV{We have said this above already.}
% This grammar can be read in two ways: as a description of how to produce
% sentences in the arithmetic expression language (generation), and as a
% description of how to recognize and parse those sentences
% (recognition).

Note how expression nesting is described using recursion in this grammar:
the Exp rule calls itself, so sentences like \verb#2+3*4# are allowed. This
poses two practical challenges for parser generation systems: first, the
precedence and associativity of the operators is not described (explicitly) by
this grammar. Second, not all parser generators provide full support for
recursion. We elaborate on these issues in the remainder of the section and in
the Spoofax and Xtext examples.

\begin{figure}[t]
\begin{alltt}
Exp ::= NUM
      | Exp "+" Exp
      | Exp "*" Exp
\end{alltt}
\caption{A grammar for a simple expression language. The \texttt{NUM} symbol
refers to number literals.}
\label{Fig:exp-grammar-simple}
\end{figure}

\paragraph{Grammar classes}

BNF can describe any grammar that maps textual sentences to trees based only on
the input symbols. These are called \emph{context-free grammars} and can be used
to parse the majority of modern programming languages. In contrast,
\emph{context-sensitive grammars} are those that also depend on the context in
which a partial sentence occurs, making them suitable for natural language
processing but at the same time, making parsing itself a lot harder since the
parser has to be aware of a lot more than just the syntax.

Parser generation was first applied in command-line tools such as \texttt{yacc}
in the early seventies. As a consequence of the relatively slow computers, much
attention was paid to the efficiency of generated parsers. Various algorithms
were designed that could parse text in a bounded amount of time and memory.
However, these time and space guarantees could only be provided for certain
subclasses of the context-free grammars, described by acronyms such as LL(1),
LL($k$), LR(1), and so on, as illustrated in
\Figure{Fig:grammar-classes}.
A particular parser tool supports a specific class of grammars --- e.g., ANTLR
supports LL($k$) and LL(*).

In the classification of \Figure{Fig:grammar-classes}, the first L stands for left-to-right scanning, and the
second L in LL and the R in LR stand for leftmost and rightmost derivation.
The constant $k$ in LL($k$) and LR($k$) indicates the maximum number (of tokens or characters) the
parser will look ahead to decide which production rule it can
recognize. Typically, grammars for
"real" DSLs tend to need only finite look-ahead and many parser tools
effectively compute the optimal value for $k$ automatically. A special
case is LL(*), where $k$ is unbounded and the parser can look
ahead arbitrarily many tokens to make decisions.

\begin{figure}[t]
\begin{center}
  \TODO{Venn diagram of grammar classes: LL, LR, ...}
\end{center}
\caption{Classes of grammars.}
\label{Fig:grammar-classes}
\end{figure}

Supporting only a subclass of all possible context-free grammars poses
restrictions on the languages that are supported b a parser
generator. For some languages, it is
not possible to write a grammar in a certain subclass, making that particular language
unparseable with a tool that only supports that particular class of grammars. For other
languages, a natural context-free grammar exists, but it must be written in a
different, often awkward way to conform to the subclass. This will be
illustrated in the Xtext example, which uses ANTLR as the underlying LL($k$)
parser technology.

Parser generators can detect if a grammar conforms to a certain subclass,
reporting conflicts that relate to the implementation of the algorithm:
\emph{shift/reduce} or \emph{reduce/reduce} conflicts for LR parsers, and
\emph{first/first} or \emph{first/follow} conflicts and direct or indirect
\emph{left recursion} for LL parsers. DSL developers can then attempt
to manually refactor the grammar to address those errors.

As an example, consider a grammar for property or field access, expressions of
the form \verb$customer.name$ or \verb$"Tim".length$:%
%
\footnote{Note that we use ID to indicate identifier patterns and
  STRING to indicate string literal patterns in these examples.}

\begin{figure}[t]
\begin{alltt}
Exp ::= ID
      | Exp "." ID 
      | STRING
\end{alltt}
\caption{A grammar for property access expressions.}
\label{Fig:field-access-grammar}
\end{figure}

This grammar uses left-recursion: the left-most symbol of one of the definitions
of \verb$Exp$ is a call to \verb$Exp$, i.e.\ it is recursive. Left-recursion is
not supported by LL parsers, which includes the ANTLR parser generator and Xtext
(which makes use of ANTLR).

The grammar can be \emph{left-factored} by changing it to a form where all left
recursion is eliminated. The essence of left-factoring is that the grammar is
rewritten in such a way that all production rules which are recursive consume at
least one token or character before going into the recursion. Left-factoring
introduces additional rules that act as intermediaries and often makes
repetition explicit using the \texttt{+} and \texttt{*} operators. The example
grammar uses recursion for repetition, which can be made explicit as follows:

\begin{alltt}
Exp ::= ID
      | (Exp ".")+ ID
      | STRING
\end{alltt}

\noindent
The resulting grammar is still left-recursive, but we can introduce an
intermediate rule to eliminate the recursive call to \verb$Exp$:

\begin{alltt}
Exp ::= ID
      | (FieldPart ".")+ ID
      | STRING

FieldPart ::= ID
            | STRING
\end{alltt}

\noindent Unfortunately, this resulting grammar still has overlapping rules (a
first/first conflict) as the \verb$ID$ symbol matches more than one rule. This
conflict can be eliminated by removing the \texttt{Exp ::= ID} rule and making
the \texttt{+} "one or more" repetition into a \texttt{*} "zero or more"
repetition:

%\begin{alltt}
%Exp ::= ID
%      | (FieldPart ".")* ID
%      | STRING
%
%FieldPart ::= ID
%            | STRING
%\end{alltt}

\begin{alltt}
Exp       ::= (FieldPart ".")* ID
            | STRING
            
FieldPart ::= ID
            | STRING
\end{alltt}

\noindent
This last grammar now describes the same language as the one from
\Figure{Fig:field-access-grammar}, but conforms to the LL(1) grammar class.

Unfortunately, it is not possible to factor all possible context-free grammars
to one of the restricted classes. Valid, unambiguous grammars exist that cannot
be factored to any of the restricted grammar classes. In practice, this means
that some languages cannot be parsed with LL or LR parsers.

\paragraph{General parsers}

Research into parsing algorithms has produced parser generators specific to
various grammar classes, but there has also been research in parsers for the
full class of context-free grammars. Naive algorithms that support the full
class of context-free grammars use backtracking, i.e.\ they allow themselves to
try to match the input using another production rule even though an
earlier-tried rule already seemed to match\MV{I don't understand the previous
sentence}. This carries the risk of exponential execution times or
non-termination and usually exhibits poor performance.

There are also general parsing algorithms that can \emph{efficiently} parse the
full class. In particular, generalized LR (GLR) and Earley parsers can parse
unambiguous grammars in linear time and gracefully cope with ambiguities with a
cubic or $O(n^3)$ time in the worst case. Spoofax is an example of a language
workbench that uses GLR parsing.


\paragraph{Ambiguity}

Grammars can be \emph{ambiguous} meaning that at least one valid sentence in the
language can be constructed in more than one (non-equivalent) way from the
production rules, corresponding to multiple possible ASTs. This obviously is a
problem for parser implementation as some decision has to be made on which AST
is preferred.

For example, the expression language grammar given in
\Figure{Fig:exp-grammar-simple} is ambiguous. For a string \texttt{1*2+3} there
are two possible trees (corresponding to different operator precedences). The
grammar does not describe which interpretation should be preferred.

\begin{Tree}
\node {Exp}
    child { node {Exp}
        child { node {Exp}
            child { node {1}}}
        child { child { node {*}}}
        child { node {Exp}
            child { node {2}}}}
    child {edge from parent[draw=none]}
    child { child { child { node {+}}}}
    child { node {Exp}
        child { child { node {3}}}};
\end{Tree}
\hspace{2em}
\begin{Tree}
\node {Exp}
    child { node {Exp}
        child { child { node {1}}}}
    child { child { child { node {*}}}}
    child {edge from parent[draw=none]}
    child { node {Exp}
        child { node {Exp}
            child { node {2}}}
        child { child { node {+}}}
        child { node {Exp}
            child { node {3}}}};
\end{Tree}

Parser generators for restricted grammar classes and generalized parsers handle
ambiguity differently. We discuss both approaches.

\paragraph{Ambiguity with grammar classes}

LL and LR parsers are deterministic parsers: they can only return one possible
tree for a given input. This means they can't handle any grammar that has
ambiguities, including the grammar in \Figure{Fig:exp-grammar-simple}.
Determining whether a grammar is ambiguous is a classical, undecidable problem.
However, it is possible to detect violations of the LL or LR grammar class
restrictions, in the form of conflicts. These conflicts do not always indicate
ambiguities (as seen with the field access grammar of
\Figure{Fig:field-access-grammar}), but by resolving all conflicts (if possible)
an unambiguous grammar can be formed.

Resolving grammar conflicts in the presence of associativity, precedence, and
other risks of ambiguity requires carefully layering the grammar in such a way
that it encodes the desired properties. To encode left-associativity and a lower
priority for the addition operator we can rewrite the grammar as follows:

\begin{alltt}
Expr ::= Expr "+" Mult
       | Mult
Mult ::= Mult "*" NUM
       | NUM
\end{alltt}

\noindent The resulting grammar is a valid LR grammar. Note how it puts the
addition operator in the highest layer to give it the lowest priority, and how
it uses left-recursion to encode left-associativity of the operators. The
grammar can be left-factored to a corresponding LL grammar (supported
by Xtext) as follows:

\begin{alltt}
Expr ::= Mult ("+" Mult)*
Mult ::= NUM ("*" NUM)*
\end{alltt}

\MB{It would be good to expand this a bit with examples of other types of
operators (post-/prefix) as well, since they follow different patterns. A more
explicit discussion of associativity is desirable as well, IMHO. Currently, the
only good sources which can easily be found are blogs by Sven E.\ and
myself\ldots}

\MV{Note that we have many more examples of this (with many more operators) in
the Xtext example. We should keep it brief here, and then discuss details in
the example section.} \LK{ok, so then we don't need to elaborate on it
here?} \MV{I think what we have is enough. Take a look at the Xtext example to
judge for yourself.} \LK{alright. then MB can remove these comments
once read and agreed to :)}

\paragraph{Ambiguity with generalized parsers}

General parsers accept grammars regardless of recursion or ambiguity. That is,
the grammar of \Figure{Fig:exp-grammar-simple} is readily accepted as a valid
grammar. In case of an ambiguity, the generated parser simply returns all
possible abstract syntax trees, e.g.\ a left-associative tree and a
right-associative tree for the expression \texttt{1+2+3}. The different trees
can be manually inspected to determine what ambiguities exist in the grammar, or
the desired tree can be programmatically selected.

Language developers can use \emph{disambiguation filters} to indicate which
interpretation should be preferred. For example, left-associativity can be
indicated on a per-production basis:

\begin{alltt}
Exp ::= NUM
      | Exp "+" Exp \curlies{left}
      | Exp "*" Exp \curlies{left}
\end{alltt}

\noindent indicating that both operators are left-associative (using the
\verb${left}$ annotation as seen in Spoofax). Operator precedence can be
indicated with relative priorities or with precedence annotations:

\begin{alltt}
Exp ::= Exp "*" Exp \curlies{left}
>
Exp ::= Exp "+" Exp \curlies{left}
\end{alltt}

\noindent indicating that the multiplication operator binds stronger than the
addition operator.

\UNDONE{Note that not all parser tools support disambiguation filters: Xtext doesn't,
but Spoofax does.} \MV{While this is true, I find this comment misleading
because Xtext is not a GLR parser and it sounded like disam filters are only
relevant for GLR parsers. I suggest we remove this sentence and mention in the
example above that Xtext does it that way.}
%
\LK{i think that was just
an old note floating around here; we discussed the xtext approach
above}
%
\LK{but some reflection may be good}
%
\MV{Maybe reemphasizing that it only exists in GLR parsers?}

\paragraph{Compositionality}

encoded precedence, associativity, etc. makes grammars resistent to
change and composition

grammar classes and composition don't mix

can you compose the arithmetic expressions grammar with the field access 
grammar \Figure{Fig:field-access-grammar}?

and what about

\footnotesize
\begin{verbatim}
for (Customer c : SELECT customer FROM accounts WHERE balance < 0) {
  ...
}
\end{verbatim}
\normalsize

reserved keywords: for, SELECT, FROM

requires a context-sensitive scanner, or a parser that operates on
characters instead of on tokens (a scannerless parser)





\subsection{Fundamentals of Projectional Editing} 

In parser-based approaches, users use text editors to enter character sequences
that represent programs. A parser then checks the program for syntactic
correctness and constructs an abstract syntax tree from the character sequence.
The AST contains all the semantic information expressed by the program i.e.\ %%
keywords.

In projectional editors, the process happens the other way round: as a user
edits the program, the AST is modified directly. This is similar to the  MVC
pattern where every editing action triggers a change in the AST. A projection
engine then creates some representation of the AST with which the user
interacts. This approach is well-known from graphical editors. In editing a UML
diagram, users don't draw pixels onto a canvas, and an "image parser" then
creates the AST. Rather, the editor creates and instance of \texttt{uml.Class}
as you drag a class from the palette to the canvas. A projection engine renders
the diagram, in this case drawing a rectangle for the class. This approach can
be generalized to also work with text editors.

Every program element is stored as a node with a unique ID (UID) in the AST.
References are based on actual pointers (references to UIDs). The AST is
actually an ASG, an abstract syntax graph, from the start because
cross-references are first-class rather than being resolved after parsing. The
program is stored using some generic tree persistence mechanism which are often
XML-based.

\subsection{Comparing Parsing and Projection}

\MV{The following paragraph needs to be ripped apart and the respective points
need to be moved into the respective subsubsections} There's a lot to say in
favor of free text editing. The linear, two-dimensional structure of text
(especially when using mono-spaced fonts) is well-known and easily understood,
while text editing operations are more-or-less universal. It's very convenient
to have a CS that also serves as the persistence format: it makes the language
essentially tool-independent and allows fairly easy integrations with other
text-based tools like versioning systems, issue trackers, documentation systems,
or email clients.


\subsubsection{Editing Experience}

To edit programs, you need a suitable editor. A normal text editor is obviously
not sufficient as you would be editing some textual representation of the AST.
For textual-looking notations, it is important that the editor tries and makes
the editing experience as text-like as possible, i.e. the keyboard actions we
have gotten used to from free-text editing should work as far as possible. MPS
does a decent job here, using, among others, the following strategies:

\begin{itemize}

  \item Every language concept that is legal at a given program location is
  available in the code completion menu. In naive implementations, users have to
  select the language concept (based on its name) and instantiate it. This is
  inconvenient. In MPS, languages can instead define aliases for language
  concepts, allowing users to "just type" the alias, after which the concept
  is immediately instantiated.

  \item So-called side transforms make sure that expressions can be entered
  conveniently. Consider a local variable declaration \verb#int a = 2;#. If this
  should be changed to \verb#int a = 2+3;# the 2 in the init expression needs
  to be replaced by an instance of the binary + operator, with the 2 in the left
  slot and the 3 in the right. Instead of removing the 2 and inserting a +,
  users can simply type + on the right side of the 2; the system performs the
  tree refactoring that moves the + to the root of the subtree, puts the 2 in
  the left slot, and then puts the cursor into the right slot, to accept the
  second argument. This means that expressions (or anything else) can be entered
  linearly, as expected.
  
  \item Delete actions are used to similar effect when elements are deleted.
  Deleting the 3 in \verb#2+3# first keeps the plus, with an empty right slot.
  Deleting the + then removes the + and puts the 2 at the root of the subtree.

  \item Wrappers support instantiation of concepts that are actually children of
  the concepts allowed in a given location. Consider again a local variable
  declaration \verb#int a;#. The concept represented could be
  LocalVariableDeclaration, a subtype of Statement, to make it legal in method
  bodies (for example). However, users simply want to start typing \verb#int#,
  i.e.\ selecting the content of the type field of the LocalVariableDeclaration.
  A wrapper can be used to support entering types where
  LocalVariableDeclarations are expected. Once a type is selected, the wrapper
  implementation creates a LocalVariableDeclaration and puts the type into its
  type field, and moves the cursor into the name slot.

  \item Smart references achieve a similar effect for references (as opposed to
  children). Consider pressing Ctrl-Space after the + in \verb#2+3#. Assume
  further, that a couple of local variables are in scope and that these can be
  used instead of the 3. These should be available in the code completion menu.
  However, technically, a VariableReference has to be instantiated, whose
  variable slot then is made to point to any of the variables in scope. This is
  tedious. Smart references trigger special editor behavior: if in a given
  context a VariableReference is allowed, the editor \emph{first} evaluates its
  scope to find the possible targets and then puts them into the code completion
  menu. If a user selects one, then the VariableReference is created, and the
  selected element is put into its variable slot. This makes the reference
  object effectively invisible in the editor.

  \item Smart delimiters are used to simplify inputting list-like data that is
  separated with a specific separator symbol, such as parameter lists. Once a
  parameter is entered, users can press comma, i.e.\ the list delimiter, to
  instantiate the next element.

\end{itemize}

Notice that, except for having to get used to the somewhat different way of
editing programs, all other problems can be remedied with good tool support.
Traditionally, this tool support has not always existed or been sufficient, and
projectional editors have gotten a bit of a bad rep because of that. In case of
MPS this tool support is available, and hence, MPS provides a productive and
pleasant working environment.

\MV{Here}


\subsubsection{Language Modularity}

Projectional editors can cope with textual notations that would be ambiguous if
they had to be parsed. This is important in two contexts:

\begin{description}

	\item[existing languages] for which a language-specific parser had been manually
	written before, should now be handled with a LWB. Projectional editors can deal
	with any "strange" syntax defined by legacy systems.

	\item[language composition] After composing separately developed languages, the
	resulting language may be ambiguous\footnote{Many grammar formalisms are not
	closed under composition.}. In projectional systems, this cannot happen. Any
	combination of languages will be syntactically valid (semantics is a different
	issue). If a composed language would be ambiguous, the user has to make a
	disambiguating decision as the program is built. For example, in MPS, if in a
	given location two language concepts are available under the same alias, just
	typing the alias won't bind, and the user has to manually decide by picking one
	alternative from the code completion menu. We discuss details in the chapter on
	language composition.

\end{description}


\subsubsection{Notational Freedom}

Since no parsing is used in projectional editors, and the mechanism works
basically like a graphical editor, notations other than text can be used in the
editor. For example, MPS supports tables as well as simple diagrams. Since these
non-textual notations are handled the same way as the textual ones (possibly
with other input gestures), they can be mixed easily: tables can be embedded
into textual source, and textual languages can be used within table cells.
Textual notations can also be used inside boxes or as connection labels in diagrams.

\subsubsection{Language Evolution}

Evolution of languages can be a bit more complex, since, if language concepts
are not present anymore in a new revision of the language, existing instances of
these concepts will break. MPS solves this problem with deprecation and
"automatic" migration scripts. We will discuss this in more detail in the
chapter on language evolution.


\subsubsection{Infrastructure Integration}

Special support also needs to be provided for infrastructure integration. Since
the CS is not pure text, a generic persistence format needs to be devised.
Therefore, special tools need to be provided for diff and merge. MPS provides
integration with the usual VCS systems and handles diff and merge in the tool,
using the concrete, projected syntax. Note that since every program element
has a UUID, \emph{move} can be distinguished from \emph{delete} and
\emph{create}, providing more useable semantics for diff and merge.
\fig{fig:mps-diff.png} shows an example of an MPS diff.

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.45]{figures/1/mps-diff.png}
  \caption[labelInTOC]{}
  \label{fig:mps-diff.png} 
\end{center}
\end{figure}



\subsubsection{Other}

In parser-based systems, the complete AST has to be reconstructable from the CS.
This implies that there can be no information in the tree that is \emph{not}
obtained from parsing the text. This is different in projectional editors. For
example, the textual notation could only project a subset of the information in
the tree. The same information can be projected with different projections, each
possibly tailored to a different stakeholder, and showing a different subset
from the overall data. Since the tree uses a generic persistence mechanism, it
can hold data that has not been planned for in the original language definition.
All kinds of meta data (documentation, presence conditions, requirements traces)
can be stored, and projected if required. MPS supports so-called annotations,
where additional data can be added to model elements of existing languages and
projecting that data inside the original projection, all without changing the
original language specification.


\subsection{Characteristics of AST formalisms}

Most AST formalisms, aka Meta Meta Models, are ways to represent trees or
graphs. Usually, such an AST formalism is "meta circular" in the sense that it
can describe itself.

\subsubsection{EMF Ecore}

The Eclipse Modeling Framework (EMF) is at the core of all of Eclipse's modeling
related activities. Its core component is the Ecore meta meta model, a version
of the EMOF standard. The central concepts are: EClass, EAttribute, EReference
and EObject, the latter providing an in-memory/run-time representation of EClass
instances. EReferences can be containing or not - each EObject can be contained
by at most one EReference instance. \fig{fig:ecore-metamodel} shows a class
diagram of Ecore.

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.45]{figures/1/ecore-metamodel-2.png}
  \caption[labelInTOC]{}
  \label{fig:ecore-metamodel} 
\end{center}
\end{figure}

The Ecore file is at the heart. From that, all kinds of other aspects are
derived, specifically, a generic tree editor, and a generated Java API for
accessing the AST of models and programs. This also forms the basis for Xtext's
processing.

EMF has grown to be a fairly large ecology within the Eclipse community and
numerous projects use EMF as a basis for model manipulation and persistence,
with Ecore for meta model definition.

\subsubsection{Spoofax' ATerm}

Spoofax uses the ATerm format to represent abstract syntax. ATerms are
a generic tree structure representation format that can be serialized
textually similar to XML or JSON. ATerms consist of the following
elements:

\begin{itemize}
\item Strings, e.g. \verb$"Mr. White"$
\item Numbers, e.g. \verb$15$ 
\item Lists, e.g. \verb$[1,2,3]$
\item Constructor applications, e.g. \verb$Order(5, 15, "Mr. White")$
\end{itemize}

\noindent
In addition to these basic elements, ATerms can also be annotated with
additional information, e.g.
%
\verb$Order(5{ProductName("Apples")}, 15, "Mr. White")$. These
annotations are often used to represent references to other parts of a
model.

The textual notation of ATerms can be used for exchanging data between
tools and as a notation for model transformations or code generation
rules. In memory, ATerms can be stored in a tools-specific fashion
(i.e., simple Java objects in the case of Spoofax). The generic
structure and serializability of ATerms also allows them to be
converted to other data formats. For example, the aterm2xml/xml2aterm
tools can convert between ATerms and XML.

\subsubsection{MPS' Structure Definition}

Every program element in MPS is a node. A node has a structure definition and
projection rules for rendering. This is also true for language definitions.
Nodes are instances of Concepts, which corresponds to EMF's EClass. Like
EClasses, concepts are meta circular, i.e. there is a concept that defines the
properties of concepts:

\footnotesize
\begin{verbatim} 
concept ConceptDeclaration extends AbstractConceptDeclaration                          
                           implements INamedConcept                     
                                      IStructureDeprecatable            
                                                                                
  instance can be root: false                                                   
                                                                                
  properties:                                                                   
    helpURL : string 
    rootable : boolean                                                             
                                                                                
  children:         
    InterfaceConceptReference  implements                 0..n                                                             
    LinkDeclaration            linkDeclaration            0..n                       
    PropertyDeclaration        propertyDeclaration        0..n               
    ConceptProperty            conceptProperty            0..n                       
    ConceptLink                conceptLink                0..n                               
    ConceptPropertyDeclaration conceptPropertyDeclaration 0..n 
    ConceptLinkDeclaration     conceptLinkDeclaration     0..n         
                                                                                
  references:                                                  
    ConceptDeclaration         extends                   0..1
                                                                                
  concept properties:                                                           
    alias = concept                       
\end{verbatim}
\normalsize
 
A concept may extend a single other concept and implement any number of
interfaces. It can declare references and child collections. It may also have a
number of primitive-type properties as well as a couple of "static" features
(those stating with "concept"). In addition, concepts can have behavior methods.
                                                                            
\subsection{Xtext Example}

\TODO{We assume an introduction of the case studies somewhere before this point}

Cooling programs represent the behavioral aspect of the refrigerator language.
Here is a trivial program that can be used to illustrate some of the features of
the language.

\footnotesize
\begin{verbatim} 
cooling program EngineProgram0 {
     
    var v: int
    event e
        
    init { set v = 1 }
       
    start:   
       on e { state s2 } 
       
    state s2:
        entry { set v = 0 }
          
} 
\end{verbatim}
\normalsize

The program declares a variable \texttt{v} and an event \texttt{e}. When the
program starts up, the init section is executed, setting \texttt{v} to 1. The
system then (automatically) transitions into the start state. There it waits
until it receives the \texttt{e} event. It then transition to the \texttt{s2}
state, where it uses an entry action to set \texttt{v} back to 0. More complex
programs include checks of changes of measurements
(\verb#compartment1->currentTemp#) and commands to the hardware %%%
(\verb#do compartment1->coolOn#), as shown in the next snippet:

\footnotesize
\begin{verbatim} 
start:
    check ( compartment1->currentTemp > maxTemp ) {
        do compartment1->coolOn
        state initialCooling
    }
    check ( compartment1->currentTemp <= maxTemp ) {
        state normalCooling
    } 
    
state initialCooling:
    check ( compartment1->currentTemp < maxTemp ) {
        state normalCooling
    }
\end{verbatim}
\normalsize


\paragraph{Grammar basics} In Xtext, the language is specified using a grammar
which is a collection of \emph{parser rules}. These rules specify (by default)
the concrete syntax of a program element, as well as its mapping to the AS.
Xtext generates an Ecore meta model to describe the AS. Here is the definition
of the CoolingProgram rule:

\footnotesize
\begin{verbatim} 
CoolingProgram:
    "cooling" "program" name=ID "{"
      (events+=CustomEvent |
        variables+=Variable)*
      (initBlock=InitBlock)?
      (states+=State)*
    "}";
\end{verbatim} 
\normalsize

Rules begin with the name (CoolingProgram), a colon, and then the rule body. The
body defines the syntactic structure of the language concept defined by the
rule. In our case, we expect the keywords \keyword{cooling} and
\keyword{program}, followed by an ID. ID is a \emph{terminal rule} that is
defined in the parent grammar from which we inherit. It is defined as an
unbounded sequence of lowercase and uppercase characters, digits, and the
underscore, although it may not start with a digit. This terminal rule is
defined as follows, again using a BNF-like syntax:

\footnotesize
\begin{verbatim} 
terminal ID: ('a'..'z'|'A'..'Z'|'_') ('a'..'z'|'A'..'Z'|'_'|'0'..'9')*;
\end{verbatim} 
\normalsize

In pure grammar languages, one would write \verb#"cooling" "program" ID"{"#,
specifying that after the two keywords we expect an ID as defined above.
However, in Xtext grammars don't just express the concrete syntax - they also
determine the mapping to the AS. We have encountered two such mappings so far.
The first one is implicit (although it can be made explicit as well): the name
of the rule will be the name of the generated meta class. So we will get a meta
class CoolingProgram. The second mapping we have encountered in \verb#name=ID#.
It specifies that the meta class get a property name that holds the contents of
the ID in the program text. Since nothing else is specified in the ID terminal
rule, the type of this property defaults to EString, Ecore's implementation of a
string data type.

The rest of the definition of a cooling program is enclosed in curly braces. It
contains three elements: first the program contains a collection of events and
variables (the asterisk specifies unbounded multiplicity), an optional init
block (optionality is specified by the question mark) and a list of states. Let
us inspect each of these in more detail. 

The expression \verb#(states+=State)*# specifies that there can be any number of
State instances in the program. The meta class gets a property \verb#states#, it
is of type State (the meta class derived from the State rule). Since we use the
\verb#+=# operator, the \verb#states# property is a list as well. In case of the
optional init block, the meta class will have an \verb#initBlock# property,
typed as InitBlock (whose parser rule we don't show and discuss here), with a
multiplicity of 0..1. Events and variables are more interesting, since the
vertical bar operator is used within the parentheses. The asterisk expresses
that whatever is inside the parentheses can occur any number of times - note
that the use of a \verb#*# usually goes hand in hand with that of a
\verb#+=#. Inside the parentheses we expect either a CustomEvent or a Variable.
Variables are assigned to the variables collection, events are assigned to the
events collection. This notation means that we can mix events and variables in
any order.

The following alternative notation would first expect all events, and
then all variables. Im the end, it depends on your end users which notation is
more suitable.

\footnotesize
\begin{verbatim} 
      (events+=CustomEvent)*
      (variables+=Variable)*
\end{verbatim} 
\normalsize

The definition of State is different, since State is intended to be an abstract
meta class with several subtypes.

\footnotesize
\begin{verbatim} 
State:
    BackgroundState | StartState | CustomState;
\end{verbatim} 
\normalsize

The vertical bar operator is used here to express alternatives regarding the
syntax. This is translated to inheritance in the meta model. The definition of
custom state is shown in the following code snippet. It uses the same grammar
language features as explained above.

\footnotesize
\begin{verbatim} 
CustomState:
    "state" name=ID ":"
        (invariants+=Invariant)*
        ("entry" "{"
            (entryStatements+=Statement)*
        "}")?
        ("eachTime" "{"
            (eachTimeStatements+=Statement)*
        "}")?
        (events+=EventHandler | signals+=SignalHandler)*;
\end{verbatim}  
\normalsize 

StartState and BackgroundState, the other two subtypes of State, share some
properties. Consequently, Xtexts AS derivation algorithm pulls them up into the
abstract State meta class so they can be accessed polymorphically.
\fig{Fig:xtext-metamodel} shows the resulting meta model using EMF's tree view.

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.7]{figures/1/xtext-metamodel.png}
  \caption[labelInTOC]{}
  \label{Fig:xtext-metamodel}  
\end{center}
\end{figure}

\paragraph{References}

\LK{maybe this should first show the grammar rule without the
  cross-reference, and then explain that's not how you normally do it
  in Xtext, but that you should make it a cross-reference instead. I
  suppose you could then even show it with
  \texttt{targetState=[State|QID]} and say that
  \texttt{targetState=[State]} is a short-hand in case it's an
  \texttt{ID}. that way maybe we can maintain a grammar perspective in
  this section}

Let us now look at statements and expressions. States
have entry and exit statements, procedural code that is executed when a state is
entered and left respectively. The \verb#set v = 1# is an example. Statement
itself is of course abstract and has all the various kinds of statements as
subtypes/alternatives (in the actual language there are many more):

\footnotesize
\begin{verbatim} 
Statement:
    IfStatement | AssignmentStatement | PerformAsyncStatement | 
    ChangeStateStatement | AssertStatement; 

ChangeStateStatement:
    "state" targetState=[State];

AssignmentStatement:
    "set" left=Expr "=" right=Expr;
\end{verbatim}  
\normalsize 

The ChangeStateStatement is used to transition into another state of the overall
state-based program. It uses the keyword \keyword{state} and then a reference to
the actual target state in the form of its name. Notice how Xtext uses brackets
to express the fact that the \verb#targetState# property points to an existing
State as opposed to containing a new one (written as \verb#targetState=State#),
i.e.: a non-containing cross-reference. In traditional grammars, the rule would
be written as \verb#"state" targetStateName=ID;# and the language implementation
would have to provide functionality to follow the \verb#targetState# link
separately.

Note that the cross-reference definition only specifies the target type of the
cross-reference, but not the CS itself: by default, the ID terminal is used for
the syntax. Initially, this ID ends up in a proxy URI which is resolved lazily
as soon as we try and access the property. The Xtext framework takes care of
most of this out-of-the-box, but you can exert a lot of control on its behavior
by means of a custom scope provider.

There are two more details to explain. In case the actual terminal or datatype
rule used to represent the reference is not an ID, but, for example, a name with
dots in it (qualified name), this can be specified as part of the reference.
Notice how in this case the vertical bar does not represent an alternative, it
is merely used as a separator between the target type and the terminal used to
represent the reference.

\footnotesize
\begin{verbatim} 
ChangeStateStatement:
    "state" targetState=[State|QID];

QID: ID ("." ID)*;
\end{verbatim}  
\normalsize 

The other remaining detail is scoping. During the linking phase, where the text
of ID (or QID) is used to find the target object, several objects with the same
name might exist, or some target elements are not visible based on visibility
rules of the language. To control the possible reference targets, scoping
functions are used. These will be explained in the next section.

\MB{I still have to reconcile the duplicate explanation of scoping in this
paragraph\ldots}


\paragraph{Expressions} The AssignmentStatement is one of the statements that
uses expressions. This leads us to discussing the implementation of expressions
in Xtext. Let us look at the definition of expressions as a whole. The following
snippet is a subset of the actual definition of expressions (we have omitted
some additional expressions that don't add anything to the description here).

\footnotesize
\begin{verbatim} 
Expr:
    ComparisonLevel;

ComparisonLevel returns Expression:
    AdditionLevel ((({Equals.left=current} "==") |
                    ({LogicalAnd.left=current} "&&") |
                    ({Smaller.left=current} "<"))
                                            right=AdditionLevel)?;

AdditionLevel returns Expression:
    MultiplicationLevel ((({Plus.left=current} "+") |
                         ({Minus.left=current} "-")) right=MultiplicationLevel)?;

MultiplicationLevel returns Expression:
    PrefixOpLevel ((({Multi.left=current} "*") |
                    ({Div.left=current} "/")) right=PrefixOpLevel)?;

PrefixOpLevel returns Expression:
    ({NotExpression} "!" "(" expr=Expr ")") |
    AtomicLevel;

AtomicLevel returns Expression:
    ({TrueExpr} "true") |
    ({FalseExpr} "false") |
    ({ParenExpr} "(" expr=Expr ")") |
    ({NumberLiteral} value=DECIMAL_NUMBER) |
    ({SymbolRef} symbol=[SymbolDeclaration|QID]);
\end{verbatim}  
\normalsize 

We first have to explain in more detail how the AST construction works in Xtext.
Obviously, as the text is parsed, meta classes are instantiated and the AST is
assembled. However, instantiation of the respective meta class happens only when
the first assignment to one of its properties is performed. If no assignment is
performed at all, no object is created. For example in case of
\verb#TrueLiteral: "true";# no instance of TrueLiteral will ever be created,
because there is nothing to assign. In this case, an action can be used to force
instantiation: \verb#TrueLiteral: {TrueLiteral} "true";# Notice that the action
can instantiate meta classes other than those that are derived from the rule
name. Unless otherwise specified, an assignment such as \verb+name=ID+ is always
interpreted as an assignment on the object that has been created most recently.
The \keyword{current} keyword can be used to access that object in case it
itself needs to be assigned to a property.

Now we know enough to understand how expressions are encoded and parsed. For the
rules with the Level suffix, no meta classes are created, because (as Xtext is
able to find out statically) they are never instantiated. They merely act as a
way to encode precedence. To understand this, let's consider how \verb#2*3# is
parsed:

\begin{itemize}
  \item We start (by definition) with the Expr rule. The Expr rule just calls
  the ComparisonLevel rule. Note that a rule call can have two effects: it
  returns an object which represents the parsed result or null in case no
  object was instantiated, or it internally throws an exception indicating that
  the input couldn't be matched to the called rule. The returned object is
  available in the calling rule using the \keyword{current} keyword.
  \item The parser now "dives down" until it matches the 2. This occurs on
  AtomicLevel, as it matches the \verb#DECIMAL\_NUMBER# terminal. At this point
  it creates an instance of NumberLiteral and assigns the actual number (2) to
  the value property. It also sets the \keyword{current} object to point to the
  just created NumberLiteral.
  \item The AtomicLevel rule ends, and the stack is unwound. We're back at
  PrefixOpLevel, in the second branch. Since nothing else is specified, we
  unwind once more.
  \item We're now back at the MultiplicationLevel. The rule we have on the stack
  is not finished yet and we try to match a * and a /. The match on * succeeds.
  At this point the so-called assignment action on the left side of the * kicks
  in (\verb#{Multi.left=current}#). This action creates an instance of Multi,
  and assigns the current (the NumberLiteral) to its left property. Then it
  assigns the newly created Multi to be the current object. At this point we
  have a subtree with the * at the root, and the NumerLiteral 2 in the left
  property.
  \item The rule hasn't ended yet. We dive down to PrefixOpLevel once more,
  matching the 3 in the same way as the two before. The NumerLiteral for 3 is
  assigned to the right property.
  \item At this point we unwind the stack further, and since no more text is
  present, no more objects are created. The tree structure is as we had
  expected.
\end{itemize}

If we'd parsed \verb#4 + 2*3# the + would have matched before the *, because it
is "mentioned earlier" in the grammar, it is in a lower-precedence group, the  
AdditionLevel. Once we're a the 4 + tree, we'd go down again to match the 2.
As we unwind the stack after matching the 2 we'd match the *, creating a Multi
again. The current, at this point, would be the 2, so it would be put onto the
left side of the *, making the * the current. Unwinding further, that * would be
put onto the right side of the +, building the tree just as we'd expect.

Notice how a rule at a given level only always delegates to rules at higher
precedence levels. So higher precedence rules always end up further down in the
tree. If we want to change this, we can use parentheses: inside those, we can
again embed an Expr, i.e. we jump back to the lowest precedence level.

\TODO{Explain why Xtext does it this way (left recursion, etc.) and refer back
to general parsing discussion.}

To use Xtext with expressions it is not necessary to completely understand the
mechanism, since expressions are always structured in the way shown above. The
code shown here can be used as a template. Adding new operators or new levels
can be done easily. 

\paragraph{Polymorphic References} In the cooling language, expressions also
include references to other entities, such as configuration parameters,
variables and hardware elements such as compressors and fans (defined in a
different model, not shown above). All of these referencable elements extend the
SymbolDeclaration meta class. This means that all of them can be referenced by
the single SymbolRef construct.


\footnotesize
\begin{verbatim} 
AtomicLevel returns Expression:
    ...
    ({SymbolRef} symbol=[SymbolDeclaration|QID]);
\end{verbatim}  
\normalsize 

The problem with this situation is that the reference itself does not encode the
kind of thing that is referenced. By looking at the reference alone we only know
that we reference some kind of symbol. This makes writing code that processes
the model cumbersome, since the target of a SymbolRef has to be taken into
account when deciding how to treat (translate, validate) a symbol reference. A
more natural design of the language would use different reference constructs for
the different referencable elements. In this case, the reference itself is
specific to the referenced meta class, making processing much easier.

\footnotesize
\begin{verbatim} 
AtomicLevel returns Expression:
    ...
    ({VariableRef} var=[Variable]);
    ({ParameterRef} param=[Parameter]);
    ({HardwareBuildingBlockRef} hbb=[HardwareBuildingBlock]);
\end{verbatim}  
\normalsize 

However, this is not possible with Xtext, since the parser cannot distinguish
the three cases syntactically. In all three cases, the reference itself is just
an ID. Only during the linking phase could the system check which kind of
element is actually referenced, but this is too late for the parser, which needs
an unambiguous grammar. The grammar could be disambiguated by using a different
syntax for each element:

\footnotesize
\begin{verbatim} 
AtomicLevel returns Expression:
    ...
    ({VariableRef} "v:" var=[Variable]);
    ({ParameterRef} "p:" param=[Parameter]);
    ({HardwareBuildingBlockRef} "bb:" hbb=[HardwareBuildingBlock]);
\end{verbatim}  
\normalsize 

While this approach will technically work, it would lead to an awkward syntax
and is hence typically not used. The only remaining alternative is to make all
referencable elements extend SymbolDeclaration and use a single reference.





\TODO{Explain how to go from AS to CS}









\subsection{Spoofax Example}

Mobl's data modeling language provides entities and entity
functions. To illustrate the language, here is an example of two
data type definitions related to a shopping list app:

\begin{verbatim}
module shopping

entity Item {
  name     : String
  checked  : Bool
  favorite : Bool
  onlist   : Bool
  order    : Num
  store    : Store
}

entity Store {
  name  : String
  open  : Time
  close : Time

  function isOpen() {
    return open <= now.getTime() && close <= now().getTime();
  }
}
\end{verbatim}

In mobl, most files starts with a module header, which can be followed
by a list of entity type definitions. Our example \emph{shopping}
module defines two entities for items on a shopping list and stores
for those items.  Entities are persistent data types that are stored
in a database and can be retrieved using mobl's querying API.

Syntax:

\begin{verbatim}
"entity" QId ":" Type "{" EntityBodyDecl* "}"  -> Definition {cons("Entity")}
"entity" QId "{" EntityBodyDecl* "}"  -> Definition {cons("EntityNoSuper")}
ID ":" Type "(" {Anno ","}* ")" -> EntityBodyDecl {cons("Property")}
ID ":" Type                     -> EntityBodyDecl {cons("PropertyNoAnnos")}
FunctionDef                               -> EntityBodyDecl
ID                              -> Anno {cons("SimpleAnno")}
"inverse" ":" ID                -> Anno {cons("InverseAnno")}
\end{verbatim}

\begin{verbatim}

"function" QId "(" {FArg ","}* ")" ":" Type "{" Statement* "}" -> FunctionDef {cons("Function")}
"return" Exp ";"                            -> Statement
{cons("Return")}

Exp "." ID "(" { Exp  ")" -> Exp {cons("MethodCall")}
Exp "." ID                -> Exp {cons("FieldAccess")}
ID                        -> Exp {cons("Var")}
\end{verbatim}



 

\subsection{MPS Example}

We start by defining a simple language for state machines. Core concepts include
StateMachine, State, Transition and Trigger. The state machine can be embedded
in C code as we will see later. The language supports the definition of state
machines as shown in the following piece of code:

{
\footnotesize
\begin{verbatim}
statemachine linefollower {                                                         
  event initialized;                                                                 
  event bumped;                                                                     
  event blocked;                                                                    
  event unblocked;                                                                  
  initial state initializing {                                                      
    initialized [true] -> running                                                   
  }                                                                                 
  state paused {                                                                    
    entry int16 i = 1;                                                              
    unblocked [true] -> running                                                     
  }                                                                                 
  state running {                                                                   
    blocked [true] -> paused                                                        
    bumped [true] -> crash                                                          
  }                                                                                  
  state crash {                                                                     
    <<transitions>>                                                                 
  }                                                                                 
}                                                                                   
\end{verbatim}
}


\TODO{Add UML diagram of the language. Generate via MPS.}

\paragraph{Concept Definition} MPS is projectional so we start with the
definition of the AS. In MPS, AS elements are called concepts. The code below shows the definition of
the concept Statemachine. It contains a collection of States and a
collection of Events. The alias is defined as "statemachine", so typing this
word inside C modules instantiates a state machines. Statemachine also 
implements a couple of interfaces; IHasIdentifierName contributes a property
name, IModuleContent makes the state machine embeddable in C Modules --- the
module owns a collection of IModuleContents, just like the Statemachine
contains States and Events.


\footnotesize
\begin{verbatim}
concept Statemachine extends MedBase              
                     implements IHasIdentifierName
                                IModuleContent    
  properties:                                     
    << ... >>                                       
                                                  
  children:                                       
    State states 0..n specializes: <none>           
    Event events 0..n specializes: <none>           
                                                  
  references:                                     
    << ... >>                                       
                                                  
  concept properties:                             
    alias = statemachine                            
\end{verbatim}
\normalsize



The State contains collections of EntryActions and ExitActions. These both
extend Action, which in turn owns a StatementList to include C code.
StatementList is a concept defined by the C core language. To make that visible,
our statemachine language extends C core. A state contains a boolean attribute
initial, to mark the initial state. Finally, a State contains a collection of 
Transitions. 

\footnotesize
\begin{verbatim}
concept Transition extends MedBase      
                   implements <none>    
  properties:                           
    << ... >>                             
                                        
  children:                             
    Trigger trigger 1 specializes: <none> 
    Expression guard 1 specializes: <none>
                                        
  references:                           
    State target 1 specializes: <none>    
                                        
  concept properties:                   
    alias = transition                    
\end{verbatim}
\normalsize


Transitions contain a Trigger, a guard condition and the target state. The
trigger is an abstract concept; various specializations are possible, the
default implementation is the EventTrigger, which references an event. The guard
condition is of type Expression; another concept inherited from the C core
language. A typesystem rule will be defined later to constrain this expression
to be boolean. The target state is a reference, i.e. we point to an existing
state instead of owning it. 


\paragraph{Editor Definition} Editors are defined via projection rules. Editors
are made of cells. When defining editors, various cell types are arranged so that the resulting syntax
has the desired structure. \fig{Fig:mps-transeditor} shows the definition of the
editor for a transition. It arranges the trigger, guard and target state in a
horizontal list of cells, the guard surrounded by brackets, and an arrow (->)
in front of the target state. 

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.7]{figures/1/mps-transeditor.png}
  \caption[labelInTOC]{}
  \label{Fig:mps-transeditor}  
\end{center}
\end{figure}

\TODO{Add a table projection to show off non textual notations}

\fig{Fig:mps-stateeditor} shows the editor definition for the State. It uses a
vertical list to arrange several horizontal elements. The first line contains
the keyword initial, the keyword state and the name property of the state. The
question mark in front of the initial label denotes this cell as being
optional; an expression determines whether it is shown or not. In this case, the
expression checks the initial property of the current state, and shows the
keyword only if is true. A quick fix is used to toggle the property. 

\begin{figure}[ht]
\begin{center}
  \includegraphics[scale=0.7]{figures/1/mps-stateeditor.png}
  \caption[labelInTOC]{}
  \label{Fig:mps-stateeditor}  
\end{center}
\end{figure}

The rest of the editor arranges the entry actions, transitions and exit actions
in a vertical arrangement. Each of the collections is indented (---->). The
entry and exit actions are only shown if the collections are not empty (another
optional cell). A quick fix is used to create the first entry and exit actions,
subsequent ones can be created by pressing enter at the end of an existing
action.

\paragraph{Intentions} Since the use of quick fixes (called intentions
in MPS) is an essential part of editing with MPS, we show how they are created in this section.
The following piece of code shows the definition on the one that triggers the
initial properties. Intentions specify for which language concept they apply, a
text that describes the intention in the intentions menu, an optional boolean
expression that determines whether the intention is currently applicable, and
the actual code that performs the quickfix. Note how this is simply a
model-to-model transformation expressed with MPS APIs. 

\footnotesize
\begin{verbatim}
intention makeInitialState for concept State { 
                                                                                                                                                           
  description(editorContext, node)->string { 
    "Statemachine: Make Initial"; 
  }                                                                             
                                                                                                                                                           
  <isApplicable = true>                                                                                                                                      
                                                                                                                                                           
  execute(editorContext, node)->void { 
    foreach s in node.ancestor<concept = Statemachine>.states { 
      s.initial = false; 
    } 
    node.initial = true; 
  }
}
\end{verbatim}
\normalsize



\paragraph{Expressions} Since we inherit the guard expression structure and
syntax from the C core language, we don't have to define expressions. It is
nonetheless interesting to look at its implementation.

Expressions are arranged into a hierarchy starting with the abstract concept
Expression. All other kinds of expressions extend Expression, directly or
indirectly. For example, the PlusExpression extends the BinaryExpression which
in turn extends Expression. BinaryExpressions have left and right child
Expressions. This way, arbitrarily complex expressions can be built.
Representing expressions as trees is a standard approach; in that sense, the
abstract syntax of MPS expressions is not very interesing. The editors are also
trivial --- in case of the plus expression, they are a horizontal list of:
editor for left argument, the plus symbol, and the editor for the right
argument.

As we have explained in the general discussion about projectional editing, MPS
supports linear input of hierarchical expressions using so-called side
transforms. The code below shows the right side transformation for
expressions that transforms an arbitrary expression into a PlusExpression but
putting the PlusExpression "on top" of the current node. Using the alias of
expressions and the inheritance hierarchy, it is possible to factor all side
transformations for all binary operations into one single action declaration,
resulting in much less implementation effort.

\footnotesize
\begin{verbatim}
side transform actions makeArithmeticExpression                                                                                                                                                                                                                                  
 
  right transformed node: Expression tag: default_                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                 
  actions :                                                                                                                                                                                                                                                                      
    add custom items  (output concept: PlusExpression)                                                                                                                                                                                                                           
      simple item                                                                                                                                                                                                                                                                
        matching text                                                                                                                                                                                                                                                            
          +                                                                                                                                                                                                                                                                      
        description text                                                                                                                                                                                                                                                         
          <default>                                                                                                                                                                                                                                                              
        icon                                                                                                                                                                                                                                                                     
          <default>                                                                                                                                                                                                                                                              
        type                                                                                                                                                                                                                                                                     
          <default>                                                                                                                                                                                                                                                              
        do transform                                                                                                                                                                                                                                                             
          (operationContext, scope, model, sourceNode, pattern)->node< > { 
            node<PlusExpression> expr = new node<PlusExpression>(); 
            sourceNode.replace with(expr); 
            expr.left = sourceNode; 
            expr.right.set new(<default>); 
            return expr.right; 
          }                    
\end{verbatim}
\normalsize



In the AST, operator precedence is encoded by where in an expression tree a
given expression is located. For example, in \verb#2+3*4# the + is higher up in
the tree than the *, encoding that the * has higher precedence. This is natural
since processing of ASTs is typically depth-first, so the * is evaluated before
the +. However, as we enter an expression linearly using the side
transformations actions, we have to make sure that we don't accidentally put a
+ "below" a *. There are two ways to deal with this:

\begin{itemize}
  \item The first alternative allows lower-precedence operators below
  higher-precedence operators in the tree, but the editor automatically renders
  parentheses to make sure visual appearance is in line with the AST.
  \item The other alternative reshuffles the tree automatically whenever an
  expression is edited, putting higher-precedence operators further down in the
  tree. Parenthesis can still be used explicitly.
\end{itemize}

In both cases the system needs information about the precedence level of
operators or expressions. This information can be stored in a concept property
(comparable to a static member in a Java class).

\paragraph{Polymorphic References}

\LK{even more so than the previous bit on references, this part
  doesn't fit here and should be moved to the section on Scoping and Linking}

 We have explained above how references work
in principle: they are actual pointers to the references element (the necessary
scopes are explained in the next section). In the section on Xtext (\TODO{make
sure we explain that first}) we have seen how from a given location only one
kind of reference for any given syntactic form can be implemented. Consider the
following example, where we refer to a local variable (a) and an event parameter
(timestamp) from within expressions:

\footnotesize
\begin{verbatim}
int a;
int b;

statemachine linefollower {                                                         
  event initialized(int timestamp);                                                                
  initial state initializing {                                                      
    initialized [now() - timestamp > 1000 && a > 3] -> running                                                   
  }                                                                                 
}                                                                                   
\end{verbatim}
\normalsize


Both references to local variables and to event parameters use the same
syntactic form: simply a name. In Xtext, this has to be implemented with a
single reference (typically called SymbolReference) that can reference to any
kind of Symbol. LocalVariableDefintions and EventParameters would both extend
Symbol, and scopes would make sure both kinds are visible from within guard
expressions. The problem with this approach is that the reference itself
contains no type information about what it references, it is simply a
SymbolReferenece. Processing code has to inspect the type of the symbol to find
out what a SymbolReference actually means.

In projectional editors this done differently. In the example above there is a
LocalVariableReference and an EventParameterReference. Both have an editor that
simply renders the name of the referenced element. Entering the reference
happens by typing the name of the referenced element (cf the concept of smart
references introduced above). In the (rare) case where there's a LocalVariable
and a EventParameter, the user has to make an explicit decision, at the time of
entry (the name won't bind, and the CC menu requires a choice). It is important
to understand that, although the names are similar, the tool still knows which
one refers to a LocalVariable and which one refers to a EventParameterReference.
Upon selection, the correct reference objects are created.



