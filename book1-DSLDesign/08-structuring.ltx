\FloatBarrier

\section{Fundamental Paradigms}
\label{learnFromGPLs}

Every DSL is different. It is driven by the domain to which it applies. However,
as it turns out, there are also a number of commonalities between DSLs. These
can be handled by modularizing and reusing (parts of) DSLs as discussed in the
\emph{next} section. In \emph{this} section we look at common paradigms for
describing DSL structure and behaviour.


\subsection{Structure}

Languages have to provide means of structuring large programs in order to keep
them manageable. Such means include modularization and encapsulation,
specification vs. implementation, specialization, types and instances as well as
partitioning. \marginnote[-1.5cm]{The language design alternatives described in
this section are usually not driven directly by the domain, or the domain experts
guiding the design of the language. Rather, they are often brought in by the
language designer or the consumers of the DSL as a means of managing overall
complexity. For this reason they may be hard to "sell" to domain experts.}


\paragraph{Modularization and Visibility} DSL often provide some kind of logical
unit structure, such as namespaces or modules. Visibility of symbols may be
restricted to the same unit, or in referenced ("imported") units. Symbols may be
declared as public or private, the latter making them changeable without
consequences for using modules.\marginnote{Most contemporary programming
languages use some form of namespaces and visibility restriction as their top
level structure.} Some form of namespaces and visibility is necessary in almost
any DSL. Often there are domain concepts that can play the role of the module,
possibly oriented towards the structure of the organization in which the DSL is
used. 


\embc{As a fundamental extension to C, this DSL contains modules with visibility
specifications and imports (\fig{modules}). Functions, state machines, tasks and
all other top-level concepts reside in modules. Header files (which are
effectively a poor way of managing symbol visibility) are only used in the
generated low level code.}

\begin{figure}[h]
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm}
  \includegraphics[scale=0.6]{figures/embc/modules.png}
  \caption[][0.3cm]{Modules are the top-level contruct in this C
  implementation. Module contents can be exported, which means they are visible
  to importing modules.}
  \label{modules}
  \rule{1\textwidth}{0.7pt}
\end{figure}

\comparch{Components and interfaces live in namespaces. Components are
implementation units, and are always private. Interfaces and data types may be
public or private. Namespaces can import each other, making the public elements
of the imported namespace visible to the importing namespace. The OSGi generator
creates two different bundles: an interface bundle that contains the public
artifacts, and an implementation bundle with the components. In case of a
distributed system, only the interface bundle is deployed on the client.}

\pension{Pension plans constitute namespaces. They are grouped into more
coarse-grained packages that are aligned with the structure of the pension
insurance business.}

\marginnote[0.8cm]{If a repository-based tool is used, the importance of
paritioning is greatly reduced. Although even in that case, there may be a set of federated and
distributed repositories that can be considered partitions}
\paragraph{Partitioning} Partitioning refers to the breaking down of programs
into several physical units such as files. These physical units do not have to
correspond to the logical modularization of the models within the partitions.
Typically each model fragment is stored in its own partition. For example, in
Java a public class has to live in a file of the same name (logical module ==
physical partition), whereas in C\# there is no relationship between namespace,
class names and the physical file and directory structure. A similar
relationship exists between partitions and viewpoints, although in most cases,
different viewpoints are stored in different partitions.

Partitioning may have consequences for language design. Consider a DSL where an
concept A contains a list of instances of concept B. The B instances then have
to be physically nested within an instance of A in the concrete syntax. If there
are many instances of B in a given model, they cannot be split into several
files. If such a split should be possible, this has to be designed into the
language. 

\comparch{A variant of this DSL that was used in another project had to be
changed to allow a namespaces to be spread over several files for
reasons of scalability and version-control granularity. In the initial version,
namespaces actually \emph{contained} the components and interfaces. In the
revised version, components and interfaces were owned by no other element, but
model files (partitions) had a namespace declaration at the top, logically
putting all the contained interfaces and components into this namespace. Since
there was no technical containment relationship between namespaces and its
elements, several files could now declare the same namespace. Changing this
design decision lead to a significant reimplementation effort because all kinds
of naming and scoping strategies changed.}


Other concerns influence the design of a partitioning strategy:

\begin{description}

\item[Change Impact] which partition changes as a consequence of a particular
change of the model (changing an element name might require changes to all
references to that element from other partitions)

\item[Link Storage] where are links stored (are they always stored in the model
that logically "points to" another one)?, and if not, how/where/when to control
reference/link storage.

\item[Model Organzation] Partitions may be used as a way of organizing the
overall model. This is particularly important if the tool does not provide a
good means of showing the overall logical structure of models and finding
elements by name and type. Organizing files with meaningful names in directory
structures is a workable alternative.

\item[Tool Chain Integration] integration with existing, file based tool chains.
Files may be the unit of checkin/checkout, versioning, branching or permission
checking. 
\end{description}

\marginnote[-1.5cm]{Another driver for using partitions is the scalability of
the DSL tool. Beyond a certain file size, the editor may become sluggish.}


It is often useful to ensure that each partition is processable separately to
reduce processing times. An alternative approach supports the explict definition
of those partitions that should be processed in a given processor run (or at
least a search path, a set of directories, to find the partitions, like an
include path in C compilers). You might even consider a separate build step to
combine the results created from the separate processing steps of the various
partitions (again like a C compiler: it compiles every file separately into an
object file, and then the linker handles overall symbol/reference resolution and
binding).

The partitioning scheme may also influence users' team collaboration when
editing models. There are two major collaboration models: real-time and
commit-based. In real-time collaboration, a user sees his model change when
another user changes that same model. Change propagation is immediate. A
database-backed repository is often a good choice regarding storage, since the
granularity tracked by the repository is the model element. In this case, the
partitioning may not be visible to the end user, since they just work "on the
repository". This approach is often (at least initially) preferred by
non-programmer DSL users.

The other collaboration mode is commit-based where a user's changes only make it
to the repository if he performs a \emph{commit}, and incoming changes are only
visible after a user has performed an \emph{update}. While this approach can be
used with database-backed repositories, it is most often used with file-based
storage. In this case, the partitioning scheme is visible to DSL users, because
it is those files they commit or update. This approach tends
to be preferred by developers, maybe because well-known versioning tools have
used the approach for a long time.



\paragraph{Specification vs. Implementation} Separating specification and
implementation supports plugging in different implementations for the same
specification and hence provides a way to "decouple the outside from the
inside".\marginnote{Interfaces, pure abstract classes, traits or function
signatures are a realization of this concept in programming languages.} This
supports the exchange of several implementations behind a single interface. This
is often required as a consequence of the development process: one stakeholder
defines the specification and a client, whereas another stakeholder provides one
or more implementations.

\embc{This DSL adds interfaces and components to C. Components provide or use
one or more interfaces. Different components can be plugged in behind the same
interface. In contrast to C++, no runtime polymorphism is supported, the
translation to plain C maps method invocation to flat function calls.}

\cooling{Cooling programs can refer to entities defined as part of the
refrigerator hardware as a means of accessing hardware elements (compressors,
fans, valves). To enable cooling programs to run with different, but similar
hardware configurations, the hardware structure can use "trait inheritance",
where a hardware trait defines a set of hardware elements, acting as a kind of
interface. Other hardware configurations can inherit these traits. As long as
cooling programs are only written against traits, they work with any
refrigerator that implements the particular set of traits against which the
program is written.}


\paragraph{Specialization} Specialization enables one entity to be a more
specific variant of another one. Typically, the more specific one can be used in
all contexts where the more general one is expected (the Liskov substitution
principle\cite{LiskovW94}). The more general one may be incomplete, requiring
the specialized ones to "fill in the holes".\marginnote{In GPLs, we know this
approach from class inheritance. "Leaving holes" is realized by abstract
methods.} Specialization in the context of DSLs can be used for implementing
variants or of evolving a program over time.
 
 
\pension{The customer using this DSL had the challenge of creating a huge set of
pension plans, implementing changes in relevant law over time, or implementing
related plans for different customer groups. Copying complete plans and then
making adaptations was not feasible for obvious reasons. Hence the DSL provides
a way for pension plans to inherit from one another. Calculation rules can be
marked \emph{abstract} (requiring overwriting in sub-plans), \emph{final} rules
are not overwritable. Visibility modifiers control which rules are considered
"implementation details".}

\cooling{A similar approach is used in the refrigerator DSL. Cooling programs
can specialize other cooling programs. Since the programs are fundamentally
state-based, we had to define what exactly it means to override a pumping 
program.}

\paragraph{Types and Instances} Types and instances refers to the ability to
define a structure that can be parametrized when it is
instantiated.\marginnote{In programming languages we know this from classes and
objects (where constructor parameters are used for parametrization) or from
components (where different instances can be connected differently to other
instances).}

\embc{Apart from C's \lcr{structs} (which are instantiatable data structures)
and components (which can be instantiated and connected), state machines can
be instantiated as well. Each instance can be in a different state at any given
time.}

\paragraph{Superposition and Aspects} Superposition\marginnote{This is
especially important in the context of product line engineering and is discussed
in \cite{VoelterVisser2011} and in section of this book.} \todo{ref} refers to
the ability to merge several model fragments according to some DSL-specific merge
operator. Aspects provide a way of "pointing to" several locations in a program
based on a pointcut operator (essentially a query over a program or its
execution), adapting the model in ways specified by the aspect. Both approaches
support the compositional creation of many different model variants from the
same set of model fragments.


\comparch{This DSL provides a way of advising component definitions, for example
to introduce additional ports from an aspect (\fig{monitoring}). An aspect may
introduce a port \ic{provided port mon: IMonitoring} that allows a central monitoring component
to query the adviced components via the \lcr{IMonitoring} interface.}

\begin{figure}[h]
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm} 
  \includegraphics[width=10cm]{figures/comparch/monitoring.png}
  \caption[][0.3cm]{The aspect component contributes an additional required port
  to each of the other components defined in the system.}
  \label{monitoring}
  \rule{1\textwidth}{0.7pt}
\end{figure} 



\exwebdsl{Entity declarations can be \emph{extended} in separate modules. This
makes it possible to declare in one module all data model declarations of a
particular feature. For example, in the \emph{researchr} application, a
\lcr{Publication} can be \lcr{Tag}ged, which requires an extension of the
\lcr{Publication} entity. This extension is defined in the \lcr{tag}
module, together with the definition of the \lcr{Tag} entity.}

\paragraph{Versioning} Often, artifacts within DSL programs have to be tracked
over time. One alternative is to simply version the model files using existing
version control systems, or the version control mechanism built into the
language workbench. However, this requires users to interact with often complex
version control systems and prevents domain-specific adaptations of the version
control strategy.

The other alternative is to make versioning and tracking over time a part of the
language. For example, model elements can be tagged with version numbers or
specify a revision chain by pointing to a previous revision, and enforcing
compatibility constraints between those revisions. Instead of declaring explicit
versions, business data is often time-dependent, where different revisions of a
business rule apply to different periods of time. Support for these approaches
can be built directly into the DSL, with various levels of tool support.

\embc{No versioning is defined into the DSL. Users work with MPS'
integration with popular version control systems.}

\comparch{Interfaces can specify a \lcr{new version of} reference to another
interface. If they do so, a constraint enforces that the new version provides at
least the same operations as the old version, plus optional additional ones.
\lcr{new version of} can also be used between components. In this case, the new
version has to have the same (or additional) provided ports with the same
interfaces or new versions of these interfaces. It must have the same or fewer
required ports. Effectively, this means that the new version of something must
be replacement-compabible with the old version (the Liskov substitution
principle again).}

\pension{In the pension workbench, calculation declare have applicability
periods. This supports the evolution of calculation rules over time, while
retaining reproducability for calculations performed at an earlier point in
time. Since the Intentional Domain Workbench is a projectional tool, pension
plans can be shown with only the version of a rule valid for a given point in
time.}

\subsection{Behavior}
\label{parabeh}

The behavior expressed with a DSL must of course be aligned with the needs of
the domain. However, in many cases, the behavior required for a domain can be
derived from well-known behavioral paradigms, with slight adaptations or
enhancements, or simply interacting with domain-specific structures or data.

Note that there are two kinds of DSLs that don't make use of these kinds of
behavior descriptions. Some DSLs really just specify structures. Examples
include data definition languages or component description languages (although
both of them often use expressions for derived data, data validation or pre- and
post-conditions). Other DSLs only specify the kind of behavior expected, and the
generator creates the algorithmic implementation. For example, a DSL may
specify, simply with a tag such as \emph{async}, that the communication between
two components shall be asynchronous. The generator then maps this to an
implementation that behaves according to this specification.

\comparch{The component architecture DSL is an example of a structure-only DSL,
since it only describes black box components and their interfaces and
relationships. It uses the specification-only approach to specify whether a
component port is intended for synchronous or asynchronous communication.}

\embc{The component extension to provides a similar notion of interfaces, ports
and components as in the previous example. However, since here they are directly
integrated with C, C expression can be used for pre- and post-conditions of
interface operations (\fig{precond}).}

\begin{figure}[h]
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm}
  \includegraphics[width=8cm]{figures/embc/precond.png}
  \caption[][0.3cm]{Precondiditions can be added to interface operations. They
  are executed whenever any runnable that implements the particular operation
  is executed.}
  \label{precond}
  \rule{1\textwidth}{0.7pt}
\end{figure} 
 
\marginnote{This is only an overview over a few paradigms; many more
exist. I refer to the excellent Wikipedia entry on \emph{Programming Paradigms} and to
the book \cite{RoyH2004}}
This section described some of the most well-known behavioural paradigms that
can serve as useful starting points for behaviour descriptions in
DSLs.
 
\paragraph{Imperative} \marginnote[0.5cm]{For many people, often including
domain experts, this approach is most obvious. Hence it is often a good starting point
for DSLs.} Imperative programs consist of a sequence of statements, or
instructions, that change the state of a program. This state may be local to
some kind of module (e.g. a procedure or an object), global (as in global
variables) or external (when talking to periphery). Procedural and
object-oriented programming are both imperative, using different means for
structuring and (in case of OO) specialization. Because of aliasing and side
effects imperative programs are expensive to analyse. Debugging imperative
programs is straight forward and involves stepping through the instructions and
watching the state change.

\embc{Since C is used as a base language, this language is fundamentally
imperative. Some of the DSLs on top of it use other paradigms.}

\cooling{The cooling language uses various paradigms, but contains sequences
of statements to implement aspects of the overall cooling behaviour.}

\paragraph{Functional} Functional programming uses functions as the core
abstraction. A function's return value only depends on the values of its
arguments. Functions cannot access global state, no side effects are allowed.
Calling the same function several times with the same argument values has to
return the same value (that value may even be cached!). No aliasing (through
mutable memory cells) is supported, because values are immutable once they are
created. Since all dependencies of a computed value are local to a function (the
arguments), various kinds of analyses are possible. If assignment to variables
is supported, then it uses a form where a variable can only be assigned once. 

To create real-world programs, a purely functional language is usually not
sufficient, because it cannot affect the environment (after all, this is a side
effect). Since there is no state to watch change as the program steps through
instructions, debugging can be done by simply showing all intermediate results
of all function calls, basically "inspecting" the state of the calculation. This
makes building debuggers much simpler. Functional programming is often
used for certain parts ("calculation core") of a more complex system.

\pension{The calculation core of pension rules is functional. Consequently, a
debugger has been implemented that, for a given set of input data, shows the
rules as a tree where all intermediate results of each function call are shown
(\fig{debugger}). No "step through" debugger is necessary.}

\begin{figure}[h]
  \includegraphics[width=11cm]{figures/pension/debugger.png}
  \caption{Debugging functional programs can be done by showing the state of
  the calculation, for example as a tree.}
  \label{debugger}
\end{figure}

A relevant subset of functional programming is pure expressions (as in \ic{3*2+7
> i}. Instead of calling functions, operators are used. However, operators are
just inline-notations for function calls. Usually the operators are hard wired
into the language and it is not possible for users to define their own
functional abstractions. This is the main differentiator to functional
programming in general.

\embc{We use expressions in the guard conditions of the state machine
extension as well as in pre- and post-conditions for interface operations. In
both cases it is not possible to define or call external functions. Of course,
C's expression language is reused here.}

\paragraph{Declarative} Declarative programming can be considered the opposite
of imperative programming (and, to some extent, functional programming). A
declarative program does not specify any control flow. It does not specify a
sequence of steps of a calculation. Instead, a declarative program only
specifies what the program should accomplish, not how. This is often done by
specifying a set of properties, equations, relationships or constraints. Some
kind of evaluation engine then tries to find solutions. The particular advantage
of this approach is that it is not predefined how a solution is found, the
evaluation engine has a lot of freedom in doing so, possibly using different
solutions in different environments, or evolving the approach over
time\footnote[][-0.5cm]{For example, the strategies for implementing SAT solvers
have evolved quite a bit over time. SAT solvers are much more scalable today.
However, the formalism for describing the logic formulas that are processed by
SAT solvers have not changed}. This large degree of freedom often makes finding
the solution expensive --- trial and error, backtracking or exhaustive search
may be used. Debugging declarative programs can be hard since the solution
algorithm may be very complex and possibly not even be know to the user of
the language.

Declarative programming has many important subgroups and use cases. For
\emph{concurrent programs}, a declarative approach allows the efficient
execution of the same program on different parallel hardware. The compiler or
runtime system can allocate the program to available computational resources. In
\emph{constraint programming}, the programmer specifies constraints between a
set of variables. The engine tries to find values for these variables that
satisfy all constraints. Solving mathematical equation systems is an example,
as is solving sets of boolean logic formulas.

\comparch{This DSL specifies timing and resource characteristics for component
and interface operations. Based on this data, one could run an algorithm which
allocates the component instances to computing hardware so that the hardware is
used as efficiently as possible, while at the same time reducing the amount of
bus traffic. This is an example of constraint solving.}

\embc{This DSL supports presence conditions for product line engineering. A
presence condition is a boolean expression over a set of configuration features
that determines whether the associated piece of code is present for a given
combination of feature selections (\fig{presencecond}). To verify the structural
integrity of programs in the face of varying feature combination, constraint
programming is used (to ensure that there is no configuration of the program
where a reference to a symbol is included, but the referenced symbol is not).
From the program, the presence conditions and the feature model a set of boolean
equations is generated. A solver then makes sure they are consistent by trying
to find an example solution that violates the boolean equations.
\todo{cite: refer to DSL impl chapter, and to literature (Czarnecki)} }

\begin{figure}
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm}
  \includegraphics[width=8cm]{figures/embc/presencecond.png}
  \caption[][0.3cm]{Code affected by a presence condition is highlighted in
  blue. The presence condition is rendered on the left of the affected code. It
  is a boolean expression over a set of (predefined) configuration parameters.}
  \label{presencecond}
  \rule{1\textwidth}{0.7pt}
\end{figure} 


\genex{Another example for declarative programming is the type system DSL used
by MPS itself. Language developers specify a set of type equations containing
free type variables, among other things. A unification engine tries to solve the
set of equations by assigning actual types to the free type variables so that
the set of equations is consistent. We describe this approach in detail in
section \todo{ref}}

\emph{Logic programming} is another subparadigm of declarative programming where
users specify logic clauses (facts and relations) as well as queries. A theorem
prover tries to solve the queries\marginnote{The Prolog language works this
way}.

\paragraph{Reactive/Event-based/Agent} In this paradigm, behavior is triggered
based on events received by some entity. Events may be created by another entity
or by the environment (through a device driver). Reactions are expressed by the
production of other events. Events may be globally visible or explicitly routed
between entities, possibly using filters and/or using priority queues. This
approach is often used in embedded systems that have to interact with the real
world, where the real world produces events as it changes. A variant of this
approach queries input signals at intervals controlled by a scheduler and
considers changes in input signals as the events.

\cooling{The cooling algorithms are reactive programs that control the cooling
hardware based on environment events. Such events include the opening of a
refrigerator door, the crossing of a temperature threshold, or a timeout that
triggers defrosting of a cooling compartment. Events are queued, and the queues
are processed in intervals determined by a scheduler.}

Debugging is simple if the timing/frequency of input events can be controlled.
Visualizing incoming events and the code that is triggered as a reaction is
relatively simple. If the timing of input events cannot be controlled, then
debugging can be almost impossible, because humans are way too slow to fit "in
between" events that may be generated by the environment in rapid succession.
For this reason, various kinds of simulators are used to debug the behaviour of
reactive systems, and sophisticated diagnostics regarding event frequencies
or queue filling levels may have to be integrated into the programs as they run
in the real environment.

\cooling{The cooling language comes with a simulator (\fig{simulator})  based on
an interpreter where the behaviour of a cooling algorithm can be debugged. Events are
explicitly created by the user, on a time scale that is compatible
with the debugging process.}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=8cm]{figures/cooling/simulator.png}
  \caption{The simulator for the cooling language shows the state of the system
  (commands, event queue, value of hardware properties, variables and tasks).
  The program can be single-stepped. The user can change the value of
  variables or hardware properties as a means of interacting with the program.}
  \label{simulator}
\end{center} 
\end{figure} 


\begin{marginfigure}
  \includegraphics[width=4cm]{figures/graphicalFlow.png}
  \caption{Graphical Notation for Flow}
  \label{graphicalFlow2}
\end{marginfigure} 
\paragraph{Dataflow} The dataflow paradigm is centered around variables with
dependencies (relationships in terms of calculation rules) among them. As a
variable changes, those variables that depend on the changing variable are
recalculated. We know this approach mainly from two use cases. One is
spreadsheets: cell formulas express dependencies to other cells. As the values
in these other cells change, the dependent cells are updated. The other use case
is data flow (or block) diagrams (\fig{graphicalFlow2}, used in embedded
software, ETL systems and enterprise messaging and complex event processing.
There, the calculations are encapsulated in the blocks, and the lines represent
dependencies --- the output of one blocks "flows" into input slot of another
block. There are three different execution modes:

\begin{itemize}
  \item The first one considers the data values as continuous signals. At the
  time one of the inputs changes, all dependent values are recalculated, the
  change triggers the recalculation, and the recalculation ripples through the
  dependency graph. This is the model used in spreadsheets.
  \item The second one considers the the data values quantized, unique messages.
  Only if a message is available for all inputs, a new output message is
  calculated. The recalculation synchronizes on the availability of a message at
  each input, and upon recalculation, these messages are consumed. This approach
  is often used in ETL and CEP systems.
  \item The third approach is time triggered. Once again, the inputs are
  understood to be continuous signals, and a scheduler determines when a new
  calculation is performed. It also makes sure that the calculation "ripples
  through from left to right" in the correct order. This model is typically used
  in embedded systems.
\end{itemize}

Debugging these kinds of systems is relatively straight forward because the
calculation is always in a distinct state. Dependencies and data flow, or the
currently active block and the available messages can easily be visualized in a
block diagram notation. Note that the calculation rules themselves are
considered black boxes here, whose inside may be built from any other paradigm,
often functional. Integrating debuggers for the inside of boxes as well is a
more challenging task.


\paragraph{State-based} The state-based paradigm describes a system's behaviour
in terms of the states the system can be in, the transitions between these
states as well as events that trigger these transitions and actions that are
executed as states change. State machines are useful for systematically
organizing the behavior of an entity. It can also be used to describe valid
sequences of events, messages or procedure calls. State machines can be used in
an event-driven mode where incoming events actually trigger transitions and the
associated actions. Alternatively a state machine can be run in a timed mode,
where a scheduler determines when event queues are checked and processed. Except
for possible real-time issues, state machines are easy to debug by highlighting
the contents of event queues and the current state.

\embc{As mentioned before, this language provides an extension that supports
directly working with state machines. Events can be passed into a state machine
from regular C code or by mapping incoming messages in components to events in
state machines that reside in components. Actions can contain arbitrary C code,
unless the state machine shall be verifiable in which case actions may only
create outgoing events or change statemachine-local variables.}

\cooling{The behaviour of cooling programs is fundamentally state driven. A
scheduler is used to execute the state machine in regular intervals. Transitions
are triggered either by incoming, queued events or by changing property values
of hardware building blocks. Note that this language is an example where a
behavioral paradigm is used without significant alterations, but working with
domain-specific data structures: refrigerator hardware and their properties.}

\marginnote{A good introduction to model checking can be found in
\cite{BerardBidoitM2001}. We elaborate on model checking in section }.
\todo{ref}
As mentioned before, state-based behavior description is also interesting
because it support model checking: a state chart is verified against a set of
specifications. The model checker either determines that the state chart
conforms to the specifications or provides a counter example. Specifications
express something about sequences of states such as: "it is not possible that
two traffic lights show green at the same time" or "whenever a pedestrian
presses the \emph{request} button, the pedestrian lights eventually will show
green".

\subsection{Combinations}

Many DSLs use combinations of various behavioural and structural paradigms
described in this section\footnote{Note how this observation leads to the
desire to better modularize and reuse some of the above paradigms. Room for
research :-)}. A couple of combinations are very typical:

\begin{itemize}
  \item a data flow language often uses a functional or imperative language to
  describe the calculation rules that express the dependencies between the
  variables (the contents of the boxes in data flow diagrams or of cells in
  spreadsheets)
  \item state machines use expressions as transition guard conditions, as well
  as typically an imperative language for expressing the actions that are
  executed as a state is entered or left, or when a transition is executed.
  \item reactive programming, where "black boxes" react to events, often use
  data flow or state-based programming to implement the behaviour that
  determines the reactions.
  \item in purely structural languages, for example, those for expressing
  components and their dependencies, a functional/expression language is often
  used to express pre- and postconditions for operations. A state-based language 
  is often used for protocol state machines, which determines the valid order of
  incoming events or operation calls.
\end{itemize}

Some of the case studies used in this section of the book also use combinations
of several paradigms.

\pension{The pension language uses functional abstractions with mathematical
symbols for the core actuary mathematics. A functional language with a normal
textual syntax is used for the higher-level pension calculation rules. A
spreadsheet/data flow language is used for expressing unit tests for pension
rules. Various nesting levels of namespaces are used to organize the rules, the
most important of which is the pension plan. A plan contains calculation rules
as well as test cases for those rules. Pension plans can specialize other plans
as a means of expressing variants. Rules in a sub-plan can override rules in the
plan from which the sub-plan inherits. Plans can be declared to be abstract,
with abstract rules that have to be implemented in sub-plans. Rules are
versioned over time, and the actual calculation formula is part of the version.
Thus, a pension plan's behaviour can be made to be different for different
points in time. }

\cooling{The cooling behavior description is described as a reactive system.
Events are produced by hardware elements (and their drivers). A state machine
constitutes the top level structure. Within it, an imperative language is used.
Programs can inherit from another program, overwriting states defined in the
base program: new transitions can be added, and the existing transitions can be
overridden as a way for an extended program to "plug into" the base program.}

