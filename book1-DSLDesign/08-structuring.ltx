\section{Fundamental Paradigms}
\label{learnFromGPLs}

Every DSL is different. It is driven by the domain to which it applies. However,
as it turns out, there are also a number of commonalities between DSLs. These
can be handled by modularizing and reusing (parts of) DSLs as discussed in the
\emph{next} section. In \emph{this} section we look at common paradigms for
describing DSL structure and behaviour.


\subsection{Structure}

Languages have to provide means of structuring large programs in order to keep
them manageable. Such means include modularization and encapsulation,
specification vs. implementation, specialization, types and instances as well as
partitioning. \marginnote[-1.5cm]{The language design alternatives described in
this section are usually not driven directly by the domain, or the domain experts
guiding the design of the language. Rather, they are often brought in by the
language designer or the consumers of the DSL as a means of managing overall
complexity. For this reason they may be hard to "sell" to domain experts.}


\paragraph{Modularization and Visibility} DSL often provide some kind of logical
unit structure, such as namespaces or modules. Visibility of symbols may be
restricted to the same unit, or in referenced ("imported") units. Symbols may be
declared as public or private, the latter making them changeable without
consequences for using modules.\marginnote{Most contemporary programming
languages use some form of namespaces and visibility restriction as their top
level structure.} Some form of namespaces and visibility is necessary in almost
any DSL. Often there are domain concepts that can play the role of the module,
possibly oriented towards the structure of the organization in which the DSL is
used. 


\embc{As a fundamental extension to C, this DSL contains modules with
visibility specifications and imports. Functions, state machines, tasks and all
other top-level concepts reside in modules. Header files (which are
effectively a poor way of managing symbol visibility) are only used in the
generated low level code.}

\comparch{Components and interfaces live in name-spaces. Components are
implementation units, and are always private. Interfaces and data types may be
public or private. Namespaces can import each other, making the public elements of the
imported namespace visible to the importing namespace. The OSGi generator
creates two different bundles: an interface bundle that contains the public
artifacts, and an implementation bundle with the components. In case of a
distributed system, only the interface bundle is deployed on the client.}

\pension{Pension plans constitute namespaces. They are grouped into more
coarse-grained packages that are aligned with the structure of the pension
insurance business.}

\marginnote[0.8cm]{If a repository-based tool is used, the importance of
paritioning is greatly reduced. Although even in that case, there may be a set of federated and
distributed repositories that can be considered partitions}
\paragraph{Partitioning} Partitioning refers to the breaking down of programs
into several physical units such as files. These physical units do not have to
correspond to the logical modularization of the models within the partitions.
Typically each model fragment is stored in its own partition. For example, in
Java a public class has to live in a file of the same name (logical module ==
physical partition), whereas in C\# there is no relationship between namespace,
class names and the physical file and directory structure. A similar
relationship exists between partitions and viewpoints, although in most cases,
different viewpoints are stored in different partitions.

Partitioning may have consequences for language design. Consider a DSL where an
concept A contains a list of instances of concept B. The B instances then have
to be physically nested within an instance of A in the concrete syntax. If there
are many instances of B in a given model, they cannot be split into several
files. If such a split should be possible, this has to be designed into the
language. 

\comparch{A variant of this DSL that was used in another project had to be
changed to allow a namespaces to be spread over several files for
reasons of scalability and version-control granularity. In the initial version,
namespaces actually \emph{contained} the components and interfaces. In the
revised version, components and interfaces were owned by no other element, but
model files (partitions) had a namespace declaration at the top, logically
putting all the contained interfaces and components into this namespace. Since
there was no technical containment relationship between namespaces and its
elements, several files could now declare the same namespace. Changing this
design decision lead to a significant reimplementation effort because all kinds
of naming and scoping strategies changed.}


Other concerns influence the design of a partitioning strategy:

\begin{description}

\item[Change Impact] which partition changes as a consequence of a particular
change of the model (changing an element name might require changes to all
references to that element from other partitions)

\item[Link Storage] where are links stored (are they always stored in the model
that logically "points to" another one)?, and if not, how/where/when to control
reference/link storage.

\item[Model Organzation] Partitions may be used as a way of organizing the
overall model. This is particularly important if the tool does not provide a
good means of showing the overall logical structure of models and finding
elements by name and type. Organizing files with meaningful names in directory
structures is a workable alternative.

\item[Tool Chain Integration] integration with existing, file based tool chains.
Files may be the unit of checkin/checkout, versioning, branching or permission
checking. 
\end{description}

\marginnote[-1.5cm]{Another driver for using partitions is the scalability of
the DSL tool. Beyond a certain file size, the editor may become sluggish.}


It is often useful to ensure that each partition is processable separately to
reduce processing times. An alternative approach supports the explict definition
of those partitions that should be processed in a given processor run (or at
least a search path, a set of directories, to find the partitions, like an
include path in C compilers). You might even consider a separate build step to
combine the results created from the separate processing steps of the various
partitions (again like a C compiler: it compiles every file separately into an
object file, and then the linker handles overall symbol/reference resolution and
binding).

The partitioning scheme may also influence users' team collaboration when
editing models. There are two major collaboration models: real-time and
commit-based. In real-time collaboration, a user sees his model change when
another user changes that same model. Change propagation is immediate. A
database-backed repository is often a good choice regarding storage, since the
granularity tracked by the repository is the model element. In this case, the
partitioning may not be visible to the end user, since they just work "on the
repository". This approach is often (at least initially) preferred by
non-programmer DSL users.

The other collaboration mode is commit-based where a user's changes only make it
to the repository if he performs a \emph{commit}, and incoming changes are only
visible after a user has performed an \emph{update}. While this approach can be
used with database-backed repositories, it is most often used with file-based
storage. In this case, the partitioning scheme is visible to DSL users, because
it is those files they commit or update. This approach tends
to be preferred by developers, maybe because well-known versioning tools have
used the approach for a long time.



\paragraph{Specification vs. Implementation} Separating specification and
implementation supports plugging in different implementations for the same
specification and hence provides a way to "decouple the outside from the
inside".\marginnote{Interfaces, pure abstract classes, traits or function
signatures are a realization of this concept in programming languages.} This
supports the exchange of several implementations behind a single interface. This
is often required as a consequence of the development process: one stakeholder
defines the specification and a client, whereas another stakeholder provides one
or more implementations.

\embc{This DSL adds interfaces and components to C. Components provide or use
one or more interfaces. Different components can be plugged in behind the same
interface. In contrast to C++, no runtime polymorphism is supported, the
translation to plain C maps method invocation to flat function calls.}

\cooling{Cooling programs can refer to entities defined as part of the
refrigerator hardware as a means of accessing hardware elements (compressors,
fans, valves). To enable cooling programs to run with different, but similar
hardware configurations, the hardware structure can use "trait inheritance",
where a hardware trait defines a set of hardware elements, acting as a kind of
interface. Other hardware configurations can inherit these traits. As long as
cooling programs are only written against traits, they work with any
refrigerator that implements the particular set of traits against which the
program is written.}


\paragraph{Specialization} Specialization enables one entity to be a more
specific variant of another one. Typically, the more specific one can be used in
all contexts where the more general one is expected (the Liskov substitution
principle\cite{LiskovW94}). The more general one may be incomplete, requiring
the specialized ones to "fill in the holes".\marginnote{In GPLs, we know this
approach from class inheritance. "Leaving holes" is realized by abstract
methods.} Specialization in the context of DSLs can be used for implementing
variants or of evolving a program over time.
 
 
\pension{The customer using this DSL had the challenge of creating a huge set of
pension plans, implementing changes in relevant law over time, or implementing
related plans for different customer groups. Copying complete plans and then
making adaptations was not feasible for obvious reasons. Hence the DSL provides
a way for pension plans to inherit from one another. Calculation rules can be
marked \emph{abstract} (requiring overwriting in sub-plans), \emph{final} rules
are not overwritable. Visibility modifiers control which rules are considered
"implementation details".}

\cooling{A similar approach is used in the refrigerator DSL. Cooling programs
can specialize other cooling programs. Since the programs are fundamentally
state-based, we had to define what exactly it means to override a pumping 
program.}

\paragraph{Types and Instances} Types and instances refers to the ability to
define a structure that can be parametrized when it is
instantiated.\marginnote{In programming languages we know this from classes and
objects (where constructor parameters are used for parametrization) or from
components (where different instances can be connected differently to other
instances).}

\embc{Apart from C's \lcr{structs} (which are instantiatable data structures)
and components (which can be instantiated and connected), state machines can
be instantiated as well. Each instance can be in a different state at any given
time.}

\paragraph{Superposition and Aspects} Superposition refers to the ability to
merge several model fragments according to some DSL-specific merge operator.
Aspects provide a way of "pointing to" several locations in a program based on a
pointcut operator (essentially a query over a program or its execution),
adapting the model in ways specified by the aspect. Both approaches support the
compositional creation of many different model variants from the same set of
model fragments.\marginnote{This is especially important in the context of product
line engineering and is discussed in \cite{VoelterVisser2011} and in \todo{ref}
section of this book.}

\embc{This DSL provides a way of advising component definitions, for example to
introduce additional ports from an aspect. An aspect may introduce a port
\ic{provided port mon: IMonitoring} that allows a central monitoring component
to query the adviced components via the \lcr{IMonitoring} interface.}

\exwebdsl{Entity declarations can be \emph{extended} in separate modules. This
makes it possible to declare in one module all data model declarations of a
particular feature. For example, in the \emph{researchr} application, a
\lcr{Publication} can be \lcr{Tag}ged, which requires an extension of the
\lcr{Publication} entity. This extension is defined in the \lcr{tag}
module, together with the definition of the \lcr{Tag} entity.}

\todo{HERE}

\paragraph{Versioning} Often, artifacts within DSL programs tracked over time.
One alternative is to simply version the model files using existing version
control systems, or the version control mechanism built into the language
workbench. However, this requires users to interact with often large and complex
version control systems and prevents domain-specific adaptations of the version
control approach.

The other alternative is to make versioning and tracking over time a part of the
DSL definition. For example, model elements can be tagged with version numbers
or specify a revision chain by pointing to a previous version, and enforcing
compatibility constraints between those versions. Instead of versions, business
data is often time-dependent, where different revisions of a business rule apply
to different periods of time. Support for these approaches can be built directly
into the DSL, with various levels of tool support.

\embc{No versioining is integrated into the DSL. Users work with MPS'
integration with popular version control systems.}

\comparch{Interfaces can specify a \lcr{new version of} reference to another
component. If they do so, a constraint enforces that the new version provides
at least the same operations, plus optional additional ones, as the old
version. \lcr{new version of} can also be used among components. In this case,
the new version as to have the same (or additional) provided ports with the same
interfaces or new versions of these interfaces. It must have the same or fewer
required ports. Effectively, this means that the new version of something must
be replacement-compabible with the old version.}

\pension{In the pension workbench, calculation rules have validity dates.
This supports the evolution of calculation rules over time, while retaining
repdocuability for calculations performed at an earlier point in time. Since the
Intentional Domwin Workbench is a projectional tool, pension plans can be shown
with only the version of a rule for a given point in time.}

\subsection{Behavior}
\label{parabeh}

The behaviorr description of a DSL must of course be aligned with the needs of
the domain. However, in many cases, the behavior required for a domain can be
derived from well-known behavior description paradigms, with slight adaptations
or enhancements, or simply interacting with domain-specific structures or data.
Note also that there are two kinds of DSLs that don't make use of these kinds of
behavior descriptions. Some DSLs really just specify structures. Examples
include data definition languages or component description languages (although
both of them often use expressions for derived data, data validation or pre- and
postconditions). Another kind of DSL only specifies the kind of behavior required, and the
generator creates the algorithmic implementation. For example, a DSL may specify
that the communication between two parts of the system should be asynchronous.
THe generator then maps this to an implementation that behaves according to this
statement. 

\comparch{This component architecture DSL is an example of a structur-only DSL,
since it only described black box components and their interfaces and
relationships.}

In this section, I want to outline some of the most well-known behavioural
paradigms that can serve as useful starting points for behaviour descriptions in
DSLs.\marginnote{This is only an overview over a few paradigms; many more exist.
I refer to the excellent Wikipedia entry on \emph{Programming Paradigms} and to
the book \cite{RoyH2004}}
 
\paragraph{Imperative} \marginnote{For many people, often including domain
experts, this approach is most obvious. Hence it is often a good starting point
for DSLs.} Imperative programs consist of a sequence of statements, or
instructions, that change the state of a program. That state may be local to
some kind of module (e.g. a procedure), global or external (when talking to
periphery). Procedural and object-oriented programming are both imperative,
using different means for structuring and (in case of OO) specialization.
Because of the aliasing problem, imperative programs are expensive to analyse.
Debugging imperative programs involves stepping through the instructions and
watching the state change.

\embc{Since C is used as a base language, this language is fundamentally
imperative. Some of the DSLs on top of it use other paradigms.}

\cooling{The cooling language uses various paradigms, but contains sequences
of statements to implement aspects of the overall cooling behaviour.}

\paragraph{Functional} Functional programming uses the function as the core
abstraction. A function's return value can only depend on the values of its
parameters. Functions cannot access global state, no side effects are allowed.
Calling the same function several times with the same arguements has to return
the same value (that value may even be cached!). No aliasing (through mutable
memory cells) is supported, because values are immutable once they are created.
Since all dependencies of a computed value are local to a function (the
arguments), various kinds of analyses are possible. If assignment to variables
is supported, then it uses a form where a variable can only be assigned once. To
create real-world programs, a purely functional language is usually not
sufficient, because it cannot affect the environment. Since there is no state to
watch change as the program steps through instructions, debugging can be done by
simply showing all intermediate results of all function calls, basically
"inspecting" the state of the calculation. This makes building debuggers much
simpler. Functional programming is hence often used for certain parts
("calculation core") of a more complex system.

\pension{The calculation core of pension rules is functional. Consequently, a
debugger has been implemented that, for a given set of input data, shows the
rules as a tree where all intermediate results of each function call are shown.
No "step through" debugger is necessary.}

A relevant subset of functional programming is pure expressions (as in $3*2+7
> i$. Instead of calling functions, operators are used. However, operators are
just inline-notations for function calls. Usually the operators are hard wired
into the language, so it is not possible for users to define their own
functional abstractions. This is the main differentiator to functional
programming in general.

\comparch{This DSL uses expressions for pre- and post conditions in operations
in component interfaces.}

\embc{We use expressions in the guard conditions of the state machine
extension. Of course, C's expression language is reused here.}

\paragraph{Declarative} Declarative programming can be considered the opposite
of imperative programming (and, to some extent, functional programming). A
declarative program does not specify any control flow. It does not specify a set
of steps of a calculation. Instead, a declarative program only specifies a what
the program should accomplish, not how. This is often done by specifying a set
of properties of the program's goal. Some kind of evaluation engine then tries
to find solutions. The particular advantage of this approach is that it is not
predefined how a solution is found, the evaluation engine has a lot of freedom
in doing so, possibly using different solutions in different environments. This
large degree of freedom often makes finding the solution expensive. Debugging
declarative programs can be hard since the solution algorithm may be very
complex, involve (systematic) trial and error or backtracking, or may not even
be know to the user of the language. 

Declarative programming has many important subgroups and use cases. For
\emph{concurrent programs}, a declarative approach allows the efficient
execution of the same program on different parallel hardware. In
\emph{constraint programming}, the programmer specifies a constraints between a
set of variables. The engine tries to find values for these variables that
satisfy all constraints. Solving mathematical equation systems is an example. So
is efficient resource allocation in embedded systems. The evaluation engine is
often called a solver. \marginnote{Examples for boolean constraint programming
include the Alloy language or any other SAT solver.}.

\comparch{This DSL specifies timing and resource characteristics for component
and interface operations. Based on this data, one could run an algorthm which
allocates the component instances to computing hardware so that the hardware is
used as efficiently as possible, while at the same time reducing the amount of
bus traffic. This is an example of constraint solving.}

\embc{This DSL supports presence conditions for product line engineering. A
presence condition is a boolean expression over a set of configuration features
that determines whether the associated piece of code is present for a given
combination of feature selections. To verify the structural integrety of
programs in the face of varying feature comination, constraint programming is
used. From the program, the presence conditions and the feature model a set of
boolean equations is generated. A solver then makes sure they are consistent by
trying to find an example solution that violates the boolean equations.
\todo{cite: refer to DSL impl chapter, and to literature (Czarnecki)} }

\genex{Another example for declarative programming is the type system DSL used
by MPS itelf. Language developers specify a set of type equations containing
free type variables, among other things. A unification engine tries to solve
the set of equations by assigning types to the free type variables so that the
set of equations is consistent. We describe this approach in detail in section
\todo{cite}}

\emph{Logic programming} is another subparadigm of declarative programming where
users specify logic clauses (facts and relations) as well as queries. A theorem
prover tries to solve the queries.\marginnote{The Prolog language works this
way}.


\paragraph{Reactive/Event-based/Agent} In this paradigm, behavior is triggered
based on events received by some entity. Events may be created by another entity
or by the environment (or a driver). Reactions are expressed by the production
of other events. Events may be globally visible or explicitly routed between
entities, possibly using filter and/or using priority queues. This approach is
often used in embedded system that have to interact with the real world. The
real world produces events as it changes. 

A variant of this approach queries the inputs at intervals controlled by a
scheduler. The input events may be queued, or they may be considered continuous
signals, whose values are queried in scheduled intervals, and output signals are
created.

\cooling{The cooling algorithms are reactive programs that control the cooling
hardware based on environment events. Such events include the opening of a
refrigerator door, the crossing of a temperature schedule, or a timeout that
triggers defrosting of a cooling compartment. Events are queued, and the queues
are processed in intervals determined by a scheduler.}

Debugging is simple if the timing of input events can be controlled. Visualizing
incoming events and the code that is triggered as a reaction is relatively
simple. If the timing of input events cannot be controlled, then debugging can
be almost impossible, because humans are way too slow to fit "in between" events
that may be generated by the environment in rapid succession. For this reason,
various kinds of simulators are used to debug the behaviour of reactive systems.

\cooling{The cooling language comes with a simulator based on an interpreter
where the behaviour of a cooling algorithm can be debugged. Events are
explicitly created by the user, on a time scale that is compatible
with the debugging process.}

\paragraph{Dataflow} The dataflow paradigm is centered around variables with
dependencies (relationships in terms of calculation rules) among them. As a
variable changes, those that depend on the variable are recalculated. We know
this approach mainly from two use cases. One is spreadsheets: cell formulas
express dependencies to other cells. As the values in these other cells change,
the dependend cells are recalculated. The other use case is data flow (or block)
diagrams (\fig{graphicalFlow2}, used in embedded software, ETL systems and
enterprise messaging and CEP. There, the calculations are encapsulated in the
blocks, and the lines represent dependencies --- the output of one blocks
"flows" into input slot of another block. There are three different execution
modes:
\begin{marginfigure}
  \includegraphics[width=4cm]{figures/graphicalFlow.png}
  \caption{Graphical Notation for Flow}
  \label{graphicalFlow2}
\end{marginfigure} 

\begin{itemize}
  \item The first one considers the data values as continuous signals. At the
  time one of the inputs changes, all dependent values are recalculated. The
  change triggers the recalculation. This is the model used in spreadsheets.
  \item The second one considers the the data values quantized, unique messages.
  Only if a message is available for all inputs, a new output message is
  calculated. The calculation synchronized on the availability of a message at
  each input, and upon calculation, these messages are consumed. This approach
  is often used in ETL and CEP systems.
  \item The third approach is time triggered. Once again, the inputs are
  understood to be continuous signals, and a scheduler determines when a new
  calculation is performed. It also makes sure that the calculation "ripples
  through from left to right" in the correct order. This model is typically used
  in embedded systems.
\end{itemize}

Debugging these kinds of
systems is relatively straight forward because the calculation is always in a
distinct state. Dependencies and data flow, or the currently active block and
the available messages can easily be visualized in the spreadsheet or in the
block diagram notation. Note that the calculation rules themselves are
considered black boxes here.


\paragraph{State-based} The state-based paradigm describes a system's behaviour
in terms of the states the system can be in, the transitions between these
states as well as events that trigger these transitions and actions that are
executed as states change. State machines are useful for systematically managing
the behavior of an entity. It can also be used to describe valid sequences of
events, messages or procedure calls. It can be used in an event-driven mode
where incoming events actually trigger transitions and the associated actions.
Alternatively a state machine can be run in a timed mode, where a scheduler
determines when event queues are checked and processed. Except for possible
real-time issues, state machines are easy to debug by highlight the state of
queues and the current state.

\embc{As mentioned before, this language provides an extension that supports
directly working with state machines.}

\cooling{The behaviour of cooling programs is fundamentally state driven. A
scheduler is used to execute the state machine in regular intervals. 
Transitions are triggered either by incoming, queued events or by changing
property values of hardware elements. Note that this language is a good example
where a behavioral paradigm is used without significant alterations, but
working with domain-specific data structures: refrigerator hardware and their
properties.} 

State-based behavior description is also interesting because it support model
checking: a state chart is verified against a set of specifications. The model
checker either determines that the state chart conforms to the specifications or
provides a counter example. Specifications express something about sequences of
states such as: "it is not possible that two traffic lights show green at the
same time" or "whenever a pedestrian presses the \emph{request} button, the
pedestrian lights eventually will show green".\marginnote{A good introduction to
model checking can be found in \cite{BerardBidoitM2001}. We elaborate on model
checking in section \todo{cite}}.

\subsection{Combinations}

Many DSLs use combinations of various behavioural and structural paradigms
described here.\marginnote{Note how this observation leads to the desire to
better modularize and reuse some of the above paradigms. Room for research :-)}
A couple of combinations are very typical:

\begin{itemize}
  \item a data flow language often uses a functional or importative language to
  describe the calculation rules that express the dependencies between the
  variables (the contents of the boxes in data flow diagrams or of cells in
  spreadsheets)
  \item state machines use expressions as transition guard conditions, as well
  as typically an imperative language for expressing the actions that are
  executed as a state is entered or left, or when a transition is executed.
  \item reactive programming, where "black boxes" react to events, often use
  data flow or state-based programming to implement the behaviour that
  determines the reactions.
  \item in purely structural languages, for example, those for expressing
  components and their dependencies, a functional language is often used to
  express pre- and postconditions for operations. A state-based language is
  often used for protocol state machines, which determines the valid order of
  incoming events or operation calls.
\end{itemize}

Some of the case studies used in this section of the book also use combinations
of several paradigms.

\pension{The pension language uses functional abstractions with mathematical
symbols for the core actuary mathematics. A functional language with a normal
textual syntax is used for the higher-level pension calculation rules. A
spreadsheet/data flow language is used for expressing unit tests for pension
rules. Various nested levels of namespaces are used to organize the rules, the
most important of which is the pension plan. It contains rules and test cases
for those rules. Pension plans can inherit from other plans as a means of
expressing variants. Rules are versioned over time, and the actual calculation
formula is part of the version. Thus, a pension plan's behaviour can be made to
be different for different points in time. Rules in a sub-plan can override
rules in the plan from which the sub-plan inherits. Plans can be declared to be
abstract, with abstract rules that have to be implemented in sub-plans. }

\cooling{The cooling behavior is a reactive system. Events are produced by
hardware elements (and their drivers). A state machine constitutes the top
level structure. Within it, an imperative language is used. Programs can
inherit from another program, overwriting states defined in the base program.
New transitions can be added, and the existing transitions can be overridden.
This way, an extended program can "plug into" the base program.}

