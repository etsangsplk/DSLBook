\FloatBarrier


\section{Semantics and Execution}
\label{semantics}
 
Semantics can be partitioned into static semantics and execution semantics.
Static semantics are implemented by the constraints and type system rules.
Execution semantics denote the observable behavior of a program $p$ as it is
executed. We look at both aspects in this chapter; but we refer to execution
semantics if we don't explicitly say otherwise.

\marginnote[1\baselineskip]{There are also a number of approaches for formally
defining semantics independent of operational mappings to target languages. However, they
don't play an important role in real-world DSL design, so we don't address them
in this book.} 
Using a function $OB$ that defines this observable behavior we can define the
semantics of a program $p_{L_D}$ by mapping it to a program $q$ in a language
for $D_{-1}$ that has the same observable behavior:
$$semantics( p_{L_D} ) := q_{L_{D-1}}\;\;\;where\;OB(p_{L_D}) ==
OB(q_{L_{D-1}})$$
Equality of the two observable behaviors can be established with a
sufficient number of tests, or with model checking and proof (which is a lot of
effort and hence rarely done). This definition of semantics reflects the
hierarchy of domains and works both for languages that describe only structure,
as well as for those that include behavioral aspects.


The technical implementation of the mapping to $D_{-1}$ can be
provided in two different ways: a DSL program can literally be transformed into
a program in an $L_{D-1}$, or an interpreter can be written in $L_{D-1}$ or
$L_{D_0}$ to execute the program. Before we spend the rest of this section
looking at these two options in detail, we first briefly look at static
semantics.


\subsection{Static Semantics/Validation}
\label{validation}

Before establishing the execution semantics by transforming or interpreting the
program, its static semantics has to be validated. Constraints and type systems
are used to this end and we describe their implementation in Part III of the
book. Here is a short overview.


\parhead{Constraints} are simply Boolean expressions that
check some property of a model. For example, one might verify that the names
of a set of attributes of some entity are unique. For a model to be statically
correct, all constraints have to evaluate to \ic{true}. Constraint checking should
only be performed for a model that is structurally/syntactically
correct\sidenote[][-3\baselineskip]{In projectional systems, you cannot build
structurally/syntactically incorrect programs in the first place. For
parser-based systems, the AST, on which the constraint checks are performed,
often is not constructed unless the syntactic structure is correct. This
automatically leads to constraint checks being performed only on
structurally/syntactically correct models.}.
 
\marginnote[5mm]{Sometimes constraints are used instead of grammar rules. For
example, instead of using a $1..n$ multiplicity in the grammar, I often use
$0..n$ together with a constraint that checks that there is at least one
element. The reason for using this approach is that if the grammar mechanism is
used, a possible error message comes from the \emph{parser}. That error message
may read something like \emph{expecting SUCH\_AND\_SUCH, found SOMETHING\_ELSE}.
This is not very useful to the DSL user. If a more tolerant ($0..n$) grammar is
used, the constraint error message can be made to express a real domain
constraint (e.g.,~\emph{at least one SUCH\_AND\_SUCH is required, because
\ldots}).}
 

\begin{marginfigure}[10mm]
  \includegraphics[width=2.5cm]{figures-design/embc/states.png}
  \caption{An example state machine with a \emph{dead end} state, a
  state that cannot be left once entered (no outgoing transitions).}
  \label{states}  
\end{marginfigure}

\embc{One driver in selecting the linguistic abstractions that go into a 
DSL is the ability to easily implement meaningful constraints. For example, in
the state machine extension it is trivial to find states that have no
outgoing transitions (dead end, \fig{states}). In a functional language, such a
constraint could be written as \ic{states.select(s|!s.isInstanceOf(StopState)).
\\select(s|s.transitions.size == 0)}.} 

\noindent When defining languages and transformations, developers often have
certain constraints in their mind which they consider obvious. They assume that no one
would ever use the language in a particular way. However, DSL users may be
creative and actually use the language in that way, leading the transformation
to crash or create non-compilable code. Make sure that all constraints are
actually implemented. This can sometimes be hard. Only extensive (automated)
testing can prevent these problems from occurring.

In many cases, a multi-stage transformations is used where a model expressed in
$L_3$ is transformed into a model expressed in $L_2$, which is then in turn
transformed into a program expressed in $L_1$\footnote{Note how this also
applies to the classical case where $L_2$ is your DSL and $L_1$ is a GPL which
is then compiled!}. Make sure that \emph{every} valid program in $L_3$ leads to
a valid program in $L_2$. If the processing of $L_2$ fails with an error message
using abstractions from $L_2$ (e.g.,~compiler errors), users of $L_3$ will not be
able to understand them; they may have never seen the programs
generated in $L_2$. Again, automated testing is the way to address this issue.

If many or complex constraints (or type system rules) are executed on a large
model, performance may become an issue. Even if the DSL tool is clever about
this and only revalidates the constraints for those program elements that
changed, it can still be a problem if some kind of whole-model validation is
tied to a particular element. To solve this problem, many DSL tools allow users
to classify the constraints according to their cost (i.e.~performance overhead).
Cheap constraints are executed for each changing program element, in real-time,
as it changes. Progressively more expensive constraints are checked, for
example, as a fragment is saved or only upon explicit request by the user.


\parhead{Type Systems} are a special kind of constraints. Consider the example
of \ic{var int x = 2 * someFunction(sqrt(2));}. The type system constraint may
check that the type of the variable is the same or a supertype of the type of
the initialization expression. However, establishing the type of the init
expression is non-trivial, since it can be an arbitrarily complex expression. A
type system is a formalism or framework for defining the rules to establish the
types of arbitrary expressions, as well as type checking constraints.
It is a form of constraint checking. We cover the implementation of type systems
in Part III of the book (\sect{typesysimpl}).


\seppar When designing constraints and type system in a language, a
decision has to be made between one of two approaches: (a) declaration of
intent and checking for conformance and (b) deriving characteristics and
checking for consistency. Consider the following examples.

\embc{Variables have to be defined in the way shown above, where a type has to
be specified explicitly. A type specification expresses the intent that this
variable be, for example, of type \ic{int}. Alternatively, a type system could
be built to automatically derive the type of the variable declaration based on
the type of the \ic{init} expression, an approach called type inference. This
would allow the following code to be written:
\ic{var x = 2 * someFunction(sqrt(2));}. Since no type is explicitly specified,
the type system will infer the type of \ic{x} to be the type calculated for the
\ic{init} expression.}

\embc{State machines that are supposed to be verified by the model checker have
to be marked as \emph{verified}. In that case, additional constraints kick in
that report certain ways of writing actions as invalid, because they cannot be
handled by the model checker. An alternative approach could check a state
machine whether these "unverifiable" ways of writing actions are used, and if
so, mark the state machine as not verifiable.}

\pension{Pension plans can inherit from other plans (called the base plan). If a
pension calculation rule overrides a rule in the base plan, then the overriding
rule has to marked as \emph{overrides}. This way, if the rule in the base plan
is removed or renamed, validation of the sub plan will report an error. An
alternative design would simply infer the fact that a rule overrides another
one if they have the same name and signature.}

\noindent Note how in all three cases the constraint checking is done in two
steps. First we declare an \emph{intent} (variable is intended to be \ic{int},
this state machine is intended to be verifiable, a rule is intended to override
another one). We then \emph{check} if the program conforms to this intention.
The alternative approach would infer the fact from the program (the variable's
type is whatever the expression's type evaluates to, state machines are
verifiable if the "forbidden" features aren't used, rules override another one
if they have the same name and signature) \emph{without} any explicitly
specified intent.

When designing constraints and type systems, a decision has to be made regarding
when to use which approach. Here are some trade-offs. The
specification/conformance approach requires a bit more code to be written, but
results in more meaningful and specific error messages. The message can express
that fact that one part of a program does not conform to a specification made by
another part of the program\footnote{It also reduces the chance that users do
something they do not intend by mistake. For example, a user might not
\emph{want} to override a method from the base class, but it might happen just
because the user uses the same name and parameters.}. The derivation/consistency
approach is less effort to write and can hence be seen to be more convenient,
but it requires more effort in constraint checking, and error messages may be
harder to understand because of the missing, explicit "hard fact" about the
program.


The specification/conformance approach can also be used to "anchor" the
constraint checker, because a fixed fact about the program is explicitly given
instead of having it derived from a (possibly large) part of the program.
This decouples models and can increase scalability. Consider the following
example. A program contains a function call, and the type checker needs to check
the typing for this call. To do so, it has to determine the type of the called
function. Assume this function does \emph{not} specify the return type
explicitly, instead it is inferred from the returned expressions. These
expressions may be calls to other functions, so the process repeats. In the
worst case, a whole chain of function calls must be tracked down in this way to
calculate the type of the function initially called by your program. Notice that
this requires accessing all the downstream programs, so these all have to be
loaded and type checked! In large systems, this can lead to serious performance
and scalability issues\sidenote{We have seen such problems in practice with
large-scale models.}. If, instead, the type of the called function were given
explicitly, no downstream models need to be accessed or loaded.


\parhead{Multi-Level Constraints} Several sets of constraints can be used to
enforce multiple levels of correctness/strictness/compliance for models. The
first level are typically basic constraints (such as name uniqueness) and typing
rules. They are checked for every program. Additional levels are often optional.
They are triggered either by a configuration switch or by using the programs for
a given purpose. Additional levels always constrain programs \emph{further}
relative to more basic levels.


\embc{A nice example of multi-level constraints can be seen in the state
machines extension to C. Structural and type system correctness (for C and for
state machines) is always checked for every program. However, if a state machine
is marked as \ic{verifiable}, then the action code is further
restricted via additional constraints. For example, it is not allowed to read
and write the same variable within a single transition (i.e.~in the total of
exit actions, transition actions and entry actions). This is necessary to keep
the complexity of the generated model checker input code within limits.}
\embc{Another example concerns the use of floating point types. Some target
devices may not have floating point units (FPUs) which means that floating point
types (\ic{float}, \ic{double}) cannot be used in programs that should be
deployed on such a target device. So, as the user changes the target device in
the build configuration, additional constraints are checked that report floating
point types as errors if the target has no FPU).}

 
\subsection{Establishing the Correctness of the Execution Engine}

Earlier we have defined the meaning of the program $p$ at $D_n$ as the
equivalent observable behavior of a program $q$ at $D_{n-1}$. This essentially
\emph{defines} the transformation or interpreter to be correct. However, this is
useless in practice. As the language developer, we have a certain behavior in
mind, and we want to make sure that the executing DSL program exhibits this
behavior. We have to make sure that the execution engine executes the DSL
program accordingly.

\begin{marginfigure}[-20mm]
\begin{center}
  \includegraphics[width=4.5cm]{figures-design/gpltesting.png}
  \caption{Test code tests the application code based on a single understanding
  of the requirements towards the system.}
  \label{gpltesting}  
\end{center}
\end{marginfigure}
In classical programming, we write the GPL code based on our understanding of
the requirements. We then write unit tests, based on the same understanding,
which test the code we wrote before (\fig{gpltesting}). 


\begin{marginfigure}
\begin{center}
  \includegraphics[width=4.5cm]{figures-design/dsltesting.png}
  \caption{Using DSLs, a test written on $D-1$ tests the $D$ program \emph{as
  well as the transformation} from $D$ to $D-1$.}
  \label{dsltesting}  
\end{center}
\end{marginfigure}
In DSL testing, we write the DSL, the DSL program and the execution engine
based on our understanding of the requirements towards the system. We can still write
unit tests (in the GPL) based on this understanding to check for the correctness
of the executing DSL program (\fig{dsltesting}). 

Writing one DSL program and one unit test ensures that this one program executes
correctly with regards to the test case. Our goal here is, however, to ensure
that the transformation is correct \emph{for all programs} we can write with the
DSL. This can be achieved by writing many DSL programs and many tests --- enough
to make sure that every branch of the transformation is covered at least
once\footnote{For the Xpand code generator there is a coverage analysis tool
that can be used to make sure that for a given test suite, all branches of the
transformation template had been executed at least once.}. As always in testing,
we encounter the coverage problem: we have to write enough example programs and
tests to cover all aspects of the language and the execution engine. In
particular, we have to \emph{first think of the corner cases} to write tests for
them\footnote{The coverage problem can be solved in some cases by automatic test
case generation and formal verification. We discuss this later in this
chapter.}.


A variant of this approach is to express the test cases
in the DSL (after extending the DSL with a ways to express tests) and then
executing the application code and the test code on the target platform
together \fig{dsltesting2}. This is often more convenient since the tests can be
formulated more concisely on the level of the DSL. As we will see later, this
approach is especially useful if we have \emph{several} execution engines: we
write the test once and then execute it on all execution engines.

\begin{figure}[h] 
\begin{center}
  \includegraphics[width=8cm]{figures-design/dsltesting2.png}
  \caption{Test cases can also be expressed with the DSL and then executed on
  the target platform together with the application code.}
  \label{dsltesting2}  
\end{center}
\end{figure}

Note that the GPL program may have \emph{additional, unintended} behaviors not
prescribed by the DSL. These can often be exploited maliciously and are known as
safety or security problems. These will not be found by testing the GPL code
based on the requirements, but only by "trying to exploit" the program (known as
penetration testing).

We will elaborate more on ensuring the correctness of the execution semantics in
this chapter, as well as in the Part III chapter on DSL testing
(\sect{testingimpl}), where we discuss the implementation aspects of DSL and IDE
testing.


\subsection{Transformation} 
\label{semtrafo}

Transformations define the execution semantics of a DSL by mapping it to another
language. In the vast majority of cases a transformation for $L_D$ recreates
those patterns and idioms in $L_{D-1}$ for which it provides linguistic
abstraction. The result may be transformed further, until a level is reached for
which a language with an execution infrastructure exists --- often $D_0$. Code
generation, the case where we generate GPL code from a DSL is thus a special
case where $L_{D_0}$ code is generated.

\begin{marginfigure}[-40mm]
  \includegraphics[width=6cm]{figures-design/embc/lego.png}
  \caption{The robot control DSL is embedded in C program modules
  and provides linguistic abstractions for controlling a small lego car. It can
  accelerate, decelerate and turn left and right. }
  \label{lego}  
\end{marginfigure}


\embc{The semantics of state machines are defined by their mapping back to C
\ic{switch} statements. This is repeated for higher $D$ languages.
The semantics of the robot control DSL (\fig{lego}) is defined by its
mapping to state machines and tasks (\fig{robotcontrol}). To explain the
semantics to the users, prose documentation is available as well.}

\begin{marginfigure}
  \includegraphics[width=3.5cm]{figures-design/embc/robotcontrol.png}
  \caption{Robot control programs are mapped to state machines and
  tasks. State machines are mapped to C, and tasks are mapped to C as well as
  to operating system configuration files (a so-called OIL file). In the end,
  everything ends up in text for downstream processing by existing tools.}
  \label{robotcontrol}  
\end{marginfigure}

\comparch{The component architecture DSL only described interfaces, components
and systems. This is all structure-only. Many constraints about structural
integrity are enforced, and a mapping to a distribution middleware is
implemented. The formal definition of the semantics are implied by the mapping
to the executable code.}




\noindent DSL programs may be mapped to \emph{multiple} languages at the same
time. Typically, there is one primary language that is used for execution of the DSL
program (C in \fig{onTheSide}). The other languages may be used to configure the
target platform (generated XML files) or provide input for verification tools
(NuSMV in \fig{onTheSide}). In this case, one has to make sure that the
semantics of all generated representations is actually the same. We discuss this
problem in \sect{synctrans}.

\begin{marginfigure}[2mm]
  \includegraphics[width=5cm]{figures-design/onTheSide.png} \caption{From the C
  state machine extensions, we generate low-level C code (for execution) as well
  an input file for the NuSMV model checker (for verification).}
  \label{onTheSide}  
\end{marginfigure}


\embc{The state machines can be transformed to a representation in NuSMV, which
is a model checker that can be used to establish properties of state machines by
exhaustive search. Examples properties include freedom from deadlocks, assuring
liveness and specific safety properties such as "it will never happen that the
out events \emph{pedestrian light green} and
\emph{car light green} are set at the same time".}




\parhead{Multi-staged Transformation} There are several reasons why the gap
between a language at $D$ and its target platform may not be bridged by a single
transformation. Instead, the overall transformation becomes a chain of
subsequent transformations, an approach also known as cascading.

Multi-staged transformation is a form of modularization, and so the reason for
doing it the same reason we always use for modularization: breaking down a big
problem into a set of smaller problems that can be independently solved. In the
case of transformations, this "big problem" is a big semantic gap between the
DSL and the target language.\sidenote[][-5\baselineskip]{One could measure this
semantic gap between two languages: how many constructs do two languages share, how many
"synonyms" exist, how many constructs are the same but have different meanings?
In practice, the size of the gap is an intuition of the transformation
designer.}. Modularization breaks down this big semantic gap into several
smaller ones, making each of them easier to understand, test andmaintain\footnote{This approach can only be used if the tools supportmulti-staged transformation well. This is not true for all DSL tools.}.


Another reason for multi-stage transformations is the potential for reuse of 
each of the stages (\fig{trafomodularization}). Reusing lower $D$ languages
and their subsequent transformations also implies reuse of potentially
non-trivial analyses or optimizations that can be done at that particular
abstraction level\marginnote[-2\baselineskip]{This is on of the reasons why we
usually generate GPL source code from DSLs, and not machine code or byte code:
we want to reuse existing transformations and optimizations provided by the GPL
compiler.}. Consider GPL compilers as an example. They can be retargetted
relatively easily by exchanging the backends (machine code generation phases) or the frontend
(programming language parsers and analyzers). For example, GCC can generate code
for many different processor architectures (exchangeable backends), and it can
generate backend code for several programming languages, among them C, C++ and
Ada (exchangeable frontends). The same is possible for DSLs. The same high $D$
models can be executed differently by exchanging the lower $D$ intermediate
languages and transformations. Or the same lower $D$ languages and
transformations can be used for different higher $D$ languages, by mapping these
different languages to the same intermediate language.
\begin{marginfigure}[-20mm] 
  \includegraphics[width=5cm]{figures-design/trafomodularization.png}
  \caption{\emph{Left:} Backend-Reuse. Different languages ($L_3/L_2$ and $L_5$)
  are transformed to the same intermediate language $L_3$, reusing its backend
  generator to $L_0$. \emph{Right:} Frontend reuse. $L_3$ is mapped to $L_2$,
  which has wto sets of backend generators, reusing the $L_3$ to $L_2$
  transformation.}
  \label{trafomodularization}  
\end{marginfigure}

\embc{The embedded C language (and some of its higher $D$ extensions) have
various translation options, for several different target platforms (Win32 and
Osek), an example of backend reuse. All of them are C code, but we generate
different idioms in the code and different make files.}

  
\noindent Multi-stage transformation can also be a natural consequence of
incremental language extension along the domain hierarchy, where we repeatedly build
additional higher level languages on top of lower level languages. When
transforming the higher level languages, it is natural and obvious to transform
them onto the next lower level, and not onto a language at $D_0$.

\embc{The extensions to C are all transformed back to C idioms during
transformation. Higher-level DSLs, for example, a simple DSL for robot control,
are reduced to C plus some extensions such as state machines and tasks
(\fig{multistage_mbeddr}), reusing the transformations for those abstractions
back to C.}

\begin{figure} 
\begin{center}
  \includegraphics[width=8cm]{figures-design/multistage_mbeddr.png}
  \caption{Multi-Stage transformation in mbeddr. MPS supports multi-stage
  transformations really well, so managing the interplay of the set of
  transformations is feasible.}
  \label{multistage_mbeddr}  
\end{center}
\end{figure}


\noindent A special case of a multi-staged transformation is a preprocessor to a
code generator. Here, a transformation reduces the set of used language
concepts in a fragment to a minimal core, and only the minimal core is supported
in the code generator. Note how, in this case, the source and target languages
of the transformation are the same. However, the target model only uses a
\emph{subset} of the concepts defined by the source/target language. A
preprocessor simplifies portability of the actual code generator: it becomes
simpler, since only the subset of the language has to be mapped to code.  


\embc{Consider the case of a state machine where you want to be able to add an
"emergency stop" feature, i.e.~a new transition from each existing state to a
new STOP state. Instead of handling this case in the code generator a model
transformation script preprocesses the state machine model and adds all the new
transitions and the new emergency stop state (\fig{states2}). Once done, the
existing generator is run unchanged. You have effectively modularized the emergency stop concern into a
preprocessor transformation.}
\begin{marginfigure}[-80mm]
  \includegraphics[width=3.5cm]{figures-design/embc/states2.png}
  \caption{A transformation adds an emergency stop feature to a state machine.
  A new state is added ($S_{es}$), and a transition from each other state to
  that new state is added as well. The transition is triggered by the
  \emph{emergency stop} event (not shown).} 
  \label{states2}  
\end{marginfigure}


\comparch{The DSL describes hierarchical component architectures (where
components are assembled from interconnected instances of other components).
Most component runtime platforms don't support such hierarchical components, so
you need to "flatten" the structure for execution. Instead of trying to do this
in the code generator, you should consider a model transformation step to do it,
and then write a simpler generator that works with a flattened, non-hierarchical
model.}

\noindent Multi-Stage transformations can be challenging. It becomes harder to understand
what is going on in total. Debugging the overall transformation can become hard,
and good tool support is needed\sidenote{MPS addresses this problem by
(optionally) keeping all intermediate models around for debugging purposes. The
language developer can select a program element in any of the intermediate
models and have MPS show a trace where the element was transformed from, and
where it was transformed to. MPS also shows the transformation code involved in
each step. }




\parhead{Efficiency and Optimization} Transforming from $D$ to $D_{-1}$ allows
using sophisticated optimizations, potentially resulting in very efficient code.
DSL uses domain-specific abstractions and hence includes a lot of domain
semantics, so optimizations can take advantage of this and produce very
efficient $D_{-1}$ code. However, building such optimizations can be very
expensive. It is especially hard to build \emph{global} optimizations that
require knowledge about the structure or semantics of large or diverse parts of
the overall program. Also, an optimization will always rely on some set of rules
that determine when and how to optimize. There will always be corner cases where
an experienced developer will be able to write more efficient $D_{-1}$ code
manually. However, this requires a competent developer and, usually, a lot of
effort \emph{for each specific program}. A tool (i.e.~the transformation in this
case) will typically address the 90\% case well: it will produce reasonably
efficient code in the vast majority of cases with very little effort (once the
optimizations have been defined). In most cases, this is good enough --- in the
remaining corner cases, $D_{-1}$ has to be written
manually\sidenote[][-2\baselineskip]{This argument pro tools is used in GPLs for
garbage collection and optimizing compilers for higher level programming
languages.}.



\parhead{Care about generated code} Ideally, generated code is a throw-away
artifact, like object files in a C compiler. However, that's not quite true. At
least during development and test of the generator you may have to read,
understand and debug the generated code. For incomplete DSLs\sidenote{We cover
completeness in \sect{sect:completeness}.}, i.e.~those where parts of the
resulting program have to be written manually in $L_{D-1}$, readability and good structure
is even more important, because the manually written code has to be integrated
with the generated parts of the $L_{D-1}$ program. Hence, generated code should use
meaningful abstractions, should be designed well, use good names for
identifiers, be documented well, and be indented correctly. In short, generated
code should generally adhere to the same standards as manually written code.
This also helps to diffuse some of the skepticism against code generation that
is still widespread in some organizations.\marginnote[-3\baselineskip]{Note that
in complete languages (where 100\% of the $L_{D-1}$ code is generated), the generated code
is never seen by a DSL user. But even in this case, concerns for code quality
apply and the code has to be understood and tested during DSL and generator
development.} However, there are several exceptions to this rule:

\begin{itemize}

  \item Sometimes generating really well structured code makes the generator
  \emph{much} more complicated. You then have to decide whether you want to live
  with some less nicely structured generated code, or whether you want to
  increase generator complexity --- a valid trade-off, since the
  generator also needs to be maintained! A good example is \emph{import}
  statements when generating Java code. It can be a lot of work to find out
  exactly which imports are needed in a generated class.
  In this case it may be better to keep the generator simple and use fully 
  qualified class names throughout the code, and/or to import a few too many
  classes\sidenote{Xtend provides special support for this problem based on an
  \icsn{ImportManager}. It makes generating the correct imports relatively
  simple.}.
  
  \item Using a generator opens up additional options you wouldn't consider when
  writing code manually (and which are hence considered ugly). An example is
  generated collection classes. Imagine your models define entities, and from
  each entity you generate a Java Bean. In Java version 1.4 and earlier, Java
  did not have generics, so in order to work with collections of entities you
  would use the generic \ic{List} class. In the context of generated code you
  might want to consider generating a specific collection class for each entity,
  with an API typed to the respective Java Bean. This makes live much more
  convenient for those people who write Java code that \emph{uses} the generated Beans.

  \item The third exception to the rule is if the code has to be highly
  optimized for reasons of performance and code size. While you can still indent 
  your code well and use meaningful names, the \emph{structure} of the code may 
  be convoluted. Note, however, that the code would look the same way if it 
  were written by hand in that case.

\embc{The components extension to C supports components with provided and
required ports. A required port declares which interface it is expected to be
connected to. The same interface can be provided by different components,
implementing the interface differently. Upon translation of the component
extension, regular C functions are generated. An outgoing call on a required
port has to be routed to the function that has been generated to implement the
called interface operation in the target component. Since each component can be
instantiated multiple times, and each instance can have its required ports
connected to \emph{different} component instances (implementing the same
interface) there is no way for the generated code to know the particular
function that has to be called for an outgoing call on a required port for a
given instance. An indirection through function pointers in used instead.
Consequently, functions implementing operations in components take an additional
\ic{struct} as an argument which provides those function pointers for each
operation of each required port. A call on a required port thus is a relatively
ugly affair based on function pointers. However, to achieve the desired goal, no
different, cleaner code approach is possible in C. It is optionally possible to
\emph{restrict} a required port to a particular component
(\fig{restrictedport}). In this case, the target function is known statically
and no function pointer-based indirection is required. The resulting code is
cleaner and more efficient. Programmers trade flexibility for performance.}

\end{itemize}

\begin{figure}[h]
\fbox{
\begin{minipage}{105mm}
  \includegraphics[width=10cm]{figures-design/embc/restrictedport.png}
\end{minipage}
}
  \caption[][0.3cm]{The required port \icsn{lowlevel} is not just bound to the
  \icsn{ILowLevel} interface, but restricted tot the \icsn{ll} port of the
  \icsn{LowLevelCode} component. This way, it is statically known which C
  function implements the behavior and the generated code can be optimized.}
  \label{restrictedport} 
\end{figure}


\parhead{Platform} Code generators can become complex. The complexity can be
reduced by splitting the overall transformation into several steps --- see
above. Another approach is to work with a manually implemented, rich domain
specific platform. It typically consists of middleware, frameworks, drivers,
libraries and utilities that are taken advantage of by the generated code.
\begin{marginfigure}[-40mm]
  \includegraphics[width=4cm]{figures-design/platform.png} \caption{Typical
  layering structure of an application created using DSLs.}
  \label{platform}  
\end{marginfigure}
Where the generated code and the platform "meet" depends on the complexity of
the generator, requirements regarding code size and performance, the
expressiveness of the target language and the potential availability of
libraries and frameworks that can be used for the task. In the extreme case, the
generator just generates code to populate/configure the frameworks (which might
already exist, or which you have to grow together with the generator) or
provides statically typed facades around otherwise dynamic data structures.
Don't go too far towards this end, however: in cases where you need to consider
resource or timing constraints, or when the target platform is predetermined and
perhaps limited, code generation is the better approach: trying to make the
platform too generic or flexible will increase \emph{its} complexity.

\begin{marginfigure}[-30mm]
  \includegraphics[width=5cm]{figures-design/cave.png}
  \caption{Stalagmites and stalactites in limestone caves as a
  metaphor for a generator and a platform: the stalagmite represents the
  platform, it grows from up the lower abstraction levels. Stalagtites
  represent the transformations, which grow down from the high abstraction
  level represented by the DSL.}
  \label{cave} 
\end{marginfigure}


\embc{For most aspects, we use only a very shallow platform. This is mostly for
performance reasons and for the fact that the subset of C that is often used
for embedded systems does not provide good means of abstraction. For example,
state machines are translated to \ic{switch} statements. If we were to generate
Java code in an enterprise system, we may populate a state machine framework
instead. In contrast, when we translate the component definitions to the
AUTOSAR target environment, a relatively powerful platform is used --- namely
the AUTOSAR APIs, conventions and generators.}


\subsection{Interpretation} 



An interpreter is basically a program that acts on the DSL program it receives
as an input. How it does that depends on the particular paradigm used (see
\sect{parabeh}). For imperative programs  it steps through the statements and
executes their side effects. In functional programs, the interpreter
(recursively) evaluates functions. For declarative programs, some other
evaluation strategy, for example based on a solver, may be used. We describe
some of the details about how to design and implement interpreters in
\sect{interpreterimpl}.

\cooling{The DSL also supports the definition of unit tests for the
asynchronous, reactive cooling algorithm. These tests are executed with an
in-IDE interpreter. A simulation environment allows the interpreter to be used
interactively. Users can "play" with a cooling program, stepping through it in
single steps, watching values change.}

\pension{The pension DSL supports the in-IDE execution of rule unit tests by an
interpreter. In addition, the rules can be debugged. The rule language is
functional, so the debugger "expands" the calculation tree, and users can
inspect all intermediate results.}


\noindent For interpretation, the domain hierarchy could be exploited as well:
the interpreter for $L_D$ could be implemented in $L_{D-1}$. However, in practice we
see interpreters written in $L_{D_0}$. They may be extensible, so new
interpreter code can be added in case specialized languages define new language
concepts. 

The abstraction level of an interpreter must be decided. One alternative might
ignore for example the use of registers when performing an assignment, avoiding
problems resulting from parallelism. Alternatively, the interpreter might model
everything and thereby can address issues related to parallelism. In other
words, an interpreter defines a virtual machine and it is fundamental that this
virtual machine has an adequate abstraction level and/or the users are aware of
exactly what it means for the execution of the program on the target hardware if
the program runs on the virtual machine. 


\subsection{Transformation vs. Interpretation} 

When defining the execution semantics for a language, a decision has to be made
between transformation (code generation) and interpretation. Here are a couple 
of criteria to help with this decision.

\begin{description}
	\item[Code Inspection] When using code generation, the resulting code can
	be inspected to check whether it resembles code that had previously been written
	manually in the DSL's domain. Writing the transformation rules can be guided by
	the established patterns and idioms in $L_{D-1}$. Interpreters are meta
	programs and as such harder to relate to existing code patterns.
	
	\item[Debugging] Debugging generated code is straight forward if the code
	is well structured (which is up to the transformation) and an execution
	paradigm is used for which a decent debugging approach exists (not the case
	for many declarative approaches). Debugging interpreters is harder, because,
	they are meta programs. For example, setting breakpoints in the DSL program
	requires the use of conditional breakpoints in the interpreter, which are 
	typically cumbersome to use\sidenote[][-7\baselineskip]{This is
	especially useful during the development of the execution engine. Once a the DSL and the engine
	are finished, users should be able to debug DSL programs on the level of the
	DSL. However, since building DSL debuggers is not directly supported by most
	language workbenches, this is a lot of work --- and users are required to debug
	on $L_{D-1}$.}
	
	\item[Performance and Optimization] The code generator can perform
	optimizations that result in small and tight generated code. The compiler for
	the generated code may come with its own optimizations which are used
	automatically if source code is generated and subsequently
	compiled, simplifying the code generator\sidenote{For example, it is not
	necessary to optimize away calls to empty functions, \icsn{if} statements that 
	always evaluate to \icsn{true}, or arithmetic expressions
	containing only constants.}.
	Generally, performance is better in generated environments, since 
	interpreters always imply an additional layer of indirection during the
	execution of the program.
	
	\item[Platform Conformance] Generated code can be tailored to any target
	platform. The code can look exactly as manually written code would look, no
	support libraries are required. This is important for systems where the source code
	(and not the DSL code) is the basis for a contractual obligations or for review
	and/or certification. Also if artifacts need to be supplied to the
	platform that are not directly executable (descriptors, meta data), code generation is
	more suitable.
	
	\item[Modularization] When incrementally building DSLs on top of existing
	languages, it is natural to use transformations to $L_{D-1}$. \sidenote{While it is
	theoretically possible to also extend interpreters incrementally along a
	hierarchy of languages, I have not seen this in practice. Interpreters are
	typically written in a GPL.}
	
	\item[Turnaround Time] Turnaround time for interpretation is better than
	for generation: no generation, compilation and packaging step is required.
	Especially for target languages with slow compilers, large amounts of generated
	code can be a problem.
	
	\item[Runtime Change] In interpreted environments, the DSL program can be
	changed as the target system runs; the DSL editor can even be integrated into
	the target system\sidenote{The term data-driven system is often used in this
	case.}.
\end{description}


\cooling{There were two reasons for implementing the interpreter for the
cooling programs. The first was that initially we didn't have a code generator
because the target architecture was not yet defined. To be able to execute
cooling programs, we needed an interpreter and simulator. Second, the
turn-around time for the domain experts as the experimented with the DSL
programs is much reduced compared to generating, compiling and running C code.
The (interpreted) simulator also allowed the domain experts to run the programs
at a speed they could follow. This proved an important means of understanding
and debugging the asynchronous reactive cooling programs.}

\embc{This DSL exploits incremental extension to the C programming language
(inductive DSL definition). In this case it is natural to use transformation to
$L_{D-1}$ as a means of defining the semantics of extensions. Also, since the
target domain is embedded software, performance, code size and reuse of the
optimizations provided by the C compiler is essential. Interpretation was never
an option.}

\comparch{The driving factor for using generation over interpretation was
platform conformance. The reason for the DSL is to automate the generation of
target platform artifacts and thereby make working with the platform more
efficient.}

\pension{Turnaround time was important for the pension contract specification.
Also, the domain experts, as the created the pension plans, did not have access
to the final execution platform. An in-IDE interpreter was clearly the best
choice.}

\exwebdsl{Platform conformance was key here. Web applications have to use the
established web standards, and the necessary artifacts have to be generated. An
interpreted approach would not work in this scenario.}


\noindent Combinations between the two approaches are also possible. For
example, transformation can create an intermediate representation which is then
interpreted. Or an interpreter can generate code on the fly as a means of
optimization. While this approach is common in GPLs (e.g.,~the JVM), we
have not seen this approach used for DSLs. 

\subsection{Sufficiency}

A program fragment is \emph{sufficient for transformation T} if the fragment
itself contains all the data necessary to executed for the transformation.
While dependent fragments are by definition not sufficient without the transitive
closure of fragments they depend on, an independent fragment may be sufficient
for one transformation, and insufficient for another.

\cooling{The hardware structure is sufficient for a transformation that
generates an HTML doc that describes the hardware. It is insufficient regarding
the C code generator, since the behavior fragment is required as well.}

\noindent Sufficiency is important when large systems are concerned. An
sufficient fragment can be used for code generation without checking out and/or loading
other fragments. This supports modular, incremental transformations of only the
changed fragments, and hence, potentially significant improvements in
performance and scalability.


\subsection{Synchronizing Multiple Mappings} 
\label{synctrans}

Ensuring the semantics of the execution engine becomes more challenging if we
transform the program to \emph{several different} targets using several
different transformations. We have to ensure that the semantics of all resulting
programs are identical\footnote{At least to the extent we care --- we may not
care if one of the resulting programs is faster or more scalable. In fact, these
differences may be the very reason for having several mappings)}. In practice,
this case often occurs if an interpreter is used in the IDE for "experimenting"
with the models, and a code generator creates efficient code for execution in
the target environment. To synchronize the semantics in this case we recommend
providing a set of test cases that are expressed on DSL level, and that are
executed in all executable representations, expecting them to succeed in all. If
the coverage of these test cases is high enough to cover all of the observable
behavior, then it can be assumed with reasonable certainty that the semantics
are indeed the same\footnote{Strictly speaking they are just bug-compatible,
i.e.~they may all make the same mistakes.}.

\pension{The unit tests in the pension plans DSL are executed by an interpreter
in the IDE. However, as Java code is generated from the pension plan
specifications, the same unit tests are also executed by the generated Java
code, expecting the same results as in the interpreted version.}

\cooling{A similar situation occurs with the cooling DSL where an in
IDE-interpreter is used for testing and experimenting with the models, and a
code generator creates the executable version of the cooling algorithm that
actually runs on the microcontroller in the refrigerator. A suite of test cases
is used to ensure the same semantics.}



\subsection{Choosing between Several Mappings} 
\label{severalmapp}

Sometimes there are several \emph{alternative} ways how a program in $L_D$ can
be translated to a single $L_{D-1}$, for example to realize different
non-functional requirements (optimizations, target platform, tracing or
logging). There are several ways how one alternative may be selected.

\begin{itemize}

  \item In analogy to compiler switches, the decision can be controlled by
  additional external data. Simple parameters passed to the transformation are
  the simplest case. A more elaborate approach is to have an additional model,
  called an annotation model, which contains data used by the transformation to
  decide how to translate the core program. The transformation uses the $L_D$
  program and the annotation model as its input. There can be several different
  annotation models for the same core model that define several different
  transformations, to be used alternatively. An annotation model is a separate
  viewpoint (\sect{sec:viewpoints}) an can hence be provided by a different
  stakeholder than the one who maintains the core $L_D$ program.

  \item Alternatively, $L_D$ can be extended to directly contain additional data 
  to guide the decision. Since the data controlling the transformation is
  embedded in the core program, this is only useful if the DSL user can
  actually decide which alternative to choose, and if only one alternative 
  should be chosen for each program. Annotation models provide more flexibility.

  \item Heuristics, based on patterns, idioms and statistics extracted from the 
  $L_D$ program, can be used to determine the applicable
  transformation as well. Codifying these rules and heuristics can be hard
  though, so this approach is rarely used.
\end{itemize}

\noindent As we have suggested above in the case of multiple transformations of
the \emph{same} $L_D$ program, here too extensive testing must be used to make
sure that all translations exhibit the same semantics (except for the
non-functional characteristics that may be expected to be different, since they
often are the reason for the different transformations in the first place).


\subsection{Reduced Expressiveness and Verification} 

It may be beneficial to limit the expressiveness of a language. Limited
expressiveness often results in more sophisticated analyzability. For example,
while state machines are not very expressive (compared to fully fledged C),
sophisticated formal verification algorithms are available (e.g.,~model checking
using SPIN\sidenote[][-2\baselineskip]{http://spinroot.com} or
NuSMV\sidenote[][-1\baselineskip]{http://nusmv.fbk.eu/}). The same is true for first-order logic,
where satisfiability (SAT) solvers\cite{Mitchell05} can be used to check
programs for consistency. If these kinds of analyses are useful for the model
purpose, then limiting the expressiveness to the respective formalism may be a
good idea, even if it makes expressing certain programs in $D$ more
cumbersome\footnote{A simple example is to use integers with ranges
\icsn{int[0..10] x;} instead of general integers. This makes programs harder to
write (ranges must be specified every time) but easier to analyze.}. Possibly a
DSL should be partitioned into several sub-DSLs, where some of them are
verifiable and some are not.

\embc{This is the approach used here: model checking is provided for the state
machines. No model checking is available for general purpose C, so behavior
that should be verifiable must be isolated into a state machine explicitly.
State machines interact with their surrounding C program in a limited an
well-defined way to isolate them and make them checkable. Also, state machines
marked as \ic{verifiable} cannot use arbitrary C code in its actions. Instead, an
action can only change the values of variables local to the state machine and
set output events (which are then mapped to external functions or component
runnables). The key here is that the state machine is completely self-contained
regarding verification: adapting the state machine to its surrounding C program
is a separate concern and irrelevant to the model checker.}


\noindent However, the language may have to be reduced to the point where domain
experts are not able to use the language because the connection to the domain is too
loose. To remedy this problem, a language with limited expressiveness can be
used at $D_{-1}$. For analysis and verification, the $L_D$ programs are
transformed down to the verifiable $L_{D-1}$ language. Verification is performed
on $L_{D-1}$, mapping the results back to $L_D$. Transforming to a verifiable
formalism also works if the formalism is not at $D_{-1}$, as long as a mapping
exists. The problem with this approach is the interpretation of analysis results
in the context of the DSL. Domain users may not be able to interpret the results
of model checkers or solvers, so they have to be translated back to the DSL.
Depending on the semantic gap between the generated model checker input program
and the DSL, this can be very hard.





\subsection{Documentation} 

Formally, defining semantics happens by mapping the DSL concepts to $D_{-1}$
concepts for which the semantics is known. For DSLs used by developers, and for
domains that are defined inductively (bottom-up), this works well. For
application domain DSLs, and for domains defined deductively (top-down), this
approach is not necessarily good enough, since the $D_{-1}$ concepts has no
inherent meaning to the users and/or the domain. An additional way of defining
the meaning of the DSL is required. Useful approaches include prose
documentation\sidenote[][-2\baselineskip]{We suggest to always write such
documentation in tutorial style, or as FAQs. Hardly anyone reads "reference documentation":
while it may be complete and correct, it is boring to read and does not guide
users through using the DSL.} as well as test cases or simulators.
This way, domain users can "play" with the DSL and write down their expectations
formally in test cases.

\embc{The extensible C language comes with a 100 page PDF that shows how to use
the MPS-based IDE, illustrates the changes to regular C, provides examples for
all C extensions and also discusses how to use the integrated analysis
tools.}

\cooling{This DSL has a separate viewpoint for defining test cases where
domain experts can codify their expectations regarding the behavior of cooling
programs. An interpreter is available to simulate the programs, observe their
progress and stimulate them to see how they react.}

\pension{This DSL supports an Excel-like tabular notation for expressing test
cases for pension calculation rules (\fig{testcases}). The calculations are
functional, and the calculation tree can be extended as a way of debugging the rules.}

\begin{figure}[h] 
  \includegraphics[width=11cm]{figures-design/pension/testcases.png}
  \caption{Test cases in the pension language allow users to specify test data
  for each input value of a rule. The rules are then evaluated by an
  interpreter, providing immediate feedback about incorrect rules.}
  \label{testcases}  
\end{figure}

