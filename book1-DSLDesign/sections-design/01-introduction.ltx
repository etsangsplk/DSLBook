\part{DSL Design}

\vspace{10mm}
\noindent This part of the book has been written together with Eelco Visser of
TU Delft. You can reach him via \verb#e.visser@tudelft.nl#.
\vspace{10mm}


 

\noindent Throughout this part of the book we refer back to the five case
studies introduced in Part I of the book (\sect{expressivity}). We use a the
following notation:

\comparch{This refers to the component architecture case study described in
\sect{cscomparch}.}
\cooling{This refers to the refrigerator configuration case study described in
\sect{cscooling}.}
\embc{This refers to the mbeddr.com extensible C case study described in
\sect{csextc}.}
\pension{This refers to the pension plans case study described in
\sect{cspension}.}
\exwebdsl{This refers to the WebDSL case study described in \sect{cswebdsl}.}

\noindent Note that in this part of the book the examples will only be used to
illustrate DSL \emph{design} and the driving design decisions. Part II of the
book will then discuss the implementation aspects.

Some aspects of DSL design have been formalized with mathematical formulae.
These are intended as an additional means of explaining some of the concepts.
Formulae are able to state properties of programs and languages in an
unambiguous way. However, I want to emphasize that reading or understanding the
formulae is \emph{not} essential for understanding the language design
discussion. So if you're not into mathematical formulae, just ignore them.
 
This part consists of three chapters. In \sect{foundations} we introduce
important terms and concepts including domain, model purpose and the structure
of programs and languages. In \sect{dimensions} we discuss a set of seven
dimensions that guide the design of DSLs: expressivity, coverage, semantics,
separation of concerns, completeness, language modularization and syntax.
Finally, in \sect{learnFromGPLs} we look at well-known structural
and behavioral paradigms (such as inheritance or state based behaviour) and
discuss their applicability to DSLs.


\chapter{Conceptual Foundations}
\label{foundations}

\chapterabstract{This chapter provides the conceptual foundations for the
discussion of the design dimensions. It consists of three sections. The first
one, \emph{Program, Languages and Domain} defines some of the terminology around
DSL design we will use in the rest of this chapter. The second section briefly
address the \emph{Purpose} of programs as a way of guiding their design. And the
third section briefly introduces parser-based and projectional editing, since
some design considerations depend on this rather fundamental difference in DSL
implementation.}


\section{Programs, Languages and Domains}
\label{hierarchical}

Domain-specific languages live in the realm of \emph{programs},
\emph{languages} and \emph{domains}. So we should start by explaining what
these things are. We will then use these concepts throughout this part of the
book.


As part of this book's treatment of DSLs, we are primarily interested in
\emph{computation}, i.e.~we are aimed at creating executable
software\footnote{This is opposed to just communicating among humans or
describing complete systems.}. So let's first consider the relation between
programs and languages. Let's define $P$ to be the set of all conceivable
programs. A \emph{program} $p$ in $P$ is the \emph{conceptual} representation of
some \emph{computation} that runs on a universal computer (Turing machine). A
\emph{language} $l$ defines a structure and notation for \emph{expressing} or
\emph{encoding} programs from $P$. Thus, a program $p$ in $P$ may have an
expression in $L$, which we will denote as $p_l$.


There can be several languages $l_1$ and $l_2$ that express the \emph{same}
conceptual program $p$ in different way $p_{l_1}$ and $p_{l_2}$ (\ic{factorial}
can be expressed in Java and Lisp, for example). There may even be multiple ways
to express the same program in a single language~$l$ (in Java, \ic{factorial}
can be expressed via recursion or with a loop). A transformation $T$ between
languages $l_1$ and $l_2$ maps programs from their $l_1$ encoding to their $l_2$
encoding, i.e.~$T(p_{l_1}) = p_{l_2}$\marginnote[-5\baselineskip]{Notice that
this transformation only changes the language used to express the program. The
conceptual program does not change. In other words, the transformation preserves
the semantics of $p_{l_1}$. We will come back to this notion as we discuss
semantics in more detail in \sect{semantics}.}.


It may not be possible to encode all programs from $P$ in a given language $l$.  
We denote as $P_l$ the subset of $P$ that can be expressed in
$l$\marginnote{Turing-complete languages can by definition express all of $P$}.
More importantly, some languages may be \emph{better} at expressing certain
programs from $P$: the program may be shorter, more readable or more
analyzable.


\pension{The pension plan language is very good at representing pension
calculations, but cannot practically be used to express general purpose
enterprise software. For example, user defined data structures and loops are
not supported.}


\parhead{Domains} What are domains? We have seen one way of defining domains in
the previous paragraph. When we said that a language $l$ covers a subset of $P$,
we can simply call this subset the \emph{domain} covered with $l$. However, this
is not a very useful approach, since it equates the scope of a domain trivially
with the scope of a language (the subset of $P$ in that domain $P_D$ is equal to
the subset of $P$ we can express with a language $l$ $P_l$). We cannot ask
questions like: "Does the language adequately cover the domain?", since it
always does, by definition.


There are two more useful approaches. In the \emph{inductive} or
\emph{bottom-up} approach we define a domain in terms of existing software used
to address a particular class of problems or products. That is, a domain $D$ is
identified as a set of programs with common characteristics or similar purpose.
Notice how at this point we do \emph{not} imply a special language to express
them. They could be expressed in any Turing-complete language. Often such
domains do not exist outside the realm of software. An especially interesting
case of the inductive approach is where we define a domain as a subset of
programs written in a specific language $P_l$ instead of the more general set
$P$. In this case we can often clearly identify the commonalities among the
programs in the domain, in the form of their consistent use of a set of
domain-specific patterns or idioms\footnote{Some people have argued for a long
time that the need to use idioms or patterns in a language is a smell, and
should be understood as hints at missing language features:
\icsn{http://bit.ly/715wiF}}. This makes building a DSL for $D$ relatively simple,
because we know exactly what the DSL has to cover, and we know what code to
generate from DSL programs.

\embc{The domain of this DSL has been defined bottom-up. Based on idioms
commonly employed when using C for embedded software development, linguistic
abstractions have been defined that provide a "shorthand" for those idioms.
These linguistic abstractions form the basis of the language extensions.}


\noindent The above examples can be considered relatively general -- the domain
of embedded software development is relatively broad. In contrast, a domain may
also be very specific.

\cooling{The cooling DSL is tailored specifically towards expressing refrigerator
cooling programs for a very specific organization. No claim is made for
broad applicability of the DSL. However, it perfectly fits into the way
cooling algorithms are described and implemented in that particular
organization.}


\noindent The second approach for defining a domain is \emph{deductive} or
\emph{top-down}. In this approach, a domain is considered a body of knowledge
about the real world, i.e.~outside the realm of software. From this perspective,
a domain $D$ is a body of knowledge for which we want to provide some form of software
support. $P_D$ is the subset of programs in $P$ that
implement interesting computations in $D$. This case is much harder to address
using DSLs, because we first have to understand precisely the nature of the
domain and identify the interesting programs in that domain.


\pension{The pensions domain has been defined in this way. The customer had been
working in the field of old-age pensions for decades and had a very detailed
understanding of what the pension domain entails. That knowledge was mainly
contained in the heads of pension experts, in pension plan requirements
documents, and, to a limited extent, encoded in the source of existing
software.}




\begin{marginfigure}
  \includegraphics[width=5cm]{figures-design/domain.png}
  \caption{The programs relevant to a domain $P_D$ and the programs expressible
  with a language $P_L$ are both subsets of the set of all programs $P$. A good
  DSL has a large overlap with its target domain ($P_L \approx P_D$).}
  \label{fig:programs}
\end{marginfigure}
\noindent In the context of DSLs, we can ultimately consider a domain $D$ by a
set of programs $P_D$, whether we take the deductive or inductive route. There can be
multiple languages in which we can express $P_D$ programs. Possibly, $P_D$ can
only be partially expressed in a language $l$ (Figure~\ref{fig:programs}).



\parhead{Domain-Specific Languages} We can now understand the notion of a
domain-specific language. A \emph{domain-specific language} $l_D$ for a domain
$D$ is a language that is \emph{specialized} for encoding programs from $P_D$.
That is, $l_D$ is more efficient in representing $P_D$ programs
than other languages, and thus, is particularly well suited for
$P_D$\footnote{There are several ways of measuring efficiency. The most obvious
one is the amount of code a developer has to write to express a problem in the
domain: the more concise, the more efficient. We will discuss this in more
detail in \sect{expressivity}.}.
It achieves this by using \emph{abstractions} suitable to the domain, and
avoiding details that are irrelevant to programs in $D$ (typically because they
are similar in all programs and can be added automatically by the execution
engine).


It is of course possible to express programs in $P_D$ with a general-purpose
language. But this is less efficient -- we may have to write much more code,
because a GPL is not specialized to that particular domain. Depending on the
expressivity of a DSL, we may also be able to use it to describe programs
outside of $D$\footnote{For example, you \emph{can} write any program with some
dialects of SQL.}. However, this is often not efficient at all, because, by
specializing a DSL for $D$, we also restrict its efficiency for expressing
programs outside of $D$. This is not a problem as long as we have scoped $D$
correctly. If the DSL actually just covers a subset of $P_D$, and we have to
express programs in $D$ for which the DSL is \emph{not} efficient, we have a
problem.


This leads us to the crucial challenge in DSL design: finding regularity in a
non-regular domain and capturing it in a language. Especially in the deductive
approach, membership of programs in the domain is determined by a human and is,
in some sense, arbitrary. A DSL for the domain hence typically represents an
explanation or interpretation of the domain, and often requires trade-offs by
under- or over-approximation (Figure~\ref{fig:approximation}). This is
especially true while we develop the DSL: an iterative approach is necessary
that evolves the language as our understanding of the domain becomes more and
more refined over time. In a DSL $l$ that is adequate for the domain, the sets
$P_l$ and $P_D$ are the same.


\begin{marginfigure}[-5cm]
\begin{center}
  \includegraphics[width=5cm]{figures-design/domainApprox.png}
  \caption{Languages L1 and L2 under-approximate and over-approximate domain D.}
  \label{fig:approximation}
\end{center}
\end{marginfigure}


\parhead{Domain Hierarchy} In the discussion of DSLs and progressively higher
abstraction levels, it is useful to consider domains organized in a
hierarchy\sidenote[][-2\baselineskip]{In reality, domains are not always as
neatly hierarchical as we make it seem in this section. Domains may overlap, for
example. Nonetheless, the notion of a hierarchy is very useful for discussing many
of the advanced topics in this book. In terms of DSLs, overlap may be addressed by
factoring the common aspects into a separate language module that can be used in
both the overlapping domains.}, in which higher domains are a subset (in terms
of scope) of the lower domains (\fig{hierarchicalDomains}).



At the bottom we find the most general domain $D_0$. It is the domain of all
possible programs $P$. Domains $D_n$, with $n > 0$, represent progressively more
specialized domains, where the set of interesting programs is a subset of those
in $D_{n-1}$ (abbreviated as $D_{-1}$). We call $\D{+1}$ a subdomain of D. For
example, $\D{1.1}$ could be the domain of embedded software, and $\D{1.2}$ could
be the domain of enterprise software. The progressive specialization can be
continued ad infinitum, in principle. For example, $\D{2.1.1}$ and $\D{2.1.2}$
are further subdomains of $\D{1.1}$: $\D{2.1.1}$ could be automotive embedded
software and $\D{2.1.2}$ could be avionics
software\sidenote[][-2\baselineskip]{At the top of the hierarchy we find
singleton domains that consist of a single program (a non-interesting boundary case).}.

\begin{figure}[h]
  \includegraphics[scale=0.7]{figures-design/hierarchicalDomains.png}
  \caption{The domain hierarchy. Domains with higher index are called subdomains
  of domains with a lower index ($D_1$ is a subdomain of $D_0$). We use just $D$
  to refer to the current domain, and $\D{+1}$ and $\D{-1}$ to refer to the
  relatively more specific and more general ones.}
  \label{hierarchicalDomains} 
\end{figure}

Languages are typically designed for a particular domain $D$. Languages for
$\D{0}$ are called general-purpose languages\sidenote{We could
define $D_0$ to be those programs expressible with Turing machines, but using GPLs for
$D_0$ is a more useful approach for this book.}. Languages for $D_n$ with $n >
0$ become more domain-specific for growing $n$. Languages for a particular $D_n$
can also be used to express programs in $D_{n+1}$. However, DSLs for $D_{n+1}$
may add additional abstractions or remove some of the abstractions found in
languages for $D_{n}$. To get back to the embedded systems domain, a DSL for
$\D{1.1}$ could include components, state machines and data types with physical
units. A language for $\D{2.1.1}$, automotive software, will retain these
extensions, but in addition provide direct support for the AUTOSAR standard. To
conform to the MISRA-C standard, C's \ic{void*} may be prohibited or removed.



\embc{The C base language is defined for $D_0$. Extensions for tasks, state
machines or components can argued to be specific to embedded systems, making
those sit in $D_{1.1}$. Progressive specialization is possible; for example, a
language for controlling small Lego robots sits on top of state machines and
tasks. It could be allocated to $D_{2.1.1}$.}




\section{Purpose}

We have said earlier that there can be several languages for the same domain.
These languages differ regarding the abstractions they make use of. Deciding
which abstractions should go into a particular language for $D$ is not always
obvious. The basis for the decision is to consider the \emph{model purpose}.
Models\sidenote[][-4\baselineskip]{As we discuss below, we use the terms program
and model as synonyms.}, and hence the languages to express them, are intended
for a specific purpose. Examples of model purpose include automatic derivation
of a $D_{-1}$ program, formal analysis and model checking, platform-independent
specification of functionality or generation of
documentation\sidenote[][-4\baselineskip]{Generation of documentation is
typically not the main or sole model purpose, but may be an important secondary
one. In general, we consider models that only serve communication among humans
to be outside the scope of this book, because they don't have to be formally defined
to achieve their purpose.}. The same domain concepts can often be abstracted in
different ways, for different purposes. When defining a DSL, we have to identify
the different purposes required, and then decide whether we can create one DSL
that fits all purposes, or create a DSL for each purpose\footnote{Defining
several DSLs for a single domain is especially useful if different stakeholders
want to express different aspects of the domain with languages suitable to their
particular aspect. We discuss this in the chapter on Viewpoints
\sect{sec:viewpoints}}.


\embc{The model purpose is the generation of an efficient low-level C
implementation of the system, while at the same time providing software
developers with meaningful abstractions. Since \emph{efficient} C code has to be
generated, certain abstractions, such as dynamically growing lists or runtime
polymorphic dispatch, are not supported even though they would be convenient for
the user. The state machines in the \ic{statemachines} language have an additional
model purpose: model checking, i.e.~proving certain properties about the state
machines (e.g.,~proving that a certain state is definitely going to be reached
after some event occurs). To make this possible, the action code used in the
state machines is limited: it is not possible, for example, to read and write
the same variable in the same action.}


\cooling{The model purpose is the generation of efficient implementation code
for various different target platforms (different types of refrigerators use
different electronics). A secondary purpose is enabling domain experts to
express the algorithms and experiment with them using simulations and tests. The
DSL is not expected to be used to visualize the actual refrigerator device 
for sales or marketing purposes.}


\pension{The model purpose of the pension DSL is to enable insurance
mathematicians and pension plan developers (who are not programmers) to define
complete pension plans, and to allow them to check their own work for
correctness using various forms of tests. A secondary purpose is the generation
of the complete calculation engine for the computing center and the website.}


\noindent The purpose of a DSL may also change over time. Consequently, this may
require changes to the abstractions or notations used in the language. From a technical
perspective, this is just like any other case of language evolution (discussed
in \sect{process}).



\section{The Structure of Programs and Languages}
\label{structureofprograms}

The discussion above is relatively theoretical, trying to capture somewhat
precisely the inherently imprecise notion of domains. Let us now move into the
field of language engineering. Here we can describe the relevant concepts in a
much more practical way.

\begin{marginfigure}[-85mm]
\begin{center}
  \includegraphics[width=3cm]{figures-design/csVsAs.png} 
  \caption{Concrete and abstract syntax for a textual variable declaration.
  Notice how the abstract syntax does not contain the keyword \icsn{var} or the
  symbols \icsn{:} and \icsn{;}.}
  \label{csVsAs} 
\end{center} 
\end{marginfigure}

\marginnote[-5mm]{The 
abstract syntax is very similar to a meta model in that it represents only a
data structure and ignores notation. The two are also different: the abstract
syntax is usually automatically derived from a grammar, whereas a meta model is
typically defined \emph{first}, independent of a notation. This means that,
while the abstract syntax may be structurally affected by the grammar, the meta
model is "clean" and represents purely the structure of the domain. In practice,
the latter isn't strictly true either, since editing and tool considerations
typically influence a meta model as well. In this book, we consider the two to
be synonyms.}
\parhead{Concrete and Abstract Syntax} Programs can be represented in their
abstract syntax and the concrete syntax forms. The \emph{concrete syntax} is the
notation with which the user interacts as he edits a program. It may be textual,
symbolic, tabular, graphical or any combination thereof. The \emph{abstract
syntax} is a data structure that represents the semantically relevant data
expressed by a program. It does not contain notational details such as keywords,
symbols, white space or positions, sizes and coloring in graphical notations.
The abstract syntax is used for analysis and downstream processing of programs. A
language definition includes the concrete and the abstract syntax, as well as
rules for mapping one to the other. \emph{Parser-based} systems map the concrete
syntax to the abstract syntax. Users interact with a stream of characters, and a
parser derives the abstract syntax by using a grammar and mapping rules.
\emph{Projectional} editors go the other way round: user interactions, although
performed through the concrete syntax, \emph{directly} change the abstract
syntax. The concrete syntax is a mere projection (that looks and feels like text
in case a textual projection is used). No parsing takes place. Spoofax and
Xtext are parser-based tools, MPS is projectional.  



\begin{marginfigure}[-3mm] \begin{center}  
\includegraphics[width=3cm]{figures-design/programsAsTrees.png}
  \caption{A program is a tree of program elements, with a single root element.}
  \label{programsAsTrees}
\end{center} 
\end{marginfigure}
While concrete syntax modularization and composition can be a challenge and
requires a discussion of textual concrete syntax details, we will illustrate
most language design concerns based on the abstract syntax. The abstract syntax
of programs are primarily trees of program \emph{elements}. Each element is an
instance of a \emph{language concept}, or \emph{concept} for short. A
language is essentially a set of concepts (we'll come back to this below).
Every element (except the root) is contained by exactly one parent element.
Syntactic nesting of the concrete syntax corresponds to a parent-child 
relationship in the abstract syntax. There may also be any number of 
non-containing cross-references
between elements, established either directly during editing (in projectional
systems) or by a name resolution (or \emph{linking}) phase that follows parsing
and tree construction.


\parhead{Fragments} A program may be composed from several program
\emph{fragments}. A fragment is a standalone tree, a partial program.
Conversely, a program is a set of fragments connected by references (discussed
below). $E_f$ is the set of program elements in a fragment $f$.
\begin{marginfigure}[-30mm]
\begin{center}
  \includegraphics[width=3cm]{figures-design/fragment.png} 
  \caption{A fragment is a program tree that stands for itself and
  potentially references other fragments.}
  \label{fragment}
\end{center} 
\end{marginfigure}





\parhead{Languages} A language $l$ consists a set of language concepts $C_l$ and
their relationships\sidenote[][-1\baselineskip]{In the world of grammars, a
concept is essentially a \emph{Nonterminal}. We will discuss the details about grammars in
the implementation section of this book}. We use the term \emph{concept} to refer to
concrete syntax, abstract syntax plus the associated type system rules and 
constraints as well as some definition of its semantics. In a fragment, each 
element $e$ is an instance of a concept $c$ defined in some language $l$. 
\begin{marginfigure}
\begin{center}
  \includegraphics[width=4cm]{figures-design/langAsSetOfConcepts.png} 
  \caption{A language is a set of concepts.}
  \label{langAsSetOfConcepts}
\end{center} 
\end{marginfigure}

\embc{In C, the statement \ic{int x = 3;} is an instance of the
\ic{LocalVariableDeclaration} concept. \ic{int} is an instance of
\ic{IntType}, and the \ic{3} is an instance of \ic{NumberLiteral}.}

\begin{figure}[h] 
\begin{center}
  \includegraphics[width=8cm]{figures-design/conceptOf.png}
  \caption[][30mm]{The statement \icsn{int x = 3;} is an instance of the
\icsn{LocalVariableDeclaration}. $co$ returns the concept for a given element.}
  \label{conceptOf} 
\end{center}
\end{figure}



\parhead{Functions} We define the \emph{concept-of} function $co$ to return
the concept of which a program element is an instance: $co \Rightarrow
\mathit{element} \to \mathit{concept}$ (see \fig{conceptOf}). Similarly we
define the \emph{language-of} function $lo$ to return the language in which a
given concept is defined: $lo \Rightarrow
\mathit{concept} \to \mathit{language}$. Finally, we define a \emph{fragment-of}
function $fo$ that returns the fragment that contains a given program element:
$fo \Rightarrow \mathit{element} \to \mathit{fragment}$ (\fig{fragmentOf}).
\begin{marginfigure}[-17mm]
\begin{center}
  \includegraphics[width=5cm]{figures-design/fragmentOf.png} 
  \caption{$fo$ returns the fragment for a given element.}
  \label{fragmentOf}
\end{center} 
\end{marginfigure}


\parhead{Relations} We also define the following sets of relationships between
program elements. $\mathit{Cdn_f}$ is the set of parent-child relationships in a
fragment $f$. Each $c \in Cdn$ has the properties $parent$ and $child$ (see
figure \fig{sets}; $Cdn$ are all the parent-child "lines" in the picture).

\embc{In \ic{int x = 3;} the local variable declaration is the $parent$ of the
\ic{type} and the \ic{init} expression \ic{3}. The concept \ic{Local-
VariableDeclaration} defines the containment relationships \ic{type} and
\ic{init}, respectively.}

\begin{marginfigure}[10mm]
\begin{center}
  \includegraphics[width=4cm]{figures-design/sets.png} 
  \caption{$fo$ returns the fragment for a given element.}
  \label{sets}
\end{center} 
\end{marginfigure}
$\mathit{Refs_f}$ is the set of non-containing cross-references between program
elements in a fragment $f$. Each reference $r$ in $\mathit{Refs_f}$ has the
properties $from$ and $to$, which refer to the two ends of the reference
relationship (see figure \fig{sets}).

\embc{For example, in the \ic{x = 10;} assignment, \ic{x} is a reference to
a variable of that name, for example, the one declared in the previous example
paragraph. The concept \ic{LocalVariableRef} has a non-containing reference
relationsip \ic{var} that points to the respective variable.}

\noindent Finally, we define an inheritance relationship that applies the Liskov
Substitution Principle (LSP) to language concepts. The LSP states that, 


\vspace{2mm}
\noindent\emph{In a
computer program, if S is a subtype of T, then objects of type T may be replaced
with objects of type S (i.e., objects of type S may be substitutes for objects
of type T) without altering any of the desirable properties of that program
(correctness, task performed, etc.)} 
\vspace{2mm}

The LSP is well known in the context of
object-oriented programming. In the context of language design it implies that a
concept $c_{sub}$ that extends another concept $c_{super}$ can be used in places
where an instance of $c_{super}$ is expected. $\mathit{Inh_l}$ is the set of
inheritance relationships for a language $l$. Each $i \in \mathit{Inh_l}$ has
the properties $super$ and $sub$.
\begin{marginfigure}[-63mm]
\begin{center}
  \includegraphics[width=5.5cm]{figures-design/inheritance.png} 
  \caption{Concepts can extend other concepts. The base concept may be defined
  in a different language.}
  \label{inheritance}
\end{center} 
\end{marginfigure}

\embc{The \ic{LocalVariableDeclaration} introduced above extends the concept
\ic{Statement}. This way, a local variable declaration can be used wherever a
\ic{Statement} is expected, for example, in the body of a function, which
is a \ic{StatementList}.}



\parhead{Independence} An important concept is the notion of independence. An
\emph{independent language} does not depend on other languages. This means that
for all parent/child, reference and inheritance relationships, both ends refer
to concepts defined in the same language. Based on our definitions above we can
define an independent language $l$ as a language for which the following
hold\footnote{As I said in the introduction to this part of the book, formulae
like these are not essential for understanding. You may ignore them if you
like.}:
\begin{align}
\forall r \in \mathit{Refs_l} &\mid \mathit{lo(r.to)} = 
	\mathit{lo(r.from)} = l
\\ 
\forall s \in \mathit{Inh_l} &\mid \mathit{lo(s.super)} = 
	\mathit{lo(s.sub)} = l
\\ 
\forall c \in \mathit{Cdn_l} &\mid \mathit{lo(c.parent)} = 
     \mathit{lo(c.child)} = l
\end{align}

Independence can also be applied to fragments. An \emph{independent fragment} is
one in which all non-containing cross-references $Refs_f$ point to elements within
the same fragment:
\begin{align}
\forall r \in \mathit{Refs_f} &\mid \mathit{fo(r.to)} 
	= \mathit{fo(r.from)} = f
\end{align}

Notice that an independent language $l$ can be used to construct dependent
fragments, as long as the two fragments just contain elements from this single
language $l$. Vice versa, a dependent language can be used to construct
independent fragments. In this case we just have to make sure that the 
non-containing cross references are "empty" in the elements in fragment $f$.


\cooling{The hardware definition language is independent, as are fragments that
use this language. In contrast, the cooling algorithm language is dependent.
The \ic{BuildingBlockRef} concept declares a reference to the
\ic{BuildingBlock} concept defined in the hardware language (\fig{dependent}).
Consequently, if a cooling program refers to a hardware setup using an instance of
\ic{BuildingBlockRef}, the fragment becomes dependent on the hardware
definition fragment that contains the referenced building block.}

\begin{marginfigure}[-50mm]
  \includegraphics[width=6cm]{figures-design/cooling/dependent.png} 
  \caption{A \icsn{BuildingBlockRef} references a hardware element from within a
  cooling algorithm fragment.}
  \label{dependent}
\end{marginfigure}


\parhead{Homogeneity} We distinguish \emph{homogeneous} and \emph{heterogeneous}
fragments. A homogeneous fragment is one in which all elements are expressed with
the same language (see formula 1.5). This means that for all parent/child
relationships ($Cdn_f$), the elements at both ends of the relationship have to
be instances of concepts defined in one language $l$ (1.6):
\sidenote[][-20mm]{An independent language can only express homogeneous
fragments.
However, a homogeneous fragment can be expressed with a dependent language if
the dependencies all come via the $Refs$ relationship.}
\begin{align} 
\forall e \in E_f &\mid \mathit{lo(co(e))} = l \\
\forall c \in \mathit{Cdn_f} &\mid \mathit{lo(co(c.parent))} = 
	\mathit{lo(co(c.child))} = l
\end{align}


\embc{A program written in plain C is homogeneous. All program elements are
instances of the C language. Using the statemachine language extension allows
us to embed state machines in C programs. This makes the respective fragment
heterogeneous (see \fig{heterogeneous}).}


\begin{figure}[h]
\fbox{
\begin{minipage}{105mm}
  \includegraphics[width=\columnwidth]{figures-design/embc/heterogeneous.png}
\end{minipage}
}
  \caption[][0.3cm]{An example of a heterogeneous fragment. This module contains
  global variables (from the \emph{core} language), a state machine (from the
  \emph{statemachines} language) and a test case (from the \emph{unittest}
  language). Note how concepts defined in the \emph{statemachine} language
  (\icsn{trigger}, \icsn{isInState} and \icsn{test statemachine}) are used inside a
  \icsn{TestCase}.}
  \label{heterogeneous} 
\end{figure}







\section{Parsing vs. Projection}

This part of the book is not about implementation techniques. However, the
decision of whether to build a DSL using a projectional editor instead of the more
traditional parser-based approach can have some consequences for the design of
the DSL. So we have to provide \emph{some} level of detail on the two at this
point. 

In the parser-based approach, a grammar specifies the sequence of tokens
and words that make up a structurally valid program. A parser is generated from
this grammar. A parser is a program that recognizes valid programs in their
textual form and creates an abstract syntax tree or graph. Analysis tools or
generators work with this abstract syntax tree. Users enter programs using the
concrete syntax (i.e.~character sequences) and programs are also stored in this
way. Example tools in this category include Spoofax and Xtext.
\begin{marginfigure}[-60mm]
  \includegraphics[width=4cm]{figures-design/parserbased.png}
  \caption{In parser-based systems, the user only interacts with the concrete
  syntax, and the AST is constructed from the information in the text.}
  \label{parserbased} 
\end{marginfigure}  


\begin{marginfigure}[-8mm]
  \includegraphics[width=4cm]{figures-design/projectional.png}
  \caption{In projectional systems, the user sees the concrete syntax, but all
  editing gestures directly influence the AST. The AST is \emph{not} extracted
  from the concrete syntax, which means the CS does not have to be parsable.}
  \label{projectional}
\end{marginfigure}

Projectional editors (also known as structured editors) work \emph{without}
grammars and parsers. A language is specified by defining the abstract syntax
tree, then defining projection rules that render the concrete syntax of the
language concepts defined by the abstract syntax. Editing actions \emph{directly}
modify the abstract syntax tree. Projection rules then render a textual (or
other) representation of the program. Users read and write programs through this
projected notation. Programs are stored as abstract syntax trees, usually as
XML. As in parser-based systems, backend tools operate on the abstract syntax
tree. 

Projectional editing is well known from graphical editors; virtually all of them
use this approach\sidenote[][-1\baselineskip]{You could argue that they are not
\emph{purely} projectional because the user can move the shapes around and the
position information has to be persistent. Nonetheless, graphical editors are
fundamentally projectional.}. However, they can also be used for textual
syntax\sidenote{While in the past projectional text editors have gotten a bad
reputation mostly because of bad usability, as of 2011, the tools have become
good enough, and computers have become fast enough to make this approach
feasible, productive and convenient to use.}. Example tools in this category
include the Intentional Domain Workbench\footnote{\icsn{http://intentsoft.com}} and
JetBrains MPS.


In this section, we do not discuss the relative advantages and drawbacks of
parser-based vs. projectional editors in any detail (although we do discuss the
trade-offs in the chapter on language implementation, \sect{syntaximpl}).
However, we will point out if and when there are different DSL design options depending on
which of the two approaches is used.






