\FloatBarrier

\chapter{Fundamental Paradigms}
\label{learnFromGPLs}

\chapterabstract{Every DSL is different. It is driven by the domain for which it
is built. However, as it turns out, there are also a number of commonalities
between DSLs. These can be handled by modularizing and reusing (parts of) DSLs
as discussed in the last section of the previous chapter. In \emph{this} section
we look at common paradigms for describing DSL structure and behavior.}




\section{Structure}

Languages have to provide means of structuring large programs in order to keep
them manageable. Such means include modularization and encapsulation,
specification vs. implementation, specialization, types and instances as well as
partitioning. \marginnote[-1.5cm]{The language design alternatives described in
this section are usually not driven directly by the domain, or the domain
experts guiding the design of the language. Rather, they are often brought in by
the language designer as a means of managing overall complexity. For this reason
they may be hard to "sell" to domain experts.}


\subsection{Modularization and Visibility} DSLs often provide some kind of logical
unit structure, such as namespaces or modules. Visibility of symbols may be
restricted to the same unit, or to referencing ("importing") units. Symbols may
be declared as public or private, the latter making them changeable without
consequences for using modules.\marginnote{Most contemporary programming
languages use some form of namespaces and visibility restriction as their top
level structure.} Some form of namespaces and visibility is necessary in almost
any DSL. Often there are domain concepts that can play the role of the module,
possibly oriented towards the structure of the organization in which the DSL is
used. 


\embc{As a fundamental extension to C, this DSL contains modules with visibility
specifications and imports. Functions, state machines, tasks and
all other top-level concepts reside in modules. Header files (which are
effectively a poor way of managing symbol visibility) are only used in the
generated low level code and not relevant to the user of mbeddr C.}



\comparch{Components and interfaces live in namespaces. Components are
implementation units, and are always private. Interfaces and data types may be
public or private. Namespaces can import each other, making the public elements
of the imported namespace visible to the importing namespace. The OSGi generator
creates two different bundles: an interface bundle that contains the public
artifacts, and an implementation bundle with the components. In case of a
distributed system, only the interface bundle is deployed on the client.}

\pension{Pension plans constitute namespaces. They are grouped into more
coarse-grained packages that are aligned with the structure of the pension
insurance business.}


\marginnote[0.95cm]{If a repository-based tool is used, the importance of
paritioning is greatly reduced. Although even in that case, there may be a set
of federated and distributed repositories that can be considered partitions.}
\subsection{Partitioning} Partitioning refers to the breaking down of programs into
several physical units such as files (typically each model fragment is stored in
its own partition). These physical units do not have to correspond to the
logical modularization of the models within the partitions. For example, in Java
a public class has to live in a file of the same name (logical module ==
physical partition), whereas in C\# there is no relationship between namespace,
class names and the physical file and directory structure. A similar
relationship exists between partitions and viewpoints, although in most cases,
different viewpoints are stored in different partitions.


Note that a reference to an element should not take into account the partition
in which the target element lives. Instead, it should only use the logical
structure. Consider an element \ic{E} that lives in a namespace \ic{x.y},
stored in a partition \ic{mainmodel}. A reference to that element should
be expressed as \ic{x.y.E}, not as \ic{mainmodel.E} or \ic{mainmodel/x.y.E}.
This is important to allow elements to move freely between partitions without
this leading to updates of all references to the element. 


Partitioning may have consequences for language design. Consider a textual DSL
where an concept A contains a list of instances of concept B. The B instances
then have to be physically nested within an instance of A in the concrete
syntax. If there are many instances of B in a given model, they cannot be split
into several files, so these files may become big and result in performance
problems. If such a split should be possible, this has to be designed into the
language.

\comparch{A variant of this DSL that was used in another project had to be
changed to allow a namespaces to be spread over several files for
reasons of scalability and version-control granularity. In the initial version,
namespaces actually \emph{contained} the components and interfaces. In the
revised version, components and interfaces were owned by no other element, but
model files (partitions) had a namespace declaration at the top, logically
putting all the contained interfaces and components into this namespace. Since
there was no technical containment relationship between namespaces and its
elements, several files could now declare the same namespace. Changing this
design decision lead to a significant reimplementation effort because all kinds
of naming and scoping strategies changed.}





\noindent Other concerns influence the design of a partitioning strategy as
well:

\begin{description}

\item[Change Impact] Which partition changes as a consequence of a particular
change of the model (changing an element name might require changes to all
references to that element from other partitions)

\item[Link Storage] Where are links stored (are they always stored in the model
that logically "points to" another one)?, and if not, how/where/when to control
reference/link storage.

\item[Model Organzation] Partitions may be used as a way of organizing the
overall model. This is particularly important if the tool does not provide a
good means of presenting the overall logical structure of models and finding
elements by name and type. Organizing files with meaningful names in directory
structures is a workable alternative.

\item[Tool Chain Integration] Integration with existing, file based tool chains.
Files may be the unit of checkin/checkout, versioning, branching or permission
checking. 
\end{description}

\marginnote[-1.5cm]{Another driver for using partitions is the scalability of
the DSL tool. Beyond a certain file size, the editor may become sluggish.}


It is often useful to ensure that each partition is processable separately to
reduce processing times. An alternative approach supports the explict definition
of those partitions that should be processed in a given processor run (or at
least a search path, a set of directories, to find the partitions, like an
include path in C compilers or the Java classpath). You might even consider a
separate build step to combine the results created from the separate processing
steps of the various partitions (again like a C compiler: it compiles every file
separately into an object file, and then the linker handles overall
symbol/reference resolution and binding).

The partitioning scheme may also influence users' team collaboration when
editing models. There are two major collaboration models: real-time and
commit-based. In real-time collaboration, a user sees his model change in real
time as another user changes that same model. Change propagation is immediate. A
database-backed repository is often a good choice regarding storage, since the
granularity tracked by the repository is the model element. In this case, the
partitioning may not be visible to the end user, since they just work "on the
repository". This approach is often (at least initially) preferred by
non-programmer DSL users. The other collaboration mode is commit-based where a
user's changes only make it to the repository if he performs a \emph{commit},
and incoming changes are only visible after a user has performed an
\emph{update}. While this approach can be used with database-backed
repositories, it is most often used with file-based storage. In this case, the
partitioning scheme is visible to DSL users, because it is those files they
commit or update. This approach tends to be preferred by developers, maybe
because well-known versioning tools have used the approach for a long time.





\subsection{Specification vs. Implementation} Separating specification and
implementation supports plugging in different implementations for the same
specification and hence provides a way to "decouple the outside from the
inside"\footnote{Interfaces, pure abstract classes, traits or function
signatures are a realization of this concept in programming languages.}. This
supports the exchange of several implementations behind a single interface. This
is often required as a consequence of the development process: one stakeholder
defines the specification and a client, whereas another stakeholder provides one
or more implementations\marginnote{The separation of specification and
implementation can also have positive effects on scalability and performance. If
the specification and implementation are separated into different fragments,
then, in order to type check a client's access to some provided service, only
the fragment that contains the specification has to be loaded/parsed/checked,
which is obviously faster than processing complete implementation.}. 

A challenge for this approach is how to ensure that all implementations are
consistent with the specification. Traditionally, only the
structural/syntactic/signature compatibility is checked. To ensure 
semantic compatibility, additional means that specify the expected
\emph{behavior} are required. This can be achieved with pre or post conditions,
invariants or protocol state machines. 


\embc{This DSL adds interfaces and components to C. Components provide or use
one or more interfaces. Different components can be plugged in behind the same
interface. To support semantic specifications, the interfaces support pre- and
post conditions as well as protocol state machines. \fig{ppc} shows an example.
Although these specifications are attached to interfaces, they are actually
checked (at runtime) for all components that provide the respective interface.}

\begin{figure}[h]
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm}
  \includegraphics[width=90mm]{figures-design/embc/ppc.png}
  \caption{An interface using semantic specifications. Preconditions check the
  values of arguments for validity. Postconditions express constraints on the
  values of \ic{query} operations after the execution of the operation. Notice
  how the value of the \ic{query} before executing the operation can be
  referred to (the \ic{old} keyword used in the postconditin for
  \ic{accelerateBy}). In addition, protocols constrain the valid sequence of
  operation invocations. For example, the \ic{accelerateBy} operation can only
  be used if the protocol state machine is already in the \ic{forward} state.
  The system gets into the \ic{forward} state by invoking the
  \ic{driveContinuoslyForward} operation.}
  \vspace{2mm}
  \label{ppc} 
  \rule{1\textwidth}{0.7pt} 
\end{figure}  




\cooling{Cooling programs can refer to entities defined as part of the
refrigerator hardware as a means of accessing hardware elements (compressors,
fans, valves). To enable cooling programs to run with different, but similar
hardware configurations, the hardware structure can use "trait inheritance",
where a hardware trait defines a set of hardware elements, acting as a kind of
interface. Other hardware configurations can inherit these traits. As long as
cooling programs are only written against traits, they work with any
refrigerator that implements the particular set of traits against which the
program is written.}



\subsection{Specialization} Specialization enables one entity to be a more
specific variant of another one. Typically, the more specific one can be used in
all contexts where the more general one is expected (the Liskov substitution
principle\cite{LiskovW94}). The more general one may be incomplete, requiring
the specialized ones to "fill in the holes".\marginnote{In GPLs, we know this
approach from class inheritance. "Leaving holes" is realized by abstract
methods.} Specialization in the context of DSLs can be used for implementing
variants or for evolving a program over time.
 
Note that defining the semantics of inheritance for domain-specific language
concepts is not always easy. The various approaches found in programming
languages, as well as the fact that some of them lead to problems (multiple
inheritance, diamond inheritance, linearization, or code duplication in Java's
interface inheritance) shows that this is not a trivial topic. It is a good idea
to just copy a suitable approach \emph{completely} from a programming language
where inheritance seems to work well. Even small changes can make the whole
approach inconsistent.
 
 
\pension{The customer using this DSL had the challenge of creating a huge set of
pension plans, implementing changes in relevant law over time, or implementing
related plans for different customer groups. Copying complete plans and then
making adaptations was not feasible, because this resulted in a maintenance
nightmare: a large number of similar, but not identical pension plans. Hence the
DSL provides a way for pension plans to inherit from one another. Calculation
rules can be marked \emph{abstract} (requiring to be overwritten in sub-plans),
\emph{final} rules are not overwritable. Visibility modifiers control which
rules are considered "implementation details".}

\cooling{A similar approach is used in the refrigerator DSL. Cooling programs
can specialize other cooling programs. Since the programs are fundamentally
state-based, we had to define what exactly it means to override a cooling
program: a subprogram can add additional event handlers and transitions to
states. New states can also be added. However, states defined in the
super-program cannot be removed.}



\subsection{Types and Instances} Types and instances refers to the ability to
define a structure that can be parametrized when it is
instantiated\marginnote{In programming languages we know this from classes and
objects (where constructor parameters are used for parametrization) or from
components (where different instances can be connected differently to other
instances).}. It supports reusing the fixed parts, and expressing variability
via the parameters.

\embc{Apart from C's \lcr{structs} (which are instantiatable data structures)
and components (which can be instantiated and connected), state machines can
be instantiated as well. Each instance can be in a different state at any given
time.}

\subsection{Superposition and Aspects} Superposition\marginnote{This is
especially important in the context of product line engineering and is discussed
in section \todo{ref} of this book.} refers to the ability to merge several
model fragments according to some DSL-specific merge operator. Aspects provide a
way of "pointing to" several locations in a program based on a pointcut operator
(essentially a query over a program or its execution), adapting the model in
ways specified by the aspect. Both approaches support the compositional creation
of many different model variants from the same set of model fragments.

\comparch{This DSL provides a way of advising component definitions from an
aspect (\fig{monitoring}). An aspect may introduce an additional port
\ic{provided port mon: IMonitoring} that allows a central monitoring component
to query the adviced components via the \lcr{IMonitoring} interface.}

\begin{figure}[h]
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm} 
  \includegraphics[width=10cm]{figures-design/comparch/monitoring.png}
  \caption[][0.3cm]{The aspect component contributes an additional required port
  to each of the other components defined in the system.}
  \label{monitoring}
  \rule{1\textwidth}{0.7pt}
\end{figure} 

\exwebdsl{Entity declarations can be \emph{extended} in separate modules. This
makes it possible to declare in one module all data model declarations of a
particular feature. For example, in the \emph{researchr} application, a
\lcr{Publication} can be \lcr{Tag}ged, which requires an extension of the
\lcr{Publication} entity. This extension is defined in the \lcr{tag}
module, together with the definition of the \lcr{Tag} entity. This is
essentially a use of superposition.}



\subsection{Versioning} Often, variability over time of elements in DSL programs
have to be tracked. One alternative is to simply version the model files using
existing version control systems, or the version control mechanism built into
the language workbench. However, this requires users to interact with often
complex version control systems and prevents domain-specific adaptations of the
version control strategy.

The other alternative is to make versioning and tracking over time a part of the
language. For example, model elements may be tagged with version numbers or
specify a revision chain by pointing to a previous revision, enforcing
compatibility constraints between those revisions. Instead of declaring explicit
versions, business data is often time-dependent, where different revisions of a
business rule apply to different periods of time. Support for these approaches
can be built directly into the DSL, with various levels of tool support.

\embc{No versioning is defined into the DSL. Users work with MPS'
integration with popular version control systems. Since this DSL is intended
for use by programmers, working with existing version control systems is not a
problem.}

\comparch{Components can specify a \lcr{new version of} reference to another
component In this case, the new version may specify or additional
provided ports with the same interfaces or new versions of these interfaces. It
may deprecate required ports. Effectively, this means that the new version of
something must be replacement-compabible with the old version (the Liskov
substitution principle again).}

\pension{In the pension workbench, calculation rules declare applicability
periods. This supports the evolution of calculation rules over time, while
retaining reproducability for calculations performed at an earlier point in
time. Since the Intentional Domain Workbench is a projectional tool, pension
plans can be shown with only the version of a rule valid for a given point in
time.}


\todo{here}



\section{Behavior}
\label{parabeh}

The behavior expressed with a DSL must of course be aligned with the needs of
the domain. However, in many cases, the behavior required for a domain can be
derived from well-known behavioral paradigms, with slight adaptations or
enhancements, or simply interacting with domain-specific structures or data.

Note that there are two kinds of DSLs that don't make use of these kinds of
behavior descriptions. Some DSLs really just specify structures. Examples
include data definition languages or component description languages (although
both of them often use expressions for derived data, data validation or pre- and
post-conditions). Other DSLs only specify the kind of behavior expected, and the
generator creates the algorithmic implementation. For example, a DSL may
specify, simply with a tag such as \emph{async}, that the communication between
two components shall be asynchronous. The generator then maps this to an
implementation that behaves according to this specification.

\comparch{The component architecture DSL is an example of a structure-only DSL,
since it only describes black box components and their interfaces and
relationships. It uses the specification-only approach to specify whether a
component port is intended for synchronous or asynchronous communication.}

\embc{The component extension to provides a similar notion of interfaces, ports
and components as in the previous example. However, since here they are directly
integrated with C, C expression can be used for pre- and post-conditions of
interface operations (\fig{precond}).}

\begin{figure}[h]
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm}
  \includegraphics[width=9cm]{figures-design/embc/precond.png}
  \caption[][0.3cm]{Precondiditions can be added to interface operations. They
  are executed at the beginning of everry runnable that implements the
  particular operation.}
  \label{precond}
  \rule{1\textwidth}{0.7pt}
\end{figure} 
  
\marginnote{This is only an overview over a few paradigms; many more
exist. I refer to the excellent Wikipedia entry on \emph{Programming Paradigms} and to
the book\cite{RoyH2004}}
This section describes some of the most well-known behavioral paradigms that
can serve as useful starting points for behavior descriptions in
DSLs.
 
\subsection{Imperative} \marginnote[0.5cm]{For many people, often including
domain experts, this approach is most obvious. Hence it is often a good starting point
for DSLs.} Imperative programs consist of a sequence of statements, or
instructions, that change the state of a program. This state may be local to
some kind of module (e.g. a procedure or an object), global (as in global
variables) or external (when talking to periphery). Procedural and
object-oriented programming are both imperative, using different means for
structuring and (in case of OO) specialization. Because of aliasing and side
effects imperative programs are expensive to analyse. Debugging imperative
programs is straight forward and involves stepping through the instructions and
watching the state change.

\embc{Since C is used as a base language, this language is fundamentally
imperative. Some of the DSLs on top of it use other paradigms.}

\cooling{The cooling language uses various paradigms, but contains sequences
of statements to implement aspects of the overall cooling behavior.}

\subsection{Functional} Functional programming uses functions as the core
abstraction. In purely functional programming, a function's return value only
depends on the values of its arguments. Calling the same function several times
with the same argument values returns the same result (that value may even be
cached!). Functions cannot access global mutable state, no side effects are
allowed. These characteristics make functional programs very easy to analyze and
optimize. These same characteristics, however, also make purely functional
programming relatively useless, because it cannot affect the environment (after
all, this is a side effect). So, functional programming is often obly used for
parts ("calculation core") of an overall program  that uses other
paradigms as well. 

Since there is no state to watch change as the program steps through
instructions, debugging can be done by simply showing all intermediate results
of all function calls, basically "inspecting" the state of the calculation. This
makes building debuggers relatively simple. 

\pension{The calculation core of pension rules is functional. Consequently, a
debugger has been implemented that, for a given set of input data, shows the
rules as a tree where all intermediate results of each function call are shown
(\fig{debugger}). No "step through" debugger is necessary.}

\begin{figure}[h]
  \includegraphics[width=11cm]{figures-design/pension/debugger.png}
  \caption{Debugging functional programs can be done by showing the state of
  the calculation, for example as a tree.}
  \label{debugger}
\end{figure}

A relevant subset of functional programming is pure expressions (as in \ic{3*2+7
> i}. Instead of calling functions, operators are used. However, operators are
just inline-notations for function calls. Usually the operators are hard wired
into the language and it is not possible for users to define their own
functional abstractions. This is the main differentiator to functional
programming in general.

\embc{We use expressions in the guard conditions of the state machine
extension as well as in pre- and post-conditions for interface operations. In
both cases it is not possible to define or call external functions. Of course,
C's expression language is reused here.}

\subsection{Declarative} Declarative programming can be considered the opposite of
imperative programming (and, to some extent, functional programming). A
declarative program does not specify any control flow. It does not specify a
sequence of steps of a calculation. Instead, a declarative program only
specifies \emph{what} the program should accomplish, not \emph{how}. This is
often done by specifying a set of properties, equations, relationships or
constraints. Some kind of evaluation engine then tries to find solutions. The
particular advantage of this approach is that it is not predefined how a
solution is found, the evaluation engine has a lot of freedom in doing so,
possibly using different solutions in different environments, or evolving the
approach over time\footnote[][-0.5cm]{For example, the strategies for
implementing SAT solvers have evolved quite a bit over time. SAT solvers are
much more scalable today. However, the formalism for describing the logic
formulas that are processed by SAT solvers have not changed}. This large degree
of freedom often makes finding the solution expensive --- trial and error,
backtracking or exhaustive search may be used\sidenote{So, often users have to
provide hints to the engine to make it run fast enough or scale to programs of
relevant size. In practice, declarative programming is often not as "pure" as it
is in theory.}. Debugging declarative programs can be hard since the solution
algorithm may be very complex and possibly not even be known to the user of the
language.

Declarative programming has many important subgroups and use cases. For
\emph{concurrent programs}, a declarative approach allows the efficient
execution of the same program on different parallel hardware. The compiler or
runtime system can allocate the program to available computational resources. In
\emph{constraint programming}, the programmer specifies constraints between a
set of variables. The engine tries to find values for these variables that
satisfy all constraints. Solving mathematical equation systems is an example,
as is solving sets of boolean logic formulas.

\comparch{This DSL specifies timing and resource characteristics for component
and interface operations. Based on this data, one could run an algorithm which
allocates the component instances to computing hardware so that the hardware is
used as efficiently as possible, while at the same time reducing the amount of
bus traffic. This is an example of constraint solving used to synthesize a
schedule.}

\embc{This DSL supports presence conditions for product line engineering. A
presence condition is a boolean expression over a set of configuration features
that determines whether the associated piece of code is present for a given
combination of feature selections (\fig{presencecond}). To verify the structural
integrity of programs in the face of varying feature combinations, constraint
programming is used (to ensure that there is no configuration of the program
where a reference to a symbol is included, but the referenced symbol is not).
From the program, the presence conditions and the feature model a set of boolean
equations is generated. A solver then makes sure they are consistent by trying
to find an example solution that violates the boolean equations.
\todo{cite: refer to DSL impl chapter, and to literature (Czarnecki)} }

\begin{figure}[h]
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm}
  \includegraphics[width=10cm]{figures-design/embc/presencecond.png}
  \caption[][0.3cm]{This module contains variability expressed with presence
  conditions. The affected program elements are highlighted in a color that
  represents the condition. If the feature \ic{highRes} is selected, the code
  uses a \ic{double} instead of an \ic{int8\_t}. The log messages are only
  included if the \ic{logging} feature is selected.}
  \label{presencecond}
  \rule{1\textwidth}{0.7pt}
\end{figure} 


\genex{The Yakindu DAMOS block diagram editor supports custom block
implementation based on the Mscript language. Mscript (\sect{mscript}) supports
declarative specification of equations between input and output parameters of a
block. A solver comes up with an closed, sequential solution that efficiently
calculates the output of an overall block diagram.}

\begin{figure}[h]
  \rule{1\textwidth}{0.7pt}
  \vspace{-0.5cm}
  \includegraphics[width=15cm]{figures-design/mscript.jpg}
  \caption[][1.0cm]{An Mscript block specifies input and output arguments
  of a block ($u$ and $v$) as well as configuration parameters
  ($initialCondition$ and $gain$). The assertions specify constraints on the
  data the blocks works with. The $eq$ statements specify how the output values
  are calculated from the input values. Stateful behaviors are
  supported, where the value for the $n$-th step depends on values from
  previous steps (e.g. $n-1$).  }
  \label{mscript}
  \rule{1\textwidth}{0.7pt}
\end{figure} 





\genex{Another example for declarative programming is the type system DSL used
by MPS itself. Language developers specify a set of type equations containing
free type variables, among other things. A unification engine tries to solve the
set of equations by assigning actual types to the free type variables so that
the set of equations is consistent. We describe this approach in detail in
section \todo{ref}}

\emph{Logic programming} is another subparadigm of declarative programming where
users specify logic clauses (facts and relations) as well as queries. A theorem
prover tries to solve the queries\marginnote{The Prolog language works this
way}.

\subsection{Reactive/Event-based/Agent} In this paradigm, behavior is triggered
based on events received by some entity. Events may be created by another entity
or by the environment (through a device driver). Reactions are expressed by the
production of other events. Events may be globally visible or explicitly routed
between entities, possibly using filters and/or using priority queues. This
approach is often used in embedded systems that have to interact with the real
world, where the real world produces events as it changes. A variant of this
approach queries input signals at intervals controlled by a scheduler and
considers changes in input signals as the events.

\cooling{The cooling algorithms are reactive programs that control the cooling
hardware based on environment events. Such events include the opening of a
refrigerator door, the crossing of a temperature threshold, or a timeout that
triggers defrosting of a cooling compartment. Events are queued, and the queues
are processed in intervals determined by a scheduler.}

Debugging is simple if the timing/frequency of input events can be controlled.
Visualizing incoming events and the code that is triggered as a reaction is
relatively simple. If the timing of input events cannot be controlled, then
debugging can be almost impossible, because humans are way too slow to fit "in
between" events that may be generated by the environment in rapid succession.
For this reason, various kinds of simulators are used to debug the behavior of
reactive systems, and sophisticated diagnostics regarding event frequencies
or queue filling levels may have to be integrated into the programs as they run
in the real environment.

\cooling{The cooling language comes with a simulator (\fig{simulator})  based on
an interpreter where the behavior of a cooling algorithm can be debugged. Events are
explicitly created by the user, on a time scale that is compatible
with the debugging process.}

\begin{figure}[h]
\begin{center}
  \includegraphics[width=8cm]{figures-design/cooling/simulator.png}
  \caption{The simulator for the cooling language shows the state of the system
  (commands, event queue, value of hardware properties, variables and tasks).
  The program can be single-stepped. The user can change the value of
  variables or hardware properties as a means of interacting with the program.}
  \label{simulator}
\end{center} 
\end{figure} 


\begin{marginfigure}[3cm]
  \includegraphics[width=4cm]{figures-design/graphicalFlow.png}
  \caption{Graphical Notation for Flow}
  \label{graphicalFlow2}
\end{marginfigure} 
\subsection{Dataflow} The dataflow paradigm is centered around variables with
dependencies (relationships in terms of calculation rules) among them. As a
variable changes, those variables that depend on the changing variable are
recalculated. We know this approach mainly from two use cases. One is
spreadsheets: cell formulas express dependencies to other cells. As the values
in these other cells change, the dependent cells are updated. The other use case
is data flow (or block) diagrams (\fig{graphicalFlow2}, used in embedded
software, extraction-transfer-load data processing systems and enterprise
messaging/complex event processing. There, the calculations or transformations
are encapsulated in the blocks, and the lines represent dependencies --- the
output of one blocks "flows" into input slot of another block. There are three
different execution modes:

\begin{itemize}
  \item The first one considers the data values as continuous signals. At the
  time one of the inputs changes, all dependent values are recalculated, the
  change triggers the recalculation, and the recalculation ripples through the
  dependency graph. This is the model used in spreadsheets.
  \item The second one considers the the data values quantized, unique messages.
  Only if a message is available for all inputs, a new output message is
  calculated. The recalculation synchronizes on the availability of a message at
  each input, and upon recalculation, these messages are consumed. This approach
  is often used in ETL and CEP systems.
  \item The third approach is time triggered. Once again, the inputs are
  understood to be continuous signals, and a scheduler determines when a new
  calculation is performed. It also makes sure that the calculation "ripples
  through from left to right" in the correct order. This model is typically used
  in embedded systems.
\end{itemize}

\noindent Debugging these kinds of systems is relatively straight forward
because the calculation is always in a distinct state. Dependencies and data flow, or the
currently active block and the available messages can easily be visualized in a
block diagram notation. Note that the calculation rules themselves are
considered black boxes here, whose inside may be built from any other paradigm,
often functional. Integrating debuggers for the inside of boxes as well is a
more challenging task.


\subsection{State-based} The state-based paradigm describes a system's behavior
in terms of the states the system can be in, the transitions between these
states as well as events that trigger these transitions and actions that are
executed as states change. State machines are useful for systematically
organizing the behavior of an entity. It can also be used to describe valid
sequences of events, messages or procedure calls. State machines can be used in
an event-driven mode where incoming events actually trigger transitions and the
associated actions. Alternatively a state machine can be run in a timed mode,
where a scheduler determines when event queues are checked and processed. Except
for possible real-time issues, state machines are easy to debug by highlighting
the contents of event queues and the current state.

\embc{As mentioned before, this language provides an extension that supports
directly working with state machines. Events can be passed into a state machine
from regular C code or by mapping incoming messages in components to events in
state machines that reside in components. Actions can contain arbitrary C code,
unless the state machine shall be verifiable in which case actions may only
create outgoing events or change statemachine-local variables.}

\cooling{The behavior of cooling programs is fundamentally state driven. A
scheduler is used to execute the state machine in regular intervals. Transitions
are triggered either by incoming, queued events or by changing property values
of hardware building blocks. Note that this language is an example where a
behavioral paradigm is used without significant alterations, but working with
domain-specific data structures: refrigerator hardware and their properties.}

As mentioned before, state-based behavior description is also interesting in the
context of model checking. The model checker either determines that the state
chart conforms to a set of specifications or provides a counter example.
Specifications express something about sequences of states such as: "it is not
possible that two traffic lights show green at the same time" or "whenever a
pedestrian presses the \ic{request} button, the pedestrian lights eventually
will show green"\marginnote[-2cm]{We provide some details about model checking in the
implementation part \todo{ref}. A good introduction to model checking can be
found in \cite{BerardBidoitM2001}. We elaborate on model checking in section.}
\todo{ref}. 

In principle, any program can be represented as a state machine and then model
checked. However, creating state machines from, say, a procedural C program is
non-trivial, and the state machines also become very big very quickly.
State-based programs \emph{are already} a state machine, and, they are typically
not that big either (after all, they have to be understood by the developer who
creates and maintains them). Consequently, many realistically-sized state
machines can be model checked efficiently.


 
\section{Combinations}

Many DSLs use combinations of various behavioral and structural paradigms
described in this section\footnote{Note how this observation leads to the
desire to better modularize and reuse some of the above paradigms. Room for
research :-)}. A couple of combinations are very typical:

\begin{itemize}
  \item a data flow language often uses a functional, imperative or declarative
  language to describe the calculation rules that express the dependencies between the
  variables (the contents of the boxes in data flow diagrams or of cells in
  spreadsheets). \fig{blockdiagram} shows an example block diagram, and
  \fig{mscript} shows an example implementation.
  \item state machines use expressions as transition guard conditions, as well
  as typically an imperative language for expressing the actions that are
  executed as a state is entered or left, or when a transition is executed. An
  example can be seen in \fig{statemachines}
  \item reactive programming, where "black boxes" react to events, often use
  data flow or state-based programming to implement the behavior that
  determines the reactions.
  \item in purely structural languages, for example, those for expressing
  components and their dependencies, a functional/expression language is often
  used to express pre- and postconditions for operations. A state-based language 
  is often used for protocol state machines, which determines the valid order of
  incoming events or operation calls.
\end{itemize}

\noindent Note that these combinations can be used to make well-established
paradigms domain specific. For example, in the Yakindu State Chart Tools
(\fig{statemachines}), a custom DSL can be plugged into an existing, reusable
state machine language and edtor. Some of the case studies used in this section
of the book also use combinations of several paradigms.

\pension{The pension language uses functional abstractions with mathematical
symbols for the core actuary mathematics. A functional language with a normal
textual syntax is used for the higher-level pension calculation rules. A
spreadsheet/data flow language is used for expressing unit tests for pension
rules. Various nesting levels of namespaces are used to organize the rules, the
most important of which is the pension plan. A plan contains calculation rules
as well as test cases for those rules. Pension plans can specialize other plans
as a means of expressing variants. Rules in a sub-plan can override rules in the
plan from which the sub-plan inherits. Plans can be declared to be abstract,
with abstract rules that have to be implemented in sub-plans. Rules are
versioned over time, and the actual calculation formula is part of the version.
Thus, a pension plan's behavior can be made to be different for different
points in time. }

\cooling{The cooling behavior description is described as a reactive system.
Events are produced by hardware elements (and their drivers). A state machine
constitutes the top level structure. Within it, an imperative language is used.
Programs can inherit from another program, overwriting states defined in the
base program: new transitions can be added, and the existing transitions can be
overridden as a way for an extended program to "plug into" the base program.}

